#!/usr/bin/env python3
"""
SignalFeed - AI å¢žå¼ºè„šæœ¬
ä½¿ç”¨ DeepSeek API ä¸ºæ–‡ç« æ·»åŠ å…³é”®è¯å’Œæ ¸å¿ƒè¦ç‚¹
æ”¯æŒæ‰¹é‡å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ 
"""

import json
import os
from datetime import datetime
from pathlib import Path
import time
import sys

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY', '')
DEEPSEEK_API_URL = 'https://api.deepseek.com/v1/chat/completions'

def call_deepseek_api(prompt, max_tokens=500):
    """è°ƒç”¨ DeepSeek API"""
    import urllib.request

    if not DEEPSEEK_API_KEY:
        print("Warning: DEEPSEEK_API_KEY not set")
        return None

    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {DEEPSEEK_API_KEY}'
    }

    data = {
        'model': 'deepseek-chat',
        'messages': [
            {'role': 'user', 'content': prompt}
        ],
        'max_tokens': max_tokens,
        'temperature': 0.7
    }

    try:
        req = urllib.request.Request(
            DEEPSEEK_API_URL,
            data=json.dumps(data).encode('utf-8'),
            headers=headers
        )

        with urllib.request.urlopen(req, timeout=30) as response:
            result = json.loads(response.read().decode('utf-8'))
            return result['choices'][0]['message']['content']

    except Exception as e:
        print(f"API Error: {e}")
        return None

def enhance_article(article):
    """ä¸ºå•ç¯‡æ–‡ç« æ·»åŠ  AI å¢žå¼ºå†…å®¹"""
    title = article.get('title', '')
    description = article.get('description', '')

    # å®šä¹‰å›ºå®šçš„æ ‡ç­¾åˆ†ç±»
    STANDARD_TAGS = [
        "AI/æœºå™¨å­¦ä¹ ", "ç¼–ç¨‹è¯­è¨€", "Webå¼€å‘", "ç§»åŠ¨å¼€å‘", "DevOps",
        "äº‘è®¡ç®—", "æ•°æ®åº“", "ç½‘ç»œå®‰å…¨", "å¼€æºé¡¹ç›®", "è½¯ä»¶å·¥ç¨‹",
        "ç³»ç»Ÿæž¶æž„", "æ€§èƒ½ä¼˜åŒ–", "æµ‹è¯•", "å·¥å…·", "ç¡¬ä»¶",
        "äº§å“è®¾è®¡", "èŒä¸šå‘å±•", "æŠ€æœ¯è¶‹åŠ¿", "å…¶ä»–"
    ]

    # æž„å»ºä¼˜åŒ–åŽçš„ prompt
    prompt = f"""è¯·åˆ†æžä»¥ä¸‹æŠ€æœ¯æ–‡ç« ï¼Œæä¾›æ ‡ç­¾å’Œæ ¸å¿ƒè¦ç‚¹ï¼š

æ ‡é¢˜ï¼š{title}
æè¿°ï¼š{description[:500]}

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "summary": "ç”¨1-2å¥è¯æ€»ç»“æ–‡ç« æ ¸å¿ƒå†…å®¹ï¼ˆä¸­æ–‡ï¼‰",
  "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
}}

è¦æ±‚ï¼š
1. tags: ä»Žä»¥ä¸‹æ ‡ç­¾ä¸­é€‰æ‹©1-2ä¸ªæœ€ç›¸å…³çš„ï¼š
   {', '.join(STANDARD_TAGS)}
2. summary: ç®€æ´æ˜Žäº†ï¼ŒæŠ“ä½æ ¸å¿ƒ
3. key_points: 3ä¸ªæœ€é‡è¦çš„è¦ç‚¹ï¼Œæ¯ä¸ªä¸è¶…è¿‡30å­—
"""

    print(f"Processing: {title[:50]}...")
    result = call_deepseek_api(prompt, max_tokens=600)

    if result:
        try:
            # æ¸…ç† markdown ä»£ç å—
            cleaned = result.strip()
            if cleaned.startswith('```'):
                lines = cleaned.split('\n')
                json_start = -1
                json_end = -1
                for i, line in enumerate(lines):
                    if '{' in line and json_start == -1:
                        json_start = i
                    if '}' in line:
                        json_end = i

                if json_start != -1 and json_end != -1:
                    cleaned = '\n'.join(lines[json_start:json_end+1])

            # è§£æž JSON
            enhanced = json.loads(cleaned)
            print(f"âœ“ Successfully enhanced")
            return enhanced
        except Exception as e:
            print(f"âœ— Failed to parse: {e}")
            return None

    return None

def load_articles():
    """åŠ è½½æ‰€æœ‰æ–‡ç« """
    articles_dir = Path(__file__).parent.parent / "data" / "articles"
    all_articles = []

    if not articles_dir.exists():
        return []

    for json_file in sorted(articles_dir.glob("*.json")):
        with open(json_file, 'r', encoding='utf-8') as f:
            articles = json.load(f)
            all_articles.extend(articles)

    return all_articles

def load_processed_hashes():
    """åŠ è½½å·²å¤„ç†çš„æ–‡ç« å“ˆå¸Œåˆ—è¡¨ï¼ˆç”¨äºŽæ–­ç‚¹ç»­ä¼ ï¼‰"""
    progress_file = Path(__file__).parent.parent / "data" / "ai_processed.txt"
    if progress_file.exists():
        with open(progress_file, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_processed_hash(url_hash):
    """ä¿å­˜å·²å¤„ç†çš„æ–‡ç« å“ˆå¸Œ"""
    progress_file = Path(__file__).parent.parent / "data" / "ai_processed.txt"
    with open(progress_file, 'a', encoding='utf-8') as f:
        f.write(f"{url_hash}\n")

def save_enhanced_articles(articles):
    """ä¿å­˜å¢žå¼ºåŽçš„æ–‡ç« """
    output_file = Path(__file__).parent.parent / "data" / "articles_enhanced.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ… Saved enhanced articles to: {output_file}")

if __name__ == "__main__":
    print("ðŸ¤– SignalFeed AI Enhancement - Starting...")

    if not DEEPSEEK_API_KEY:
        print("âŒ Error: DEEPSEEK_API_KEY environment variable not set")
        print("Please set it with: export DEEPSEEK_API_KEY='your-api-key'")
        exit(1)

    # è§£æžå‘½ä»¤è¡Œå‚æ•°
    batch_size = 20  # æ¯æ‰¹å¤„ç†çš„æ–‡ç« æ•°
    if len(sys.argv) > 1:
        try:
            batch_size = int(sys.argv[1])
        except:
            print(f"Invalid batch size, using default: {batch_size}")

    # åŠ è½½æ–‡ç« 
    articles = load_articles()
    print(f"ðŸ“Š Loaded {len(articles)} articles")

    # åŠ è½½å·²å¤„ç†çš„æ–‡ç« 
    processed_hashes = load_processed_hashes()
    print(f"ðŸ“ Already processed: {len(processed_hashes)} articles")

    # ç­›é€‰æœªå¤„ç†çš„æ–‡ç« 
    unprocessed = [a for a in articles if a.get('url_hash') not in processed_hashes]
    print(f"ðŸ”„ To process: {len(unprocessed)} articles")

    if not unprocessed:
        print("âœ… All articles already processed!")
        exit(0)

    # æ‰¹é‡å¤„ç†
    total = len(unprocessed)
    batch_to_process = unprocessed[:batch_size]
    print(f"\nðŸ”„ Processing batch: {len(batch_to_process)} articles")
    print(f"   Remaining after this batch: {total - len(batch_to_process)}")

    # åŠ è½½çŽ°æœ‰çš„å¢žå¼ºæ–‡ç« ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
    enhanced_file = Path(__file__).parent.parent / "data" / "articles_enhanced.json"
    if enhanced_file.exists():
        with open(enhanced_file, 'r', encoding='utf-8') as f:
            all_enhanced = json.load(f)
        print(f"ðŸ“‚ Loaded {len(all_enhanced)} existing enhanced articles")
    else:
        all_enhanced = []

    # åˆ›å»ºå“ˆå¸Œåˆ°æ–‡ç« çš„æ˜ å°„
    enhanced_map = {a.get('url_hash'): a for a in all_enhanced if a.get('url_hash')}

    # å¤„ç†å½“å‰æ‰¹æ¬¡
    success_count = 0
    for i, article in enumerate(batch_to_process, 1):
        print(f"\n[{i}/{len(batch_to_process)}] ", end='')
        enhanced = enhance_article(article)

        if enhanced:
            article['ai_enhanced'] = enhanced
            enhanced_map[article['url_hash']] = article
            save_processed_hash(article['url_hash'])
            success_count += 1
        else:
            # å³ä½¿å¤±è´¥ä¹Ÿæ·»åŠ åˆ°æ˜ å°„ä¸­ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
            enhanced_map[article['url_hash']] = article
            save_processed_hash(article['url_hash'])

        # é¿å… API é™æµ
        time.sleep(1.5)

    # ä¿å­˜æ‰€æœ‰å¢žå¼ºåŽçš„æ–‡ç« 
    all_enhanced = list(enhanced_map.values())
    save_enhanced_articles(all_enhanced)

    print(f"\nâœ… Batch complete!")
    print(f"   Successfully enhanced: {success_count}/{len(batch_to_process)}")
    print(f"   Total enhanced articles: {len(all_enhanced)}")
    print(f"   Remaining to process: {total - len(batch_to_process)}")

    if total > len(batch_to_process):
        print(f"\nðŸ’¡ Run again to process the next batch")
