#!/usr/bin/env python3
"""
SignalFeed - AI å¢å¼ºè„šæœ¬
ä½¿ç”¨ DeepSeek API ä¸ºæ–‡ç« æ·»åŠ å…³é”®è¯å’Œæ ¸å¿ƒè¦ç‚¹
æ”¯æŒæ‰¹é‡å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ 
"""

import json
import os
from datetime import datetime
from pathlib import Path
import time
import sys
import re
import html

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY', '')
DEEPSEEK_API_URL = 'https://api.deepseek.com/v1/chat/completions'
MAX_ANALYSIS_INPUT_CHARS = 7000
MIN_FULLTEXT_LENGTH = 900

def call_deepseek_api(prompt, max_tokens=500):
    """è°ƒç”¨ DeepSeek API"""
    import urllib.request

    if not DEEPSEEK_API_KEY:
        print("Warning: DEEPSEEK_API_KEY not set")
        return None

    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {DEEPSEEK_API_KEY}'
    }

    data = {
        'model': 'deepseek-chat',
        'messages': [
            {'role': 'user', 'content': prompt}
        ],
        'max_tokens': max_tokens,
        'temperature': 0.7
    }

    try:
        req = urllib.request.Request(
            DEEPSEEK_API_URL,
            data=json.dumps(data).encode('utf-8'),
            headers=headers
        )

        with urllib.request.urlopen(req, timeout=30) as response:
            result = json.loads(response.read().decode('utf-8'))
            return result['choices'][0]['message']['content']

    except Exception as e:
        print(f"API Error: {e}")
        return None

def normalize_text(value):
    """å‹ç¼©ç©ºç™½å¹¶å»é™¤é¦–å°¾ç©ºç™½"""
    if value is None:
        return ""
    text = re.sub(r"\s+", " ", str(value))
    return text.strip()

def clean_content_text(value):
    """å°† HTML å†…å®¹è½¬æ¢ä¸ºæ›´æ˜“åˆ†æçš„çº¯æ–‡æœ¬"""
    text = str(value or "")
    if not text.strip():
        return ""

    text = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", text)
    text = re.sub(r"(?i)<br\s*/?>", "\n", text)
    text = re.sub(r"(?i)</(p|div|h[1-6]|li|blockquote|section|article|tr)>", "\n", text)
    text = re.sub(r"(?i)<li[^>]*>", "\n- ", text)
    text = re.sub(r"<[^>]+>", " ", text)
    text = html.unescape(text)
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def build_analysis_input(article):
    """ä¼˜å…ˆä½¿ç”¨ RSS åŸæ–‡ä½œä¸ºåˆ†æè¾“å…¥ï¼Œæ‘˜è¦ä½œä¸ºå›é€€"""
    raw_content = article.get("content", "")
    raw_description = article.get("description", "")

    cleaned_content = clean_content_text(raw_content)
    cleaned_description = clean_content_text(raw_description)

    has_fulltext_signal = bool(article.get("is_fulltext")) or len(cleaned_content) >= MIN_FULLTEXT_LENGTH
    use_content = len(cleaned_content) >= 120

    selected = cleaned_content if use_content else cleaned_description
    source_type = "fulltext" if has_fulltext_signal and use_content else "summary"

    truncated = False
    if len(selected) > MAX_ANALYSIS_INPUT_CHARS:
        selected = selected[:MAX_ANALYSIS_INPUT_CHARS].rstrip()
        truncated = True

    if not selected:
        selected = normalize_text(article.get("title", ""))
        source_type = "summary"

    return selected, source_type, truncated

def build_prompt(title, source, analysis_input, source_type):
    """æ„å»ºæ›´å¼ºè°ƒæ·±åº¦åˆ†æçš„æç¤ºè¯"""
    # å®šä¹‰å›ºå®šçš„æ ‡ç­¾åˆ†ç±»
    standard_tags = [
        "AI/æœºå™¨å­¦ä¹ ", "ç¼–ç¨‹è¯­è¨€", "Webå¼€å‘", "ç§»åŠ¨å¼€å‘", "DevOps",
        "äº‘è®¡ç®—", "æ•°æ®åº“", "ç½‘ç»œå®‰å…¨", "å¼€æºé¡¹ç›®", "è½¯ä»¶å·¥ç¨‹",
        "ç³»ç»Ÿæ¶æ„", "æ€§èƒ½ä¼˜åŒ–", "æµ‹è¯•", "å·¥å…·", "ç¡¬ä»¶",
        "äº§å“è®¾è®¡", "èŒä¸šå‘å±•", "æŠ€æœ¯è¶‹åŠ¿", "å…¶ä»–"
    ]

    source_hint = "RSSå…¨æ–‡" if source_type == "fulltext" else "RSSæ‘˜è¦"
    return f"""ä½ æ˜¯èµ„æ·±æŠ€æœ¯ç¼–è¾‘ã€‚è¯·åŸºäºä¸‹é¢çš„æ–‡ç« ææ–™ï¼Œè¾“å‡ºç»“æ„åŒ–è§£è¯»ã€‚

æ ‡é¢˜ï¼š{title}
æ¥æºï¼š{source}
ææ–™ç±»å‹ï¼š{source_hint}

æ–‡ç« ææ–™ï¼š
{analysis_input}

è¯·ä¸¥æ ¼è¾“å‡º JSONï¼ˆä¸è¦ markdown ä»£ç å—ï¼‰ï¼š
{{
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "summary": "1-2å¥è¯æ€»ç»“æ ¸å¿ƒç»“è®ºï¼ˆä¸­æ–‡ï¼‰",
  "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
  "analysis": ["åˆ†æ1", "åˆ†æ2", "åˆ†æ3"]
}}

è¦æ±‚ï¼š
1. tagsï¼šåªèƒ½ä»ä»¥ä¸‹æ ‡ç­¾ä¸­é€‰æ‹© 1-2 ä¸ªæœ€ç›¸å…³æ ‡ç­¾ï¼š
   {', '.join(standard_tags)}
2. summaryï¼šå›ç­”â€œè¿™ç¯‡æ–‡ç« æœ€æ ¸å¿ƒè®²äº†ä»€ä¹ˆâ€ï¼Œä¸è¦å†™æ³›æ³›ç©ºè¯ã€‚
3. key_pointsï¼šç»™ 3 æ¡å…³é”®äº‹å®/è§‚ç‚¹ï¼›å¿…é¡»ä¸ summary æ˜æ˜¾ä¸åŒï¼Œä¸èƒ½åŒä¹‰æ”¹å†™ã€‚
4. analysisï¼šç»™ 2-3 æ¡â€œä¸ºä»€ä¹ˆé‡è¦/æ½œåœ¨å½±å“/å®è·µå»ºè®®â€çš„åˆ†æï¼Œå¯åˆç†æ¨æ–­ä½†ä¸å¾—ç¼–é€ åŸæ–‡ä¸å­˜åœ¨çš„å…·ä½“äº‹å®ã€‚
5. è‹¥ææ–™è¾ƒçŸ­æˆ–ä»…æ‘˜è¦ï¼Œanalysis å…è®¸å‡å°‘åˆ° 1-2 æ¡ï¼Œå¹¶æ˜ç¡®ä½¿ç”¨è°¨æ…è¡¨è¿°ã€‚
6. æ¯æ¡è¦ç‚¹å°½é‡æ§åˆ¶åœ¨ 18-40 å­—ã€‚
"""

def sanitize_list(items, max_items=3):
    """æ¸…ç†æ¨¡å‹è¾“å‡ºåˆ—è¡¨ï¼Œé¿å…è„æ•°æ®æ±¡æŸ“é¡µé¢"""
    if isinstance(items, str):
        items = [items]
    if not isinstance(items, list):
        return []

    cleaned = []
    seen = set()
    for item in items:
        text = normalize_text(item)
        if not text:
            continue
        # æ¸…ç†å¸¸è§é¡¹ç›®ç¬¦å·ï¼Œä¿æŒå±•ç¤ºä¸€è‡´
        text = re.sub(r"^[\-\*\d\.\)\s]+", "", text).strip()
        key = text.casefold()
        if not text or key in seen:
            continue
        seen.add(key)
        cleaned.append(text[:120])
        if len(cleaned) >= max_items:
            break
    return cleaned

def sanitize_enhanced_result(parsed, source_type, input_chars, truncated):
    """è§„èŒƒåŒ– AI è¾“å‡ºç»“æ„"""
    if not isinstance(parsed, dict):
        return None

    tags = sanitize_list(parsed.get("tags"), max_items=2)
    summary = normalize_text(parsed.get("summary"))[:220]
    key_points = sanitize_list(parsed.get("key_points"), max_items=3)
    analysis = sanitize_list(parsed.get("analysis"), max_items=3)

    if not summary:
        return None

    return {
        "tags": tags,
        "summary": summary,
        "key_points": key_points,
        "analysis": analysis,
        "analysis_source": source_type,
        "analysis_input_chars": input_chars,
        "analysis_truncated": bool(truncated),
    }

def enhance_article(article):
    """ä¸ºå•ç¯‡æ–‡ç« æ·»åŠ  AI å¢å¼ºå†…å®¹"""
    title = article.get('title', '')
    source = article.get('source', 'Unknown')
    analysis_input, source_type, truncated = build_analysis_input(article)
    prompt = build_prompt(title, source, analysis_input, source_type)

    print(f"Processing: {title[:50]}... [{source_type}, {len(analysis_input)} chars]")
    result = call_deepseek_api(prompt, max_tokens=900)

    if result:
        try:
            # æ¸…ç† markdown ä»£ç å—
            cleaned = result.strip()
            if cleaned.startswith('```'):
                lines = cleaned.split('\n')
                json_start = -1
                json_end = -1
                for i, line in enumerate(lines):
                    if '{' in line and json_start == -1:
                        json_start = i
                    if '}' in line:
                        json_end = i

                if json_start != -1 and json_end != -1:
                    cleaned = '\n'.join(lines[json_start:json_end+1])

            # è§£æ JSON å¹¶è§„èŒƒåŒ–ç»“æœ
            parsed = json.loads(cleaned)
            enhanced = sanitize_enhanced_result(
                parsed,
                source_type=source_type,
                input_chars=len(analysis_input),
                truncated=truncated,
            )
            if enhanced:
                print(f"âœ“ Successfully enhanced")
                return enhanced
            print("âœ— Invalid structure after sanitize")
            return None
        except Exception as e:
            print(f"âœ— Failed to parse: {e}")
            return None

    return None

def load_articles():
    """åŠ è½½æ‰€æœ‰æ–‡ç« """
    articles_dir = Path(__file__).parent.parent / "data" / "articles"
    all_articles = []

    if not articles_dir.exists():
        return []

    for json_file in sorted(articles_dir.glob("*.json")):
        with open(json_file, 'r', encoding='utf-8') as f:
            articles = json.load(f)
            all_articles.extend(articles)

    return all_articles

def load_processed_hashes():
    """åŠ è½½å·²å¤„ç†çš„æ–‡ç« å“ˆå¸Œåˆ—è¡¨ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰"""
    progress_file = Path(__file__).parent.parent / "data" / "ai_processed.txt"
    if progress_file.exists():
        with open(progress_file, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_processed_hash(url_hash):
    """ä¿å­˜å·²å¤„ç†çš„æ–‡ç« å“ˆå¸Œ"""
    progress_file = Path(__file__).parent.parent / "data" / "ai_processed.txt"
    with open(progress_file, 'a', encoding='utf-8') as f:
        f.write(f"{url_hash}\n")

def save_enhanced_articles(articles):
    """ä¿å­˜å¢å¼ºåçš„æ–‡ç« """
    output_file = Path(__file__).parent.parent / "data" / "articles_enhanced.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ… Saved enhanced articles to: {output_file}")

if __name__ == "__main__":
    print("ğŸ¤– SignalFeed AI Enhancement - Starting...")

    if not DEEPSEEK_API_KEY:
        print("âŒ Error: DEEPSEEK_API_KEY environment variable not set")
        print("Please set it with: export DEEPSEEK_API_KEY='your-api-key'")
        exit(1)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    batch_size = 20  # æ¯æ‰¹å¤„ç†çš„æ–‡ç« æ•°
    if len(sys.argv) > 1:
        try:
            batch_size = int(sys.argv[1])
        except:
            print(f"Invalid batch size, using default: {batch_size}")

    # åŠ è½½æ–‡ç« 
    articles = load_articles()
    print(f"ğŸ“Š Loaded {len(articles)} articles")

    # åŠ è½½å·²å¤„ç†çš„æ–‡ç« 
    processed_hashes = load_processed_hashes()
    print(f"ğŸ“ Already processed: {len(processed_hashes)} articles")

    # ç­›é€‰æœªå¤„ç†çš„æ–‡ç« 
    unprocessed = [a for a in articles if a.get('url_hash') not in processed_hashes]
    print(f"ğŸ”„ To process: {len(unprocessed)} articles")

    if not unprocessed:
        print("âœ… All articles already processed!")
        exit(0)

    # æ‰¹é‡å¤„ç†
    total = len(unprocessed)
    batch_to_process = unprocessed[:batch_size]
    print(f"\nğŸ”„ Processing batch: {len(batch_to_process)} articles")
    print(f"   Remaining after this batch: {total - len(batch_to_process)}")

    # åŠ è½½ç°æœ‰çš„å¢å¼ºæ–‡ç« ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    enhanced_file = Path(__file__).parent.parent / "data" / "articles_enhanced.json"
    if enhanced_file.exists():
        with open(enhanced_file, 'r', encoding='utf-8') as f:
            all_enhanced = json.load(f)
        print(f"ğŸ“‚ Loaded {len(all_enhanced)} existing enhanced articles")
    else:
        all_enhanced = []

    # åˆ›å»ºå“ˆå¸Œåˆ°æ–‡ç« çš„æ˜ å°„
    enhanced_map = {a.get('url_hash'): a for a in all_enhanced if a.get('url_hash')}

    # å¤„ç†å½“å‰æ‰¹æ¬¡
    success_count = 0
    for i, article in enumerate(batch_to_process, 1):
        print(f"\n[{i}/{len(batch_to_process)}] ", end='')
        enhanced = enhance_article(article)

        if enhanced:
            article['ai_enhanced'] = enhanced
            enhanced_map[article['url_hash']] = article
            save_processed_hash(article['url_hash'])
            success_count += 1
        else:
            # å³ä½¿å¤±è´¥ä¹Ÿæ·»åŠ åˆ°æ˜ å°„ä¸­ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
            enhanced_map[article['url_hash']] = article
            save_processed_hash(article['url_hash'])

        # é¿å… API é™æµ
        time.sleep(1.5)

    # ä¿å­˜æ‰€æœ‰å¢å¼ºåçš„æ–‡ç« 
    all_enhanced = list(enhanced_map.values())
    save_enhanced_articles(all_enhanced)

    print(f"\nâœ… Batch complete!")
    print(f"   Successfully enhanced: {success_count}/{len(batch_to_process)}")
    print(f"   Total enhanced articles: {len(all_enhanced)}")
    print(f"   Remaining to process: {total - len(batch_to_process)}")

    if total > len(batch_to_process):
        print(f"\nğŸ’¡ Run again to process the next batch")
