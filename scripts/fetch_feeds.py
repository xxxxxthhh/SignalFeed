#!/usr/bin/env python3
"""
SignalFeed - RSS æŠ“å–è„šæœ¬
æ¯å¤©å®šæ—¶æŠ“å– RSS è®¢é˜…æºï¼Œä¿å­˜æ–‡ç« æ•°æ®
"""

import urllib.request
import xml.etree.ElementTree as ET
import json
import hashlib
from datetime import datetime
from pathlib import Path
import time
import re

def parse_rss_feed(url):
    """è§£æ RSS/Atom è®¢é˜…æº"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 SignalFeed/1.0'}
        req = urllib.request.Request(url, headers=headers)

        with urllib.request.urlopen(req, timeout=10) as response:
            content = response.read()

        root = ET.fromstring(content)

        # å°è¯•è§£æ RSS 2.0
        if root.tag == 'rss' or root.find('channel') is not None:
            return parse_rss_2_0(root, url)
        # å°è¯•è§£æ Atom
        elif 'atom' in root.tag.lower():
            return parse_atom(root, url)
        else:
            return None

    except Exception as e:
        print(f"Error parsing {url}: {e}")
        return None

def parse_rss_2_0(root, url):
    """è§£æ RSS 2.0 æ ¼å¼"""
    channel = root.find('channel')
    if channel is None:
        return None

    feed_title = channel.find('title')
    feed_title = feed_title.text if feed_title is not None else 'Unknown'

    items = []
    for item in channel.findall('item')[:1]:  # æ¯ä¸ªæºåªæŠ“å–æœ€æ–° 1 ç¯‡
        title = item.find('title')
        link = item.find('link')
        description = item.find('description')
        pub_date = item.find('pubDate')

        # ä¼˜å…ˆè¯»å– content:encoded å­—æ®µï¼ˆå…¨æ–‡ï¼‰
        content_encoded = item.find('{http://purl.org/rss/1.0/modules/content/}encoded')

        # ç¡®å®šå†…å®¹å’Œæ˜¯å¦ä¸ºå…¨æ–‡
        if content_encoded is not None and content_encoded.text:
            content = content_encoded.text
            is_fulltext = True
        elif description is not None and description.text:
            content = description.text
            is_fulltext = len(description.text) > 1000  # è¶…è¿‡1000å­—ç¬¦è®¤ä¸ºæ˜¯å…¨æ–‡
        else:
            content = ''
            is_fulltext = False

        items.append({
            'title': title.text if title is not None else 'No Title',
            'link': link.text if link is not None else '',
            'content': content,
            'description': description.text if description is not None else '',
            'is_fulltext': is_fulltext,
            'pub_date': pub_date.text if pub_date is not None else ''
        })

    return {
        'feed_title': feed_title,
        'url': url,
        'items': items
    }

def parse_atom(root, url):
    """è§£æ Atom æ ¼å¼"""
    ns = {'atom': 'http://www.w3.org/2005/Atom'}

    feed_title = root.find('atom:title', ns)
    if feed_title is None:
        feed_title = root.find('title')
    feed_title = feed_title.text if feed_title is not None else 'Unknown'

    items = []
    entries = root.findall('atom:entry', ns)
    if not entries:
        entries = root.findall('entry')

    for entry in entries[:1]:  # æ¯ä¸ªæºåªæŠ“å–æœ€æ–° 1 ç¯‡
        title = entry.find('atom:title', ns)
        if title is None:
            title = entry.find('title')

        link = entry.find('atom:link', ns)
        if link is None:
            link = entry.find('link')
        link_href = link.get('href') if link is not None else ''

        # ä¼˜å…ˆè¯»å– content å­—æ®µï¼ˆå…¨æ–‡ï¼‰
        content_elem = entry.find('atom:content', ns)
        if content_elem is None:
            content_elem = entry.find('content')

        summary = entry.find('atom:summary', ns)
        if summary is None:
            summary = entry.find('summary')

        updated = entry.find('atom:updated', ns)
        if updated is None:
            updated = entry.find('updated')

        # ç¡®å®šå†…å®¹å’Œæ˜¯å¦ä¸ºå…¨æ–‡
        if content_elem is not None and content_elem.text:
            content = content_elem.text
            is_fulltext = True
        elif summary is not None and summary.text:
            content = summary.text
            is_fulltext = len(summary.text) > 1000  # è¶…è¿‡1000å­—ç¬¦è®¤ä¸ºæ˜¯å…¨æ–‡
        else:
            content = ''
            is_fulltext = False

        items.append({
            'title': title.text if title is not None else 'No Title',
            'link': link_href,
            'content': content,
            'description': summary.text if summary is not None else '',
            'is_fulltext': is_fulltext,
            'pub_date': updated.text if updated is not None else ''
        })

    return {
        'feed_title': feed_title,
        'url': url,
        'items': items
    }

def clean_html(text):
    """æ¸…ç† HTML æ ‡ç­¾"""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_url_hash(url):
    """ç”Ÿæˆ URL çš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
    return hashlib.md5(url.encode()).hexdigest()

def load_processed_urls():
    """åŠ è½½å·²å¤„ç†çš„ URL"""
    processed_file = Path(__file__).parent.parent / "data" / "processed_urls.txt"
    if processed_file.exists():
        with open(processed_file, 'r') as f:
            return set(line.strip() for line in f)
    return set()

def save_processed_url(url_hash):
    """ä¿å­˜å·²å¤„ç†çš„ URL"""
    processed_file = Path(__file__).parent.parent / "data" / "processed_urls.txt"
    with open(processed_file, 'a') as f:
        f.write(f"{url_hash}\n")

if __name__ == "__main__":
    print("ğŸš€ SignalFeed - Starting RSS fetch...")

    # åŠ è½½è®¢é˜…æº
    feeds_file = Path(__file__).parent.parent / "data" / "feeds.json"
    with open(feeds_file) as f:
        feeds = json.load(f)

    print(f"ğŸ“¡ Fetching {len(feeds)} feeds...")

    # åŠ è½½å·²å¤„ç†çš„ URL
    processed_urls = load_processed_urls()
    print(f"ğŸ“ Already processed: {len(processed_urls)} articles")

    # æŠ“å–æ‰€æœ‰è®¢é˜…æº
    all_articles = []
    new_count = 0

    for i, feed_url in enumerate(feeds, 1):
        print(f"[{i}/{len(feeds)}] Fetching {feed_url[:50]}...")
        result = parse_rss_feed(feed_url)

        if result and result['items']:
            for item in result['items']:
                url_hash = get_url_hash(item['link'])

                # å»é‡æ£€æŸ¥
                if url_hash not in processed_urls:
                    # ä½¿ç”¨å®Œæ•´å†…å®¹ï¼Œä¸å†é™åˆ¶å­—ç¬¦æ•°
                    content = item.get('content', item.get('description', ''))

                    article = {
                        'title': clean_html(item['title']),
                        'link': item['link'],
                        'content': content,  # å®Œæ•´å†…å®¹ï¼ˆå¯èƒ½åŒ…å« HTMLï¼‰
                        'description': clean_html(item.get('description', ''))[:500],  # çº¯æ–‡æœ¬æ‘˜è¦ï¼Œç”¨äºé¢„è§ˆ
                        'is_fulltext': item.get('is_fulltext', False),
                        'source': result['feed_title'],
                        'pub_date': item['pub_date'],
                        'fetched_at': datetime.now().isoformat(),
                        'url_hash': url_hash
                    }
                    all_articles.append(article)
                    processed_urls.add(url_hash)
                    save_processed_url(url_hash)
                    new_count += 1

        time.sleep(0.3)  # é¿å…è¯·æ±‚è¿‡å¿«

    print(f"\nâœ… Fetch complete!")
    print(f"ğŸ“Š New articles: {new_count}")
    print(f"ğŸ“Š Total processed: {len(processed_urls)}")

    # ä¿å­˜æ–‡ç« æ•°æ®
    if all_articles:
        date_str = datetime.now().strftime('%Y-%m-%d')
        output_file = Path(__file__).parent.parent / "data" / "articles" / f"{date_str}.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Saved to: {output_file}")
    else:
        print("â„¹ï¸  No new articles to save")
