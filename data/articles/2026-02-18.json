[
  {
    "title": "Taking Our Minds for Granted",
    "link": "https://idiallo.com/blog/taking-our-mind-for-granted?src=feed",
    "content": "\n\t\t\t\t\t\n\t\t\t<p>How did we do it before ChatGPT? How did we write full sentences, connect ideas into a coherent arc, solve problems that had no obvious answer? We thought. That's it. We simply sat with discomfort long enough for something to emerge.</p>\n\t\t\t<p>I find this fascinating. You have a problem, so you sit down and think until you find a solution. Sometimes you're not even sitting down. You go for a walk, and your mind quietly wrestles with the idea while your feet carry you nowhere in particular. A solution emerges not because you forced it, but because you thought it through. What happened in that moment is remarkable: new information was created from the collision of existing ideas inside your head. No prompt. No query. Just you.</p>\n\n<p>I remember the hours I used to spend debugging a particularly stubborn problem at work. I would stare at the screen, type a few keystrokes, then delete them. I'd meet with our lead engineer and we would talk in circles. At home, I would lie in bed still turning the problem over. And then one night, somewhere around 3 a.m., I dreamt I was running the compiler, making a small change, watching it build, and suddenly it worked. I woke up knowing the answer before I had even tested it. I had to wait until morning to confirm what my sleeping mind had already solved.</p>\n\n<p>That's the mind doing what it was built to do.</p>\n\n<p>Writers know this feeling too. A sentence that won't cooperate in the afternoon sometimes writes itself during a morning shower. Scientists have described waking up with the solution to a problem they fell asleep wrestling with. Mendeleev wrote in his dairy that he saw <a href=\"https://www.themarginalian.org/2016/02/08/mendeleev-periodic-table-dream/\">the periodic table in a dream</a>. The  mind that keeps working when we stop forcing it.</p>\n\n<p>The mind can generate new ideas from its own reflection, something we routinely accuse large language models of being incapable of. LLMs recombine what already exists; the human mind makes unexpected leaps. But increasingly, it feels as though we are outsourcing those leaps before we ever attempt them. Why sit with a half-formed thought when you can just ask? Why let an idea marinate when a tool can hand you something polished in seconds?</p>\n\n<pre><code class=\"code-lang-default\">if (Math.random() &gt; .4) {\n    take_a_leap()\n}\n</code></pre>\n\n<p>The risk isn't that AI makes us lazy. It's that we slowly forget what it felt like to think hard, and stop believing we're capable of it. It's like forgetting how to do long division because you've always had a calculator in your pocket. </p>\n\n<p>The mind is like any muscle. Leave it unstrained and it weakens. Push it and it grows. The best ideas you will ever have are still inside you, waiting for the particular silence that only comes when you stop reaching for your phone.</p>\n\n<p>In the age of AI, the most radical thing you can do might simply be to think.</p>\n\t\t\t\n\t\t\t\t",
    "description": "How did we do it before ChatGPT? How did we write full sentences, connect ideas into a coherent arc, solve problems that had no obvious answer? We thought. That's it. We simply sat with discomfort long enough for something to emerge. I find this fascinating. You have a problem, so you sit down and think until you find a solution. Sometimes you're not even sitting down. You go for a walk, and your mind quietly wrestles with the idea while your feet carry you nowhere in particular. A solution emer",
    "is_fulltext": true,
    "source": "iDiallo.com",
    "pub_date": "Wed, 18 Feb 2026 12:00:00 GMT",
    "fetched_at": "2026-02-18T12:21:28.874136",
    "url_hash": "fb17494b432680ed1feffd957046773e"
  },
  {
    "title": "Two challenges of incremental backups",
    "link": "https://utcc.utoronto.ca/~cks/space/blog/tech/IncrementalBackupsTwoChallenges",
    "content": "<div class=\"wikitext\"><p>Roughly speaking, there are two sorts of backups that you can make,\n<em>full backups</em> and <em>incremental backups</em>. At the abstract level,\nfull backups are pretty simple; you save everything that you find.\nIncremental backups are more complicated because they save only the\nthings that changed since whatever they're relative to. People want\nincremental backups despite the extra complexity because they save\na lot of space compared to backing up everything all the time.</p>\n\n<p>There are two general challenges that make incremental backups more\ncomplicated than full backups. The first challenge is reliably\nfinding everything that's changed, in the face of all of the stuff\nthat can change in filesystems (or other sources of data). Full\nbackups only need to be able to traverse all of the filesystem (or\npart of it), or in general the data source, and this is almost\nalways a reliable thing because all sorts of things and people use\nit. Finding everything that has changed has historically been more\nchallenging because it's not something that people do often outside\nof incremental backups.</p>\n\n<p>(And when people do it they may not notice if they're missing some\nthings, the way they absolutely will notice if a general traversal\nskips some files.)</p>\n\n<p>The second challenge is handling things that have gone away. Once\nyou have a way to find everything that's changed it's not too\ndifficult to build a backup system that will faithfully reproduce\neverything that definitely was there as of the incremental. All you\nneed to do is save every changed file and then unpack the sequence\nof full and incremental backups on top of each other, with the\nlatest version of any particular file overwriting any previous one.\nBut people often want their incremental restore to reflect the state\nof directories and so on as of the incremental, which means removing\nthings that have been deleted (both files and perhaps entire directory\ntrees). This means that your incrementals need some way to pass on\ninformation about things that were there in earlier backups but\naren't there now, so that the restore process can either not restore\nthem or remove them as it restores the sequence of full and incremental\nbackups.</p>\n\n<p>While there are a variety of ways to tackle the first challenge,\nbackup systems that want to run quickly are often constrained by\nwhat features operating systems offer (and also what features your\nbackup system thinks it can trust, which isn't always the same\nthing). You can checksum everything all the time and keep a checksum\ndatabase, but that's usually not going to be the fastest thing.\nThe second challenge is much less constrained by what the operating\nsystem provides, which means that in practice it's much more on you\n(the backup system) to come up with a good solution. Your choice\nof solution may interact with how you solve the first challenge,\nand there are tradeoffs in various approaches you can pick (for\nexample, do you represent deletions explicitly in the backup format\nor are they implicit in various ways).</p>\n\n<p>There is no single right answer to these challenges. I'll go as far\nas to say that the answer depends partly on what sort of data and\nchanges you expect to see in the backups and partly where you want\nto put the costs between creating backups and handling restores.</p>\n</div>\n<div> (<a href=\"https://utcc.utoronto.ca/~cks/space/blog/tech/IncrementalBackupsTwoChallenges?showcomments#comments\">One comment</a>.) </div>",
    "description": "",
    "is_fulltext": true,
    "source": "Chris's Wiki :: blog",
    "pub_date": "2026-02-18T04:26:13Z",
    "fetched_at": "2026-02-18T12:21:34.587020",
    "url_hash": "974381b9d38ba022b28742dd2189741b"
  },
  {
    "title": "The case for gatekeeping, or: why medieval guilds had it figured out",
    "link": "https://www.joanwestenberg.com/the-case-for-gatekeeping-or-why-medieval-guilds-had-it-figured-out/",
    "content": "<img src=\"https://www.joanwestenberg.com/content/images/2026/02/ChatGPT-Image-Feb-18--2026--01_15_25-PM.png\" alt=\"The case for gatekeeping, or: why medieval guilds had it figured out\"><p>Every open source maintainer I&apos;ve talked to in the last six months has the same complaint: the absolute flood of mass-produced, AI-generated, mass-submitted slop requests have turned their repositories into a slush pile. The contributions <em>look</em> like contributions, they have commit messages, they reference issues and they follow templates etc.</p><p>But they are, almost uniformly, garbage.</p><p>A high PR count on a repository used to actually mean something. If strangers were showing up to fix your edge cases, you&apos;d built something people cared about. Now a high PR count signals that your repo has become a target for resume-padding bots, grifters and AI-assisted contribution farmers who need their GitHub activity graph to glow green for recruiter eyeballs or just want to swamp a project in pursuit of vulnerabilities. Open source, in other words, has an open slop problem.</p><p>And I think the solution is one that would&apos;ve been perfectly obvious to a thirteenth-century Florentine weaver.</p><h2 id=\"the-guild-system-solved-exactly-this-problem\">The guild system solved exactly this problem</h2><p>The medieval guild system gets a bad rap. It&apos;s usually remembered as a protectionist racket // a cartel of craftspeople colluding to keep prices high and competition low. And that critique isn&apos;t entirely wrong. The guilds did restrict entry. They did maintain monopolies. Adam Smith hated them, and he had reasons.</p><p>But the guilds also solved a problem: how do you maintain quality standards in a decentralized production environment when you can&apos;t personally verify every participant?</p><p>A master weaver in the Arte della Lana couldn&apos;t inspect every bolt of cloth produced in Florence. But he <em>could</em> verify that the person producing it had spent years as an apprentice, passed through the journeyman stage, and demonstrated competence to other masters who staked their own reputations on the assessment. The guild was, at bottom, a web of trust backed by skin in the game. You vouched for people. If they turned out to be frauds, you were fucked, too.</p><p>The open source ecosystem used to have something like this, but it was organic. You&apos;d show up on a mailing list. You&apos;d lurk. You&apos;d file a good bug report. You&apos;d submit a small patch and wait. Over time, established contributors would come to recognize your handle and your judgment. You&apos;d build a reputation the slow way, through repeated interactions with people who were paying attention. Linus Torvalds didn&apos;t need a credentialing system for the Linux kernel because the community was small enough, and engaged enough, that trust emerged from the social fabric itself.</p><p>That fabric is shredded now.</p><h2 id=\"what-open-was-supposed-to-mean\">What &quot;open&quot; was supposed to mean</h2><p>Richard Stallman&apos;s vision for free software was rooted in an ethical claim about user freedom. When Stallman argued that software should be free, he meant free as in speech: users should be able to study, modify, and redistribute the code that runs their lives. The model that Eric Raymond championed in <em>The Cathedral and the Bazaar</em> added the empirical claim &quot;many eyes make all bugs shallow,&quot; but even Raymond assumed those eyes belonged to people who could actually see.</p><p>The &quot;open&quot; in open source was always about access to code, not the abolition of all quality filters on human participation. But the culture developed an allergy to gatekeeping so severe that suggesting contributors should meet any bar at all became politically radioactive. And that allergy made perfect sense when the failure mode was &quot;talented person gets excluded by arbitrary social dynamics.&quot; It makes considerably less sense when the failure mode is &quot;thousands of LLM-generated PRs that change variable names to slightly worse variable names fuck absolutely everything for absolutely everyone.&quot;</p><h2 id=\"what-a-modern-guild-would-actually-look-like\">What a modern guild would actually look like</h2><p>We need a verified not-shit-person badge. Some mechanism, ideally decentralized, ideally reputation-based, that lets maintainers distinguish between &quot;human who has demonstrated basic competence and good faith&quot; and &quot;entity or bot submitting or causing to be submitted auto-generated changes to mass repositories for credential farming.&quot;</p><p>This is, functionally, <em>a guild</em>. And before the libertarian-leaning contingent of Hacker News has a collective aneurysm, let me be specific about what I mean:</p><p>I don&apos;t mean you need a certificate to write Python. I mean something closer to what the Debian project has done with its Web of Trust model for decades: existing trusted contributors vouch for new ones. Your vouching carries weight proportional to your own standing. If you vouch for someone who turns out to be a spam vector, that costs you something. The system works because it makes reputation legible without making it bureaucratic.</p><p>You could imagine this layered onto GitHub or GitLab with relatively modest infrastructure. Contributor rings, where the inner rings are people vouched for by other inner-ring people. Maintainers could then filter PRs by trust level. Not blocking anyone from forking or submitting, but giving maintainers a signal they desperately need.</p><p>Chaucer&apos;s pilgrims each carried letters of introduction from their parishes; the principle is old enough that it shows up in <em>The Canterbury Tales</em> as an assumed feature of civilized travel.</p><p>TL:DR: Every mass-generated PR a maintainer has to review is time stolen from actual development. Every fake contribution that gets merged degrades the codebase. Every green-square farmer who pads their profile with AI-generated commits makes the GitHub contribution graph less useful as a signal, which ironically makes the farming less valuable too, which means they need to do more of it.</p><p>Would a guild system be perfect? Obviously not. Would it create new forms of exclusion? Probably. Would medieval Florentine weavers recognize the problem we&apos;re dealing with? I suspect they&apos;d find it eerily familiar.</p><p>And there is no need // reason to re-invent the wheel.</p>",
    "description": "Every open source maintainer I&apos;ve talked to in the last six months has the same complaint: the absolute flood of mass-produced, AI-generated, mass-submitted slop requests have turned their repositories into a slush pile. The contributions look like contributions, they have commit messages, they reference issues and they follow",
    "is_fulltext": true,
    "source": "Westenberg.",
    "pub_date": "Wed, 18 Feb 2026 02:21:09 GMT",
    "fetched_at": "2026-02-18T12:21:54.908792",
    "url_hash": "6351306af2019140698b77dcfc2fb688"
  },
  {
    "title": "What Package Registries Could Borrow from OCI",
    "link": "https://nesbitt.io/2026/02/18/what-package-registries-could-borrow-from-oci.html",
    "content": "<p>Every package manager ships code as an archive, and every one of them has a slightly different way to do it. npm wraps tarballs in a <code class=\"language-plaintext highlighter-rouge\">package/</code> directory prefix. RubyGems nests gzipped files inside an uncompressed tar. Alpine concatenates three gzip streams and calls it a package. Python cycled through four distribution formats in twenty years. RPM used cpio as its payload format for nearly three decades before finally dropping it in 2025.</p>\n\n<p>Meanwhile, the container world converged on a single format: OCI, the Open Container Initiative spec. And over the past few years, OCI registries have quietly started storing things that aren’t containers at all: Helm charts, Homebrew bottles, WebAssembly modules, AI models. The format was designed for container images, but the underlying primitives turn out to be general enough that it’s worth asking whether every package manager could use OCI for distribution.</p>\n\n<h3 id=\"what-oci-actually-is\">What OCI actually is</h3>\n\n<p>OCI defines three specifications: a Runtime Spec (how to run containers), an Image Spec (how to describe container contents), and a Distribution Spec (how to push and pull from registries).</p>\n\n<p>At the storage level, an OCI registry deals in two primitives: <strong>manifests</strong> and <strong>blobs</strong>. A manifest is a JSON document that references one or more blobs by their SHA-256 digest. A blob is an opaque chunk of binary content, and tags are human-readable names that point to manifests.</p>\n\n<p>A container image manifest looks like this:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"schemaVersion\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"mediaType\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"application/vnd.oci.image.manifest.v1+json\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"config\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"mediaType\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"application/vnd.oci.image.config.v1+json\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"digest\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"sha256:abc123...\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1234</span><span class=\"w\">\n  </span><span class=\"p\">},</span><span class=\"w\">\n  </span><span class=\"nl\">\"layers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"mediaType\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"application/vnd.oci.image.layer.v1.tar+gzip\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"digest\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"sha256:def456...\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">56789</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">]</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The config blob holds metadata (what OS, what architecture, what environment variables). Each layer blob holds a tarball of filesystem changes. The registry doesn’t care what’s inside the blobs, only that each one is identified and verified by its digest.</p>\n\n<p>The <a href=\"https://opencontainers.org/posts/blog/2024-03-13-image-and-distribution-1-1/\">v1.1 update</a> in February 2024 added <code class=\"language-plaintext highlighter-rouge\">artifactType</code>, which declares what kind of thing a manifest describes so a registry can distinguish a Helm chart from a container image from a Homebrew bottle, and <code class=\"language-plaintext highlighter-rouge\">subject</code>, which lets one artifact reference another and is how signatures and SBOMs get attached to the thing they describe. Before 1.1, people stored non-container artifacts by setting custom media types on the config blob, which worked but registries sometimes rejected or mishandled the results.</p>\n\n<p>To push an artifact, you upload each blob (to <code class=\"language-plaintext highlighter-rouge\">/v2/&lt;name&gt;/blobs/uploads/</code>), then push a manifest that references those blobs by digest and size. To pull, you fetch the manifest, read the digests, and download the blobs. Because everything is addressed by digest, the registry only stores one copy of any given blob even if multiple artifacts reference it.</p>\n\n<h3 id=\"why-oci-and-not-something-purpose-built\">Why OCI and not something purpose-built</h3>\n\n<p>The format itself carries a lot of container-specific ceremony, but every major cloud provider already runs an OCI-compliant registry: GitHub Container Registry, Amazon ECR, Azure Container Registry, Google Artifact Registry. Self-hosted options like Harbor and Zot are mature. Authentication, access control, replication, and CDN-backed blob storage all exist because container registries already solved those problems at scale, and a package registry built on OCI inherits all of it without reimplementing any of it.</p>\n\n<p><a href=\"https://oras.land/\">ORAS</a> (OCI Registry As Storage) is a CNCF project that abstracts the multi-step OCI upload process into simple commands:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>oras push registry.example.com/mypackage:1.0.0 \\\n  package.tar.gz:application/vnd.example.package.v1.tar+gzip\n</code></pre></div></div>\n\n<p>This uploads the file as a blob, creates a manifest referencing it, and tags it. Helm, Flux, Crossplane, and the Sigstore signing tools all use ORAS or the underlying OCI client libraries.</p>\n\n<h3 id=\"what-package-managers-ship-today\">What package managers ship today</h3>\n\n<p>No individual choice here is wrong, but seventeen different answers to the same basic problem suggests the archive format was never the part anyone thought hard about.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Ecosystem</th>\n      <th>Format</th>\n      <th>What’s inside</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>npm</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.tgz</code> (gzip tar)</td>\n      <td>Files under a <code class=\"language-plaintext highlighter-rouge\">package/</code> prefix</td>\n    </tr>\n    <tr>\n      <td>PyPI</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.whl</code> (zip) or <code class=\"language-plaintext highlighter-rouge\">.tar.gz</code></td>\n      <td>Wheel: pre-built files + <code class=\"language-plaintext highlighter-rouge\">.dist-info</code>. Sdist: source + <code class=\"language-plaintext highlighter-rouge\">PKG-INFO</code></td>\n    </tr>\n    <tr>\n      <td>RubyGems</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.gem</code> (tar of gzips)</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">metadata.gz</code> + <code class=\"language-plaintext highlighter-rouge\">data.tar.gz</code> + <code class=\"language-plaintext highlighter-rouge\">checksums.yaml.gz</code></td>\n    </tr>\n    <tr>\n      <td>Maven</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.jar</code> (zip)</td>\n      <td>Compiled <code class=\"language-plaintext highlighter-rouge\">.class</code> files + <code class=\"language-plaintext highlighter-rouge\">META-INF/MANIFEST.MF</code></td>\n    </tr>\n    <tr>\n      <td>Cargo</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.crate</code> (gzip tar)</td>\n      <td>Source + <code class=\"language-plaintext highlighter-rouge\">Cargo.toml</code> + <code class=\"language-plaintext highlighter-rouge\">Cargo.lock</code></td>\n    </tr>\n    <tr>\n      <td>NuGet</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.nupkg</code> (zip)</td>\n      <td>DLL assemblies + <code class=\"language-plaintext highlighter-rouge\">.nuspec</code> XML metadata</td>\n    </tr>\n    <tr>\n      <td>Homebrew</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.bottle.tar.gz</code></td>\n      <td>Compiled binaries under install prefix</td>\n    </tr>\n    <tr>\n      <td>Go</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.zip</code></td>\n      <td>Source under <code class=\"language-plaintext highlighter-rouge\">module@version/</code> path prefix</td>\n    </tr>\n    <tr>\n      <td>Hex</td>\n      <td>Outer tar of inner files</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">VERSION</code> + <code class=\"language-plaintext highlighter-rouge\">metadata.config</code> + <code class=\"language-plaintext highlighter-rouge\">contents.tar.gz</code> + <code class=\"language-plaintext highlighter-rouge\">CHECKSUM</code></td>\n    </tr>\n    <tr>\n      <td>Debian</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.deb</code> (ar archive)</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">debian-binary</code> + <code class=\"language-plaintext highlighter-rouge\">control.tar.*</code> + <code class=\"language-plaintext highlighter-rouge\">data.tar.*</code></td>\n    </tr>\n    <tr>\n      <td>RPM</td>\n      <td>Custom binary format</td>\n      <td>Header sections + cpio payload (v4) or custom format (v6)<sup id=\"fnref:rpm5\"><a href=\"#fn:rpm5\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\">1</a></sup></td>\n    </tr>\n    <tr>\n      <td>Alpine</td>\n      <td>Concatenated gzip streams</td>\n      <td>Signature + control tar + data tar</td>\n    </tr>\n    <tr>\n      <td>Conda</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.conda</code> (zip of zstd tars) or <code class=\"language-plaintext highlighter-rouge\">.tar.bz2</code></td>\n      <td><code class=\"language-plaintext highlighter-rouge\">info/</code> metadata + package content</td>\n    </tr>\n    <tr>\n      <td>Dart/pub</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.tar.gz</code></td>\n      <td>Source + <code class=\"language-plaintext highlighter-rouge\">pubspec.yaml</code></td>\n    </tr>\n    <tr>\n      <td>Swift PM</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.zip</code></td>\n      <td>Source archive</td>\n    </tr>\n    <tr>\n      <td>CPAN</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.tar.gz</code></td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.pm</code> files + <code class=\"language-plaintext highlighter-rouge\">Makefile.PL</code> + <code class=\"language-plaintext highlighter-rouge\">META.yml</code> + <code class=\"language-plaintext highlighter-rouge\">MANIFEST</code></td>\n    </tr>\n    <tr>\n      <td>CocoaPods</td>\n      <td>No archive format</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">.podspec</code> points to source URLs</td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"the-weird-ones\">The weird ones</h3>\n\n<p><strong>RubyGems</strong> nests compression inside archiving instead of the other way around. A <code class=\"language-plaintext highlighter-rouge\">.gem</code> is an uncompressed tar containing individually gzipped files. So the outer archive provides no compression, and each component is compressed separately. This means you can extract the metadata without decompressing the data, which is a reasonable optimization, but the format looks strange at first glance because everything else in the Unix world puts gzip on the outside.</p>\n\n<p><strong>Alpine APK</strong> abuses a quirk of the gzip specification. The gzip format allows concatenation of multiple streams into a single file, and technically any compliant decompressor should handle it. Alpine packages are three separate gzip streams (signature, control, data) concatenated into one file. Since gzip provides no metadata about where one stream ends and the next begins, you have to fully decompress each segment to find the boundary. Kernel modules inside APK packages are often already gzipped, so you get gzip-inside-tar-inside-gzip.</p>\n\n<p><strong>RPM</strong> used cpio as its payload format from 1995 until RPM v6 shipped in September 2025. The cpio format has a 4GB file size limit baked into its header fields. For 30 years, no RPM package could contain a file larger than 4GB. RPM v6 finally dropped cpio in favor of a custom format.</p>\n\n<p><strong>Debian</strong> deliberately chose the <code class=\"language-plaintext highlighter-rouge\">ar</code> archive format from the 1970s. The reasoning was practical: the extraction tools (<code class=\"language-plaintext highlighter-rouge\">ar</code>, <code class=\"language-plaintext highlighter-rouge\">tar</code>, <code class=\"language-plaintext highlighter-rouge\">gzip</code>) are available on virtually every Unix system, even in minimal rescue environments. You can unpack a <code class=\"language-plaintext highlighter-rouge\">.deb</code> with nothing but POSIX utilities. Probably the most intentional format choice on this list.</p>\n\n<p><strong>npm’s <code class=\"language-plaintext highlighter-rouge\">package/</code> prefix</strong> means every tarball wraps its contents in a <code class=\"language-plaintext highlighter-rouge\">package/</code> directory that gets stripped during install. This causes issues with relative <code class=\"language-plaintext highlighter-rouge\">file:</code> dependencies inside tarballs, where npm tries to resolve paths relative to the tarball rather than the unpacked directory.</p>\n\n<p><strong>Python</strong> cycled through four distribution formats. Source tarballs with <code class=\"language-plaintext highlighter-rouge\">setup.py</code> (1990s), eggs (2004, inspired by Java JARs, could be imported while still zipped), sdists (standardized tar.gz), and finally wheels (2012). Eggs lived for nineteen years before PyPI stopped accepting them in August 2023. The wheel format encodes Python version, ABI tag, and platform tag in the filename, which is more metadata than most ecosystems put in the filename but less than what goes in the manifest.</p>\n\n<p><strong>Conda</strong> maintained two incompatible formats for years. The legacy <code class=\"language-plaintext highlighter-rouge\">.tar.bz2</code> and the modern <code class=\"language-plaintext highlighter-rouge\">.conda</code> (a zip containing zstandard-compressed tars). The switch from bzip2 to zstandard yielded significant decompression speedups, but every tool in the ecosystem had to support both formats indefinitely.</p>\n\n<p><strong>Hex</strong> (Erlang/Elixir) has two checksum schemes in the same package. The deprecated “inner checksum” hashes concatenated file contents. The current “outer checksum” hashes the entire tarball. Both are present for backward compatibility.</p>\n\n<h3 id=\"whos-already-using-oci\">Who’s already using OCI</h3>\n\n<p>Homebrew is a traditional package manager, not a “cloud-native” tool, and its migration to OCI already happened under pressure.</p>\n\n<p>In February 2021, JFrog <a href=\"https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/\">announced</a> that Bintray would shut down on May 1. Homebrew’s bottles were hosted on Bintray. The maintainers had about three months to move their entire archive of precompiled binaries somewhere else, and they landed on GitHub Packages, which stores everything as OCI blobs on <code class=\"language-plaintext highlighter-rouge\">ghcr.io</code>. <a href=\"https://brew.sh/2021/04/12/homebrew-3.1.0/\">Homebrew 3.1.0</a> shipped April 12, 2021, with GHCR as the default download location.</p>\n\n<p>The transition was rough in the ways you’d expect. CI pipelines across the industry broke because macOS images on services like <a href=\"https://discuss.circleci.com/t/macos-image-users-homebrew-brownout-2021-04-26/39872\">CircleCI</a> shipped with old Homebrew versions that still pointed at Bintray. During a brownout on April 26, any system running an older Homebrew got 502 errors. Older bottle versions were never migrated, so anyone pinned to an old formula version got 404s and had to build from source. The fix was <code class=\"language-plaintext highlighter-rouge\">brew update</code>, but CI environments cached old Homebrew versions and didn’t auto-update.</p>\n\n<p>After the dust settled, the OCI-based storage enabled things that wouldn’t have been practical on Bintray. Homebrew 4.0.0 (February 2023) switched from git-cloned tap metadata to a <a href=\"https://brew.sh/2023/02/16/homebrew-4.0.0/\">JSON API</a> that leverages the structured OCI manifests, and <code class=\"language-plaintext highlighter-rouge\">brew update</code> dropped from running every 5 minutes to every 24 hours.</p>\n\n<p>Manifest-based integrity checking replaced the old checksum approach, though this introduced <a href=\"https://github.com/Homebrew/brew/issues/12300\">its own class of bugs</a> where manifest checksums wouldn’t match. Platform multiplexing came naturally from OCI image indexes, which map platform variants (<code class=\"language-plaintext highlighter-rouge\">arm64_sonoma</code>, <code class=\"language-plaintext highlighter-rouge\">x86_64_linux</code>) to individual manifests without Homebrew having to build that logic itself.</p>\n\n<p>When you run <code class=\"language-plaintext highlighter-rouge\">brew install</code>, the client fetches the OCI image index manifest from <code class=\"language-plaintext highlighter-rouge\">ghcr.io/v2/homebrew/core/&lt;formula&gt;/manifests/&lt;version&gt;</code>, selects the right platform manifest, then HEADs the blob URL to get a 307 redirect to a signed URL on <code class=\"language-plaintext highlighter-rouge\">pkg-containers.githubusercontent.com</code> where Fastly’s CDN serves the actual bytes. GHCR requires a bearer token even for public images, so Homebrew hardcodes <code class=\"language-plaintext highlighter-rouge\">QQ==</code> as the bearer token. The bottle inside the blob is still a gzipped tarball with the same internal structure it always had.</p>\n\n<p>Helm charts followed a similar path. Helm v3.8 added native OCI registry support, and the old <code class=\"language-plaintext highlighter-rouge\">index.yaml</code> repository format is being phased out. Azure CLI retired legacy Helm repository support in September 2025. Charts push with <code class=\"language-plaintext highlighter-rouge\">helm push</code> using <code class=\"language-plaintext highlighter-rouge\">oci://</code> prefixed references, and the chart tarball goes into a layer blob.</p>\n\n<h3 id=\"what-would-change\">What would change</h3>\n\n<p><strong>Platform variants get first-class support.</strong> OCI image indexes map platform descriptors to manifests. A package with builds for five platforms would have an index pointing to five manifests, each pointing to the right blob. This is cleaner than npm’s convention of publishing platform-specific binaries as separate <code class=\"language-plaintext highlighter-rouge\">optionalDependencies</code> packages, or Python’s approach of uploading multiple wheels with platform-encoded filenames and letting pip pick the right one.</p>\n\n<p><strong>Signing and attestation come built in.</strong> Every ecosystem is building its own signing infrastructure independently. npm added <a href=\"https://docs.npmjs.com/generating-provenance-statements\">Sigstore-based provenance</a> in 2023, PyPI added <a href=\"https://docs.pypi.org/attestations/\">attestations</a> in 2024, Cargo has <a href=\"https://github.com/rust-lang/rfcs/pull/3403\">RFC 3403</a> open, and RubyGems has had signature support for years that almost nobody uses because the tooling never reached the point where it was easy enough to be default behavior. Each effort required dedicated engineering time from small registry teams who were already stretched thin.</p>\n\n<p>OCI’s <code class=\"language-plaintext highlighter-rouge\">subject</code> field and referrers API provide a single mechanism for all of this. Cosign and Notation can sign any OCI artifact, storing the signature as a separate artifact in the same registry that references the signed content via <code class=\"language-plaintext highlighter-rouge\">subject</code>. SBOMs attach the same way, as do build provenance attestations, vulnerability scan results, and license audits: push an artifact with <code class=\"language-plaintext highlighter-rouge\">subject</code> pointing to the thing it describes, and any client can discover it through the referrers API.</p>\n\n<p>The security ecosystem around OCI registries (cosign, notation, Kyverno, OPA Gatekeeper, Ratify) represents years of investment that package registries could inherit. A policy engine enforcing “all artifacts must be signed before deployment” wouldn’t care whether it’s looking at a container image or a RubyGem, because the referrers API works the same way for both.</p>\n\n<p><strong>Deduplication and registry sustainability.</strong> Content-addressable storage identifies every blob by its SHA-256 digest, so if two packages contain an identical file the registry stores it once, and if two concurrent uploads push the same blob the registry accepts both but keeps one copy.</p>\n\n<p>Shared content between unrelated source packages is rare, so this matters more for binary packages where the same shared libraries get bundled into Homebrew bottles for different formulas, the same runtime components appear in multiple Conda packages, and Debian’s archive carries the same <code class=\"language-plaintext highlighter-rouge\">.so</code> files across dozens of packages and versions.</p>\n\n<p>The community-funded registries are where this adds up. rubygems.org, crates.io, PyPI, and hex.pm run on bandwidth donated by CDN providers, primarily Fastly. These registries serve terabytes of package data to millions of developers on infrastructure that someone is volunteering to cover.</p>\n\n<p>Content-addressable storage won’t eliminate those costs, but a registry that’s been running for ten years has accumulated a lot of identical blobs that a content-addressable backend would collapse into single copies, and the savings compound as the registry grows.</p>\n\n<p><strong>Content-addressed mirroring.</strong> Mirroring a package registry today requires reimplementing each registry’s API and storage format, and every ecosystem’s mirror implementation is different: the Simple Repository API for PyPI, the registry API for npm, the compact index for RubyGems. Anyone can stand up an OCI-compliant mirror with off-the-shelf software like Harbor, Zot, or the CNCF Distribution project, which is a much lower bar than reverse-engineering a bespoke registry protocol.</p>\n\n<p>Content-addressable storage changes the trust model. If you have a blob’s SHA-256 digest, you can verify its integrity regardless of which server you downloaded it from, because two registries serving the same digest are provably serving the same bytes. This is the same property that makes <a href=\"/2025/12/18/docker-is-the-lockfile-for-system-packages.html\">Docker images work as lockfiles for system packages</a>: once you have the digest, the content is immutable and verifiable no matter where it came from.</p>\n\n<p>A mirror doesn’t need to be trusted to be honest, only to be available. The manifest contains the digests, and the blobs can come from anywhere: geographic mirrors, corporate caches, peer-to-peer distribution, even a USB drive with an OCI layout directory. When Fastly has an outage and rubygems.org goes down with it, any alternative source that can serve matching bytes becomes a valid mirror without any special trust relationship.</p>\n\n<p><strong>Registry infrastructure is already built.</strong> Running rubygems.org or crates.io means running custom storage, custom CDN configuration, and custom authentication. A package registry built on OCI offloads the most expensive parts to infrastructure that already exists with SLAs and dedicated engineering teams, and the registry team can spend more time on what actually matters: <a href=\"/2025/12/22/package-registries-are-governance-as-a-service.html\">governance</a>, the package index, dependency resolution, and search.</p>\n\n<h3 id=\"what-wouldnt-work-well\">What wouldn’t work well</h3>\n\n<p><strong>The two-step fetch.</strong> If a package manager client talks directly to the OCI registry, it needs to fetch the manifest, parse it, then download the blob before extraction can start. The container world doesn’t care about this because you’re pulling maybe 5-10 layers for a single image. Package installs fan out across the dependency graph: a fresh <code class=\"language-plaintext highlighter-rouge\">npm install</code> on a mid-sized project might resolve 800 transitive dependencies, each needing its own manifest fetch before the content download can begin.</p>\n\n<p>A client could pipeline aggressively and fetch manifests concurrently, but the OCI Distribution Spec doesn’t have a batch manifest endpoint, so 800 packages still means 800 separate HTTP requests that don’t exist in the current model where npm can GET a tarball directly by URL.</p>\n\n<p>There’s a way around this: if registries included OCI blob digests in their existing metadata responses instead of (or alongside) direct tarball URLs, clients could skip the manifest fetch entirely and download blobs by digest. The difference in request flow looks like this:</p>\n\n<p>A pure OCI pull requires three hops: fetch the manifest, request the blob (which returns a 307 redirect), then download from the signed CDN URL. A smarter integration where the registry resolves the manifest internally reduces that to two: the registry’s metadata API returns the digest and a direct CDN URL, and the client downloads the blob and verifies it against the digest.</p>\n\n<p>Homebrew doesn’t quite do this yet. The <code class=\"language-plaintext highlighter-rouge\">brew install</code> flow described earlier requires two extra round-trips on top of the content transfer: one for the manifest, one for the redirect.</p>\n\n<p>The 307 redirect isn’t purely a latency cost; it’s also how the registry verifies the bearer token before handing off to the CDN, so registries adopting this pattern would need to decide whether their blobs are truly public or whether they want to keep that gatekeeper step. For registries with private package tiers, like npm’s paid plans or NuGet’s Azure Artifacts integration, the redirect model matters because access control at the blob level is part of the product.</p>\n\n<p>The formula metadata already knows the GHCR repository and tag, so the index service is already doing part of the resolution. If the formula JSON included the blob digest and a direct CDN URL, both hops disappear and the client downloads the blob in a single request while still verifying integrity by digest. Package managers that <a href=\"/2026/02/15/separating-download-from-install-in-docker-builds.html\">separate download from install</a> could take it further by batching blob fetches during a dedicated download phase.</p>\n\n<p><strong>Metadata is the actual hard problem.</strong> OCI manifests have annotations (arbitrary key-value strings) and a config blob, but package metadata like dependency trees, version constraints, platform compatibility rules, and license information doesn’t fit naturally into either. Each ecosystem would end up defining its own conventions for encoding metadata, its own <code class=\"language-plaintext highlighter-rouge\">mediaType</code> for its config blob, its own annotation keys.</p>\n\n<p>The reason every package manager invented its own archive format is not because tar and zip are insufficient for archiving files, but because the metadata conventions are what make each ecosystem different. What makes a <code class=\"language-plaintext highlighter-rouge\">.gem</code> different from a <code class=\"language-plaintext highlighter-rouge\">.crate</code> is how dependencies are expressed and what platform compatibility means, not the compression algorithm wrapping the source code. OCI standardizes how bytes move between machines, not what those bytes mean to a package manager.</p>\n\n<p><strong>Small package overhead.</strong> The OCI ceremony of manifests, layers, media types, and digest computation makes sense for multi-layer container images that can be gigabytes. For a 50KB npm package, the manifest JSON, config blob, digest computation for each, and the multi-step chunked upload API add up to several HTTP round-trips and a few hundred bytes of protocol overhead where the current model needs a single PUT. The fixed cost doesn’t scale down with the artifact, and a large share of packages on registries like npm and PyPI are small enough that the protocol overhead becomes a meaningful fraction of the payload.</p>\n\n<p><strong>Registry UI confusion.</strong> When a registry contains both container images and packages, the user experience gets muddled. GitHub Container Registry shows <code class=\"language-plaintext highlighter-rouge\">docker pull</code> commands for everything, but a Homebrew bottle needs <code class=\"language-plaintext highlighter-rouge\">brew install</code> and a Helm chart needs <code class=\"language-plaintext highlighter-rouge\">helm pull</code>. The UX for this is generally not great.</p>\n\n<p><strong>Not all registries are equal.</strong> The OCI 1.1 features that make non-container artifacts work well (custom <code class=\"language-plaintext highlighter-rouge\">artifactType</code>, the referrers API, the <code class=\"language-plaintext highlighter-rouge\">subject</code> field) aren’t universally supported. The OCI Image Specification advises that artifacts concerned with portability should follow specific conventions for <code class=\"language-plaintext highlighter-rouge\">config.mediaType</code>, and not all registries handle custom media types consistently. Registry implementations lag the spec, and the gap between what the spec allows and what any given registry supports is a source of bugs.</p>\n\n<p><strong>Offline and air-gapped use.</strong> A <code class=\"language-plaintext highlighter-rouge\">.deb</code> or <code class=\"language-plaintext highlighter-rouge\">.rpm</code> file is self-contained. You can copy it to a USB drive and install it on an air-gapped machine. An OCI artifact requires a manifest and one or more blobs, stored by digest in a registry’s content-addressable layout. Exporting to a self-contained format (OCI layout on disk) is possible but adds a step that simpler archive formats don’t need.</p>\n\n<p><strong>Who pays.</strong> GHCR storage and bandwidth are <a href=\"https://docs.github.com/en/billing/concepts/product-billing/github-packages\">currently free</a> for public images, with a promise of at least one month’s notice before that changes. At standard GitHub Packages rates ($0.25/GB/month for storage, $0.50/GB for bandwidth), Homebrew’s bottle archive would cost substantially more than zero. GitHub absorbs that as an in-kind subsidy, and the Homebrew 3.1.0 release notes explicitly thank them for it.</p>\n\n<p>If rubygems.org or PyPI moved all their package storage to GHCR tomorrow, someone would need to have a similar conversation with GitHub, or AWS, or Google. The current model of Fastly donating CDN bandwidth is fragile, but it exists and it’s understood.</p>\n\n<p>Adopting OCI for distribution is partly a technical decision about storage and protocols, but it’s also a decision about who funds the infrastructure that the ecosystem depends on and what leverage that creates. Shifting from Fastly-donated CDN to GitHub-donated OCI storage changes the answer to that question without necessarily improving it.</p>\n\n<h3 id=\"the-smarter-integration\">The smarter integration</h3>\n\n<p>Package registries do more than serve archives. They maintain an index of all packages, versions, and metadata that clients can search and resolve dependencies against, whether that’s npm’s registry API, PyPI’s Simple Repository API, crates.io’s <a href=\"https://github.com/rust-lang/crates.io-index\">git-based index</a>, RubyGems’ compact index, or Go’s module proxy protocol. OCI registries have no equivalent: you can list tags for a repository, but there’s no API for “give me all packages matching this query” or “resolve this dependency tree.”</p>\n\n<p>Splitting the roles this way makes more sense than having clients talk to the OCI registry directly. The registry uses OCI as a blob storage backend and integrates the content-addressable properties into the metadata APIs it already operates.</p>\n\n<p>Every package manager client already makes a metadata request before downloading anything. npm fetches the packument, pip fetches the Simple Repository API, Bundler fetches the compact index, <code class=\"language-plaintext highlighter-rouge\">go</code> hits the module proxy. These responses already include download URLs for specific versions.</p>\n\n<p>If those responses included OCI blob digests and direct download URLs pointing at OCI-backed storage, clients would get the content-addressable integrity checks, the mirroring properties, and the deduplication without ever needing to speak the OCI Distribution protocol themselves. The registry’s index service resolves the OCI manifest internally and hands the client a digest and a URL.</p>\n\n<p>The registry keeps full control of discovery, dependency resolution, version selection, and platform matching, all the ecosystem-specific logic that OCI doesn’t and shouldn’t try to handle. The OCI layer underneath provides content-addressable blob storage, signing via the referrers API, and the ability for mirrors to serve blobs by digest without special trust.</p>\n\n<p>Clients don’t need to know they’re talking to OCI-backed storage any more than they need to know whether the registry uses S3 or GCS underneath today. Homebrew already works roughly this way: the formula metadata points clients at GHCR, and the OCI manifest and redirect are implementation details of the download path.</p>\n\n<p>A registry doesn’t even need to migrate its existing packages to get some of these benefits. OCI 1.1’s <code class=\"language-plaintext highlighter-rouge\">artifactType</code> allows minimal manifests that exist purely as anchors for the referrers API. A registry could push a small OCI manifest for each package version, with the package’s digest in the annotations, and use it as the <code class=\"language-plaintext highlighter-rouge\">subject</code> that signatures and SBOMs attach to. The actual tarball continues to be served from the existing CDN. The signing and attestation infrastructure works without moving a single byte of package data.</p>\n\n<p>The OCI metadata model could also inform how registries design their own APIs. The Distribution Spec separates “list of versions” (the paginated tags endpoint, <code class=\"language-plaintext highlighter-rouge\">?n=&lt;limit&gt;&amp;last=&lt;tag&gt;</code>) from “metadata for a specific version” (the manifest for that tag). npm’s packument does neither: it returns a single JSON document containing metadata for every version of a package, with no pagination.</p>\n\n<p>For a package with thousands of versions that response can be megabytes. When <a href=\"https://github.com/npm/cli/issues/7529\">npm 10.4.0 stopped using the abbreviated metadata format</a>, installing npm itself went from downloading 2.1MB of metadata to 21MB. The full packuments also caused <a href=\"https://github.com/npm/cli/issues/7276\">out-of-memory crashes</a> when the CLI cached them in an unbounded map during dependency resolution.</p>\n\n<p>Most registries were designed when packages had dozens of versions, not thousands, and pagination wasn’t an obvious concern. PyPI’s Simple Repository API lists all files for a package in one response, though <a href=\"https://peps.python.org/pep-0700/\">PEP 700</a> added version listing metadata after the fact. crates.io takes a different approach with a git-based index that stores one file per crate, all versions as line-delimited JSON, while RubyGems’ compact index and Go’s module proxy both return complete version lists in a single response. None of these designed for pagination early on because the scale wasn’t there yet, and retrofitting pagination onto an existing API is harder than building it in from the start.</p>\n\n<p>If a registry is already rethinking its metadata endpoints to integrate OCI blob digests, that’s a natural time to adopt the structural pattern of paginated version listing plus per-version metadata fetched on demand.</p>\n\n<h3 id=\"would-it-actually-help\">Would it actually help</h3>\n\n<p>Homebrew’s migration happened under duress when Bintray died, and the rough edges were real: broken CI, missing old versions, a new class of checksum bugs. None of it required changing the archive format: the bottles are the same gzipped tarballs they always were, just stored and addressed differently.</p>\n\n<p>Most of the drawbacks, the manifest fan-out, the redirect tax, the metadata gap, come from treating OCI as the client-facing protocol rather than as infrastructure behind the registry’s existing API. The technical path through that is less disruptive than adopting a new distribution protocol from scratch.</p>\n\n<p>The registries that would benefit most from OCI’s storage and signing primitives are the community-funded ones: rubygems.org, crates.io, PyPI, hex.pm. They’re also the ones least able to afford the migration or negotiate the hosting arrangements that make it sustainable. This question is becoming less hypothetical as funding conversations around open source registries increasingly reference OCI adoption, and the registries on the receiving end of those conversations should understand what they’d be gaining and what they’d be giving up.</p>\n\n<p>Converging on shared storage primitives is the easy part of the problem. Each ecosystem’s metadata semantics are genuinely different and will stay that way. The harder question is whether the funding arrangements that come with OCI adoption serve the registries or the infrastructure providers offering to host them.</p>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:rpm5\">\n      <p>v5 was a fork by Jeff Johnson, RPM’s long-time maintainer, after he split from Red Hat around 2007. No major distribution adopted it. The mainline project skipped to v6 to avoid confusion. <a href=\"#fnref:rpm5\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>",
    "description": "OCI's storage primitives applied to package management.",
    "is_fulltext": true,
    "source": "Andrew Nesbitt",
    "pub_date": "2026-02-18T00:00:00+00:00",
    "fetched_at": "2026-02-18T12:21:56.591656",
    "url_hash": "3f67a4abf0ad635c06bd231b02b6fb92"
  },
  {
    "title": "Markdown’s Moment",
    "link": "https://feed.tedium.co/link/15204/17278321/markdown-growing-influence-cloudflare-ai",
    "content": "\n      <h2>For some reason, a bunch of big companies are really leaning into Markdown right now. AI may be the reason, but I kind of love the possible side benefits.</h2><img src=\"https://static.tedium.co/uploads/tedium021726.gif\" alt=\"Markdown’s Moment\"><div class=\"whitebox\"><p><strong>So, here’s something</strong> that I didn’t expect to be saying in 2026: There seems to be a nonzero chance that Markdown might become the new RSS.</p><p>“Whoa, crazy talk! It’s not even a protocol!” I hear you saying. But the evidence has seemed to pick up of late in a couple of different directions.</p><p>The first is the budding interest in publishing on the AT Protocol, which is working to solve the network-effect challenges that have forced many of us to send newsletters rather than post blogs on RSS feeds.</p><p>That’s exciting, if incredibly niche. But simultaneously, massive developer platforms are starting to offer something called “<a href=\"https://blog.cloudflare.com/markdown-for-agents/\">Markdown for Agents</a>”—something Cloudflare announced late last week, and which <a href=\"https://laravel-news.com/laravel-cloud-adds-markdown-for-agents-to-serve-ai-friendly-content\">Laravel Cloud</a> quickly followed up on a few days later. And <a href=\"https://vercel.com/blog/making-agent-friendly-pages-with-content-negotiation\">Vercel jumped on it</a> a couple of weeks ago.</p><p>(The news <a href=\"https://www.theverge.com/tech/877295/microsoft-notepad-markdown-security-vulnerability-remote-code-execution\">wasn’t all good</a> for Markdown, but most of it was.)</p><p>Some SEO old hands, like my friend Jon Henshaw, have <a href=\"https://coywolf.com/newsletter/markdown-is-the-new-amp-for-ai/\">reacted to this news with skepticism</a>, having had bad old memories of Google AMP and its sibling technologies Signed Exchanges and Core Web Vitals:</p><blockquote><p><strong>It’s 2026, and now I’m reading everywhere that all our pages must have <a href=\"https://daringfireball.net/projects/markdown/\">Markdown</a> versions, and it feels like AMP (and SXG and CWV) all over again.</strong> Except this time, the promise is that AI agents will better understand and interact with your site if you have them. The rationale is that HTML is too complex and consumes too many tokens to parse and analyze content. Whereas Markdown pages, with their simplicity, are ideal.</p></blockquote><p><em>(Side note: Core Web Vitals make me want to pull my hair out.)</em></p><p>Jon is a smart guy and follows this stuff closer than me (<a href=\"https://coywolf.com/news/\">Coywolf News</a> is a great site), but I will casually defend this push towards Markdown as a lingua franca of the Web. (Not the agentic Web. Just the Web. More on that later.) I actually think it’s really a great move for publishers that comes with way fewer inherent issues than Google AMP ever did.</p><p>For one thing, this is all standards-based, not something that was just invented that you need to manage. It’s literally using existing <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Content_negotiation\">content negotiation headers</a> that web servers already support, not forcing folks to learn something new. Plus it’s hard to argue with a point like this from Vercel:</p><blockquote><p>A typical blog post weighs 500KB with all the HTML, CSS, and JavaScript. However, the same content as Markdown is only 2KB. That’s a 99.6% reduction in payload size.</p></blockquote><p>That’s good for budget-minded AI agents, but it’s also good for people who run websites.</p><p>Additionally, Markdown has been in increasingly wide use for 20 years, and it keeps growing in popularity—and unlike the weird carousels and oddly specific rules of Google AMP, lots of people know how to use it. And the use of headers to deliver Markdown pages is already baked into Web standards, just waiting for folks to use it.</p></div><div class=\"adlayout ad-patreon\"><div class=\"md:grid md:grid-cols-3 lg:grid-cols-4 items-start gap-8\"><div class=\"md:col-span-1 max-w-[300px]\"><a href=\"https://ko-fi.com/tedium\" rel=\"nofollow sponsored\" class=\"block hover:no-underline\"><img src=\"https://static.tedium.co/uploads/kofi_logo.png\" alt=\"… You?\" class=\"w-full h-auto max-w-[300px] m-0\" loading=\"lazy\" /></a></div><div class=\"md:col-span-2 lg:col-span-3\"><h5 class=\"text-xl font-bold mb-2\" style=\"background-color: var(--ad-accent)\"><a href=\"https://ko-fi.com/tedium\" rel=\"nofollow sponsored\" class=\"block hover:no-underline\"> Sponsored By … You? </a></h5><div class=\"adcopy !max-w-none\"><p><strong>If you find weird or unusual topics</strong> like this super-fascinating, the best way to tell us is to <strong><a href=\"https://ko-fi.com/tedium\">give us a nod on Ko-Fi</a></strong>. It helps ensure that we can keep this machine moving, support outside writers, and bring on the tools to support our writing. (Also it’s heartening when someone chips in.)</p><p>We accept advertising, too! <a href=\"http://tedium.co/advertising/\">Check out this page to learn more</a>.</p></div></div></div></div><div class=\"whitebox\"><figure><img src=\"https://proxy.tedium.co/USUrGBA-1JIq1u5u60phP25R7w8=/1000x667/filters:quality(80)/uploads/Depositphotos_26272647_S.jpg\" width=\"1000\" height=\"667\" loading=\"lazy\" alt=\"Depositphotos_26272647_S.jpg\" /><figcaption>OK, so how many of these servers are getting flooded with request from AI agents right now? (<a href=\"https://depositphotos.com/?ref=26887078&amp;utm_source=linkCopy&amp;utm_medium=referral\">DepositPhotos.com</a>)</figcaption></figure><h3>This is really a tactic to help site owners avoid an AI-generated hug of death</h3><p><strong>Plus, there’s the rendering</strong>—Markdown is an antidote to the internet we currently run, which is highly dependent on programming languages and visual tricks that AI agents and honestly most people don’t even need. To me, when I see, “Cloudflare wants to give every webpage a Markdown version,” my thought is essentially, “Oh, they want to make AI agents stop DDoSing these poor PHP servers that still dominate the internet.”</p><p>When I see publishers talking about how their sites are getting flooded with viewers and getting slammed with unwanted hosting bills, it is clear that what we are doing is not tenable. Having Cloudflare put up a static Markdown file that takes up less space and has 0% of the JavaScript of the main page sounds like a win to me.</p><p>And if you’re building your pages semantically, as many publishers are likely already doing because they want to rank on Google, converting all that content to Markdown is going to be a cinch. <a href=\"https://tedium.co/2024/09/25/wordpress-wp-engine-open-web-negative-effects/\">Frequent</a> Tedium <a href=\"https://tedium.co/2025/01/10/wordpress-automattic-open-source-business-challenges/\">skepticism target</a> Matt Mullenweg is <a href=\"https://www.therepository.email/mullenweg-calls-for-markdown-endpoints-on-wordpress-org-as-he-pushes-web-os-vision\">pushing for its addition</a> to the WordPress.org website.</p><p>Just imagine, if you’re running an open-source project, and you didn’t have to force your users to see a loading page with anime characters just to keep the site online. Instead, you could tell Claude and Gemini and Perplexity to grab the data in a format they already use, and serve that in a static form, saving your poor forum from being drowned in dynamic requests.</p><p>There are lots of ethical qualms with AI, and you may want to just block them entirely, as is your right as a site owner. But I think diminishing a new-every-load HTML page to an unchanging Markdown file could save a lot of processing cycles for legacy server owners who have been trying to keep an extremely popular wiki online for 20 years.</p><p>I think there are websites and forums out there that have been absolutely wrecked by the rise of AI. Cloudflare, while still facing <a href=\"https://arstechnica.com/security/2024/07/cloudflare-once-again-comes-under-pressure-for-enabling-abusive-sites/\">periodic reputational issues</a>, has offered itself up as a line of defense for publishers. That’s noble—and while I get not everyone likes them, I think this particular offering is a good-for-the-internet move long-term.</p><figure><img src=\"https://proxy.tedium.co/uW3_Bwv5r3m49R2Nr12OQ-mUqi8=/1000x310/filters:quality(80)/uploads/library-books.jpg\" width=\"1000\" height=\"310\" loading=\"lazy\" alt=\"library-books.jpg\" /><figcaption>And while we’re at it, let’s start printing books in Markdown. Yeah! Let’s really take this idea to an extreme! (<a href=\"https://depositphotos.com/?ref=26887078&amp;utm_source=linkCopy&amp;utm_medium=referral\">DepositPhotos.com</a>)</figcaption></figure><h3>But hear me out: What if we just offered our pages in Markdown because it made the internet more accessible?</h3><p><strong>Yes, the reason for all of this is AI,</strong> because everything is about AI right now, but honestly, it would be a really awesome thing to offer for regular users, too.</p><p>Recently, I’ve been trying to take on a project with the Tedium website—it’s not quite done yet, but I’m trying to get the whole thing onto the AT Protocol, <a href=\"https://tedium.co/2024/10/23/twitter-archive-backup-script-bluesky/\">mimicking my upload</a> of my Twitter archive to Bluesky. (I’ve gotten the upload to work, it’s just the details that need to be tweaked. Here’s a <a href=\"https://pdsls.dev/at://did:plc:ibzvsahcpkzxbdcw4jrr2kzq/site.standard.document/3mezmcwrt3f2q\">sample post</a> that came out okay.) I’m using a tool called <a href=\"https://sequoia.pub\">Sequoia</a>, which makes it possible to plug a static site into the same protocol Bluesky uses. It parses the roughly 1,300 pages and then uploads them to a server on the ATmosphere.</p><figure><img src=\"https://proxy.tedium.co/f1HrwxPcoIWYb74cF0dbgr47-Ms=/865x840/filters:quality(80)/uploads/StandardSite.png\" width=\"865\" height=\"840\" loading=\"lazy\" alt=\"StandardSite.png\" /><figcaption>Genuinely cool to see this kind of collab happening in the ATmosphere.</figcaption></figure><p>At the center of this is something called <a href=\"https://standard.site\">Standard.site</a>, which aims to make a space for long-form content on the AT protocol. It’s not prescriptive to Markdown, though you <em>could</em> use it to share posts in Markdown if you wanted. It sounds promising—and like the budding efforts in the fediverse, it aims to make content easier to discover. Which is the problem RSS hoped to solve a quarter-century ago, admittedly—but this is doing it with more glue.</p><p>To me, I see a connection between the push to make Markdown an undercurrent of the agentic Web and this weird experiment on the fringes of emerging social tech. And honestly I would not be surprised if web browsers plugged into these AI-targeted Markdown feeds to give users a lightweight experience. (You know what else could use this?!? Email.)</p><p>It’s so fascinating, seeing this thing I’ve come to really appreciate as a writer turn into this ad-hoc building block of the modern internet. Even if I find it uncomfortable that AI is the vessel it rode in on.</p><p>When I found it, it was my superpower—the tool I used to plow through five articles a day at a new job. It was the cruft-buster, the starting point, the README file. And now it’s become something else entirely—something that could get us back to basics without the extra cruft of AMP or the stress of Core Web Vitals. (And even better, that didn’t come from Google.)</p><p>Honestly, I’m kind of here for it.</p></div><div class=\"graybox\"><h5>Markdown-Free Links</h5><p><strong>We’ve been losing</strong> a lot of good music folks of late, most recently <a href=\"https://variety.com/2026/music/obituaries-people-news/billy-steinberg-dead-songwriter-like-a-virgin-true-colors-1236664839/\">Billy Steinberg</a>, the dude who wrote “Like A Virgin” and “I Touch Myself.” Fortunately, friend of Tedium Chris Dalla Riva <a href=\"https://www.cantgetmuchhigher.com/p/writing-hits-for-madonna-celine-dion\">got to chat with him in 2023</a>.</p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kKt46Lch2bo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><p><strong>I know a fellow traveler</strong> when I see one, and with that in mind I want to give a shout to Rabbit Hole, a new-ish YouTube channel that recently asked <a href=\"https://www.youtube.com/watch?v=kKt46Lch2bo\">why office chairs have five legs</a>. A promising start.</p><p><strong>Also, we have to mention <a href=\"https://www.washingtonpost.com/obituaries/2026/02/17/jesse-jackson-dead-civil-rights/\">Jesse Jackson</a>,</strong> a civil rights icon and easily the most well-known “<a href=\"https://wtop.com/dc/2026/02/jesse-jackson-had-strong-connections-to-dc-held-local-public-office/\">shadow senator</a>” in U.S. history.</p><p>--</p><p>Find this one an interesting read? <a href=\"https://tedium.co/2026/02/17/markdown-growing-influence-cloudflare-ai/\">Share it with a pal</a>!</p><p>And a quick shout to our sponsor <a href=\"https://la-machine.fr/?utm_source=newsletter&amp;utm_medium=email&amp;utm_campaign=tedium\">la machine</a>, which doesn’t support Markdown, but has a good reason for not doing so.</p></div>\n    <img src=\"https://feed.tedium.co/link/15204/17278321.gif\" height=\"1\" width=\"1\"/>",
    "description": "For some reason, a bunch of big companies are really leaning into Markdown right now. AI may be the reason, but I kind of love the possible side benefits.",
    "is_fulltext": true,
    "source": "Tedium: The Dull Side of the Internet.",
    "pub_date": "2026-02-18T04:01:53Z",
    "fetched_at": "2026-02-18T12:21:58.075309",
    "url_hash": "adcc79b2a3ffad4aff9c0199536ce784"
  }
]