[
  {
    "title": "Mitchell Hashimoto: My AI Adoption Journey",
    "link": "https://simonwillison.net/2026/Feb/5/ai-adoption-journey/#atom-everything",
    "content": "\n    \n<p><strong><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey\">Mitchell Hashimoto: My AI Adoption Journey</a></strong></p>\nSome really good and unconventional tips in here for getting to a place with coding agents where they demonstrably improve your workflow and productivity. I particularly liked:</p>\n<ul>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-2-reproduce-your-own-work\">Reproduce your own work</a> - when learning to use coding agents Mitchell went through a period of doing the work manually, then recreating the same solution using agents as an exercise:</p>\n<blockquote>\n<p>I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce identical results in terms of quality and function (without it being able to see my manual solution, of course).</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-3-end-of-day-agents\">End-of-day agents</a> - letting agents step in when your energy runs out:</p>\n<blockquote>\n<p>To try to find some efficiency, I next started up a new pattern: <strong>block out the last 30 minutes of every day to kick off one or more agents.</strong> My hypothesis was that <em>perhaps</em> I could gain some efficiency if the agent can make some <em>positive progress</em> in the times I can't work anyways.</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-4-outsource-the-slam-dunks\">Outsource the Slam Dunks</a> - once you know an agent can likely handle a task, have it do that task while you work on something more interesting yourself.</p>\n</li>\n</ul>\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46903558\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/mitchell-hashimoto\">mitchell-hashimoto</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a></p>\n\n\n\n",
    "description": "Mitchell Hashimoto: My AI Adoption Journey Some really good and unconventional tips in here for getting to a place with coding agents where they demonstrably improve your workflow and productivity. I particularly liked: Reproduce your own work - when learning to use coding agents Mitchell went through a period of doing the work manually, then recreating the same solution using agents as an exercise: I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce ",
    "is_fulltext": true,
    "source": "Simon Willison's Weblog",
    "pub_date": "2026-02-05T23:39:07+00:00",
    "fetched_at": "2026-02-06T00:35:17.818873",
    "url_hash": "f7b48c1b172760e58199282af636438b"
  },
  {
    "title": "My AI Adoption Journey",
    "link": "https://mitchellh.com/writing/my-ai-adoption-journey",
    "content": "",
    "description": "",
    "is_fulltext": false,
    "source": "Mitchell Hashimoto",
    "pub_date": "Thu, 05 Feb 2026 00:00:00 GMT",
    "fetched_at": "2026-02-06T00:35:29.184245",
    "url_hash": "cece43d75662f518b9bea7f22b1b319c"
  },
  {
    "title": "Heritability of intrinsic human life span is about 50% when heritability is redefined to be something completely different",
    "link": "https://dynomight.net/lifespan/",
    "content": "<p>How heritable is hair color? Well, if you’re a redhead and you have an identical twin, they will definitely also be a redhead. But the age at which twins go gray seems to vary a bit based on lifestyle. And there’s some randomness in where melanocytes end up on your skull when you’re an embryo. And your twin might dye their hair! So the correct answer is, some large number, but less than 100%.</p>\n\n<p>OK, but check this out: Say I redefine “hair color” to mean “hair color except ignoring epigenetic and embryonic stuff and pretending that no one ever goes gray or dyes their hair et cetera”. Now, hair color is 100% heritable. Amazing, right?</p>\n\n<p>Or—how heritable is IQ? The wise man answers, “Some number between 0% or 100%, it’s not that important, please don’t yell at me.” But whatever the number is, it depends on society. In our branch of the multiverse, some kids get private tutors and organic food and $20,000 summer camps, while other kids get dysfunctional schools and lead paint and summers spent drinking Pepsi and staring at glowing rectangles. These things surely have at least <em>some</em> impact on IQ.</p>\n\n<p>But again, watch this: Say I redefine “IQ” to be “IQ in some hypothetical world where every kid got exactly the same school, nutrition, and parenting, so none of those non-genetic factors matter anymore.” Suddenly, the heritability of IQ is higher. Thrilling, right? So much science.</p>\n\n<p>If you want to redefine stuff like this… that’s not <em>wrong</em>. I mean, heritability is a pretty arbitrary concept to start with. So if you prefer to talk about heritability in some other world instead of our actual world, who am I to judge?</p>\n\n<p>Incidentally, here’s a <a href=\"https://doi.org/10.1126/science.adz1187\">recent paper</a>:</p>\n\n<p><img src=\"/img/lifespan/title.png\" alt=\"\" /></p>\n\n<p>I stress that this is a perfectly OK paper. I’m picking on it mostly because it was published in Science, meaning—like all Science papers—it makes grand claims but is woefully vague about what those claims mean or what was actually done. Also, publishing in Science is morally wrong and/or makes me envious. So I thought I’d try to explain what’s happening.</p>\n\n<p>It’s actually pretty simple. At least, now that I’ve spent several hours reading the paper and its appendix over and over again, I’ve now convinced myself that it’s pretty simple. So, as a little pedagogical experiment, I’m going to try to explain the paper three times, with varying levels of detail.</p>\n\n<h2 id=\"explanation-1-the-very-extremely-high-level-picture\">Explanation 1: The very extremely high level picture</h2>\n\n<p>The normal way to estimate the heritability of lifespan is using twin data. Depending on what dataset you use, this will give 23-35%. This paper built a mathematical model that tries to simulate how long people <em>would</em> live in a hypothetical world in which no one dies from any non-aging related cause, meaning no car accidents, no drug overdoses, no suicides, no murders, and no (non-age-related) infectious disease. On that simulated data, for simulated people in a hypothetical world, heritability was 46-57%.</p>\n\n<h2 id=\"commentary\">Commentary</h2>\n\n<p>Everyone seems to be interpreting this paper as follows:</p>\n\n<blockquote>\n  <p>Aha! We thought the heritability of lifespan was 23-35%. But it turns out that it’s around 50%. Now we know!</p>\n</blockquote>\n\n<p>I understand this. Clearly, when the editors at Science chose the title for this paper, their goal was to lead you to that conclusion. But this is not what the paper says. What it says is this:</p>\n\n<blockquote>\n  <p>We built a mathematical model of alternate universe in which nobody died from accidents, murder, drug overdoses, or infectious disease. In that model, heritability was about 50%.</p>\n</blockquote>\n\n<h2 id=\"explanation-2-the-very-high-level-picture\">Explanation 2: The very high-level picture</h2>\n\n<p>Let’s start over. Here’s figure 2 from the paper.</p>\n\n<p><img src=\"/img/lifespan/fig2.png\" alt=\"\" /></p>\n\n<p>Normally, heritability is estimated from twin studies. The idea is that identical twins share 100% of their DNA, while fraternal twins share only 50%. So if some trait is more correlated among identical twins than among fraternal twins, that suggests DNA influences that trait. There are statistics that formalize this intuition. Given a dataset that records how long various identical and fraternal twins lived, these produce a heritability number.</p>\n\n<p>Two such traditional estimates appear as black circles in the above figures. For the Danish twin cohort, lifespan is estimated to be 23% heritable. For the Swedish cohort, it’s 35%.</p>\n\n<p>This paper makes a “twin simulator”. Given historical data, they fit a mathematical model to simulate the lifespans of “new” twins. Then they compute heritability on this simulated data.</p>\n\n<p>Why calculate heritability on simulated data instead of real data? Well, their mathematical model contains an “extrinsic mortality” parameter, which is supposed to reflect the chance of death due to all non-aging-related factors like accidents, murder, or infectious disease. They assume that the chance someone dies from any of this stuff is constant over people, constant over time, and that it accounts for almost all deaths for people aged between 15 and 40.</p>\n\n<p>The point of building the simulator is that it’s possible to <em>change</em> extrinsic mortality. That’s what’s happening in the purple curves in the above figure. For a range of different extrinsic mortality parameters, they simulate datasets of twins. For each simulated dataset, they estimate heritability just like with a real dataset.</p>\n\n<p>Note that the purple curves above nearly hit the black circles. This means that if they run their simulator with extrinsic mortality set to match reality, they get heritability numbers that line up with what we get from real data. That suggests their mathematical model isn’t totally insane.</p>\n\n<p>If you decrease extrinsic mortality, then you decrease the non-genetic randomness in how long people live. So heritability goes up. Hence, the purple curves go up as you go to the left.</p>\n\n<h2 id=\"intermission-on-science\">Intermission: On Science</h2>\n\n<p>My explanation of this paper relies on some amount of guesswork. For whatever reason, Science has decided that papers should contain almost no math, even when the paper in question is <em>about</em> math. So I’m mostly working from an English description. But even that description isn’t systematic. There’s no place that clearly lays out all the things they did, in order. Instead, you get little hints, sort of randomly distributed throughout the paper. There’s an appendix, which the paper confidently cites over and over. But if you actually read the appendix, it’s just more disconnected explanations of random things except now with equations set in glorious Microsoft Work format.</p>\n\n<p>Now, in most journals, authors write everything. But Science has professional editors. Given that every single statistics-focused paper in Science seems to be like this, we probably shouldn’t blame the authors of this one. (Other than for their decision to publish in Science in the first place.)</p>\n\n<p>I do wonder what those editors are doing, though. I mean, let me show you something. Here’s the first paragraph where they start to actually explain what they actually did, from the first page:</p>\n\n<p><img src=\"/img/lifespan/paragraph.png\" alt=\"\" /></p>\n\n<p>See that <em>h(t,θ)</em> at the end? What the hell is that, you ask? That’s a good question, because it was never introduced before this and is never mentioned again. I guess it’s just supposed to be <em>f(t,θ)</em>, which is fine. (I yield to none in my production of typos.) But if paying journals ungodly amounts of money brought us to this, of what use are those journals?</p>\n\n<p>Moving on…</p>\n\n<h2 id=\"explanation-3-also-pretty-high-level-but-as-low-as-were-doing-to-go\">Explanation 3: Also pretty high level, but as low as we’re doing to go</h2>\n\n<p>Probably most people don’t need this much detail and should skip this section. For everyone else, let’s start over one last time.</p>\n\n<p>The “normal” way to estimate heritability is by looking at correlations between different kinds of twins. Intuitively, if the lifespans of identical twins are more correlated than the lifespans of fraternal twins, that suggests lifespan is heritable. And it turns out that one estimator for heritability is “twice the difference between the correlation among identical twins and the correlation among fraternal twins, all raised together.” There are other similar estimators for other kinds of twins. These normally say lifespan is perhaps 20% and 35% heritable.</p>\n\n<p>This paper created an equation to model the probability a given person will die at a given age. The parameters of the equation vary from person to person, reflecting that some of us have DNA that predisposes us to live longer than others. But the idea is that the chances of dying are fairly constant between the ages of 15 and 40, after which they start increasing.</p>\n\n<p>This equation contains an “extrinsic mortality” parameter. This is meant to reflect the chance of death due to all non-aging related factors like accidents or murder, etc. They assume this is constant. (Constant with respect to people and constant over time.) Note that they don’t actually look at any data on causes of death. They just add a constant risk of death that’s shared by all people at all ages to the equation, and then they call this “extrinsic mortality”.</p>\n\n<p>Now remember, different people are supposed to have different parameters in their probability-of-death equations. To reflect this, they fit a Gaussian distribution (bell curve) to the parameters with the goal of making it fit with historical data. The idea is that if the distribution over parameters were too broad, you might get lots of people dying at 15 or living until 120, which would be wrong. If the distribution were too concentrated, then you might get everyone dying at 43, which would also be wrong. So they find a good distribution, one that makes the ages people die in simulation look like the ages people actually died in historical data.</p>\n\n<p>Right! So now they have:</p>\n\n<ol>\n  <li>An equation that’s supposed to reflect the probability a given person dies at a given age.</li>\n  <li>A distribution over the parameters of that equation that’s supposed to produce population-wide death ages that look like those in real historical data.</li>\n</ol>\n\n<p>Before moving on, I remind you of two things:</p>\n\n<ol>\n  <li>They assume their death equation <em>entirely</em> determines the probability someone will die in a given year.</li>\n  <li>They assume that the shape of someone’s death equation is <em>entirely</em> determined by genetics.</li>\n</ol>\n\n<p>The event of a person dying at a given age is random. But the <em>probability</em> that this happens is assumed to be fixed and determined by genes and genes alone.</p>\n\n<p>Now they simulate different kinds of twins. To simulate identical twins, they just draw parameters from their parameter distribution, assign those parameters to two different people, and then let them randomly die according to their death equation. (Is this getting morbid?) To simulate fraternal twins, they do the same thing, except instead of giving the two twins identical parameters, they give them <em>correlated</em> parameters, to reflect that they share 50% of their DNA.</p>\n\n<p>How exactly do they create those correlated parameters? They don’t explain this in the paper, and they’re quite vague in the supplement. As far as I can tell they sample two sets of parameters from their parameter distribution such that the <em>parameters</em> are correlated at a level of 0.5.</p>\n\n<p>Now they have simulated twins. They can simulate them with different extrinsic mortality values. If they lower extrinsic mortality, heritability of lifespan goes up. If they lower it to zero, heritability goes up to around 50%.</p>\n\n<h2 id=\"more-commentary\">More commentary</h2>\n\n<p>Almost all human traits are partly genetic and partly due to the environment and/or random. If you could change the world and reduce the amount of randomness, then <em>of course</em> heritability would go up. That’s true for life expectancy just life for anything else. So what’s the point of this paper?</p>\n\n<p>There is a point!</p>\n\n<ol>\n  <li>\n    <p>Sure, obviously heritability would be higher in a world without accidents or murder. We don’t need a paper to know that. But <em>how much</em> higher? It’s impossible to say without modeling and simulating that other world.</p>\n  </li>\n  <li>\n    <p>Our twin datasets are really old. It’s likely that non-aging-related deaths are lower now in the past, because we have better healthcare and so on. This means that the heritability of lifespan for people alive today may be larger than it was for the people in our twin datasets, some of whom were born in 1870. We won’t know for sure until we’re all dead, but this paper gives us a way to guess.</p>\n  </li>\n  <li>\n    <p>Have I mentioned that heritability depends on society? And that heritability changes when society changes? And that heritability is <a href=\"https://dynomight.net/heritable/\">just a ratio</a> and you should stop trying to make it be a non-ratio because only-ratio things cannot be non-ratios? This is a nice reminder.</p>\n  </li>\n</ol>\n\n<p>Honestly, I think the model the paper built is quite clever. Nothing is perfect, but I think this is a pretty good run at the question of “how high would the heritability of lifespan be if extrinsic mortality were lower.</p>\n\n<p>I only have two objections. The first is to the Science writing style. This is a paper describing a statistical model. So shouldn’t there be somewhere in the paper where they explain exactly what they did, in order, from start to finish? Ostensibly, I think this is done in the left-hand column on the second page, just with little detail because Science is written for a general audience. But personally I think that description is the worst of all worlds. Instead of giving the high-level story in a coherent way, it throws random technical details at you without enough information to actually make sense of them. Couldn’t the full story with the full details at least be in the appendix? I feel like this wasted hours of my time, and that if someone wanted to reproduce this work, they would have almost no chance of doing so from the description given. How have we as a society decided that we should take our “best” papers and do this to them?</p>\n\n<p>But my main objection is this:</p>\n\n<p><img src=\"/img/lifespan/title-box.png\" alt=\"\" /></p>\n\n<p>At first, I thought this was absurd. The fact that people die in car accidents is not a “confounding factor”. And pretending that no one dies in a car accidents does not “address” some kind of bias. That’s just computing heritability in some other world. Remember, heritability is not some kind of Platonic form. It is an <em>observational statistic</em>. There is no such thing as “true” heritability, independent of the contingent facts of our world.\nBut upon reflection, I think they’re trying to say something like this:</p>\n\n<blockquote>\n  <p>Heritability of intrinsic human lifespan is about 50% when extrinsic mortality is adjusted to be closer to modern levels.</p>\n</blockquote>\n\n<p>The problem is: I think this is… not true? Here are the actual heritability estimates in the paper, varying by dataset (different plots) the cutoff year (colors) and extrinsic mortality (x-axis).</p>\n\n<p><img src=\"/img/lifespan/full.png\" alt=\"\" /></p>\n\n<p>When extrinsic mortality goes down, heritability goes up. So the obvious question is: What is extrinsic mortality in modern people?</p>\n\n<p>This is a tricky question, because “extrinsic mortality” isn’t some simple observational statistic. It is a parameter in their model. (Remember, they never looked at causes of death.) So it’s hard to say, but they seem to suggest that extrinsic mortality in modern people is 0.001 / year, or perhaps a bit less.</p>\n\n<p>The above figures have the base-10 logarithm of extrinsic mortality on the x-axis. And the base-10 logarithm of 0.001 is -3. But if you look at the curves when the x-axis is -3, the heritability estimates <em>are not 50%</em>. They’re more like 35-45%, depending on the particular model and age cutoff.</p>\n\n<p>So here’s my suggested title:</p>\n\n<blockquote>\n  <p>Heritability of intrinsic human lifespan is about 40% when extrinsic mortality is adjusted to modern levels, according to our simulation.</p>\n</blockquote>\n\n<p>There might be a reason I don’t work at Science.</p>",
    "description": "ratios will be ratios",
    "is_fulltext": true,
    "source": "DYNOMIGHT",
    "pub_date": "2026-02-05T00:00:00+00:00",
    "fetched_at": "2026-02-06T00:35:30.278317",
    "url_hash": "8ad76f5d4aa80e21835c0770dc6e845f"
  },
  {
    "title": "Life pro tip: a Steam Deck can be a bluetooth speaker",
    "link": "https://xeiaso.net/notes/2026/steam-deck-bluetooth-speaker/",
    "content": "\n        <p>Bluetooth headphones are great, but they have one main weakness: they can only get audio streams from a single device at a time. Facts and Circumstances™️ mean that I have to have hard separation of personal and professional workloads, and I frequently find myself doing both at places like coworking spaces.</p>\n        <figure class=\"max-w-3xl mx-auto not-prose w-full undefined\"><a href=\"https://files.xeiaso.net/notes/2026/steam-deck-bluetooth-speaker/the-spread.jpg\"><picture><source type=\"image/avif\" srcset=\"https://files.xeiaso.net/notes/2026/steam-deck-bluetooth-speaker/the-spread.avif\"/><source type=\"image/webp\" srcset=\"https://files.xeiaso.net/notes/2026/steam-deck-bluetooth-speaker/the-spread.webp\"/><img loading=\"lazy\" src=\"https://files.xeiaso.net/notes/2026/steam-deck-bluetooth-speaker/the-spread.jpg\"/></picture></a></figure>\n        <p>Often I want to have all of these audio inputs at once:</p>\n        <ul>\n        <li>Notifications from games on my Steam Deck (mostly FFXIV)</li>\n        <li>Notification sounds from Slack at work</li>\n        <li>Music on my personal laptop</li>\n        <li>Anything else from my phone</li>\n        </ul>\n        <p>When I'm in my office at home, I'll usually have all of these on speaker because I'm the only person there. I don't want to disturb people at this coworking space with my notification pings or music.</p>\n        <p>Turns out a Steam Deck can act as a BlueTooth speaker with no real limit to the number of inputs! Here's how you do it:</p>\n        <ul>\n        <li>Open Bluetooth settings on the Steam Deck and device you want to pair.</li>\n        <li>Look for the name of your laptop on the Steam Deck or Steam Deck on your laptop. This may require you to &quot;show all devices&quot; as usually the UI wants to prevent you from pairing a laptop to another computer because this normally doesn't make sense.</li>\n        <li>Pair the two devices together and confirm the request on both sides.</li>\n        <li>Select your Steam Deck as a speaker on your laptop.</li>\n        <li>Max out the volume on the laptop and control the volume on the deck.</li>\n        </ul>\n        <p>This is stupidly useful. It also works with any Linux device, so if you have desktop Linux on any other machines you can also use them as speakers. I really wish this was a native feature of macOS and Windows. It's one of the best features of desktop Linux that nobody knows about.</p>\n        <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img class=\"h-16 w-16 rounded-xs\" alt=\"Cadey is coffee\" loading=\"lazy\" src=\"https://stickers.xeiaso.net/sticker/cadey/coffee\"/></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#cadey\">Cadey</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>Sorry if this is a bit worse than my usual writing style, I have a big life\n        event coming up and preparations for it are having secondary side effects that\n        have made focusing deep enough for good writing hard. It'll get better! I'm\n        just kinda stressed, sorry.</p></div></div></div>\n      ",
    "description": "Your headphones may only let you get audio from one source at once, but Linux has no such limitations!",
    "is_fulltext": true,
    "source": "Xe Iaso's blog",
    "pub_date": "Thu, 05 Feb 2026 00:00:00 GMT",
    "fetched_at": "2026-02-06T00:35:32.384133",
    "url_hash": "00b2875c4e807a53932a4d3513453c61"
  },
  {
    "title": "How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls?",
    "link": "https://devblogs.microsoft.com/oldnewthing/20260205-00/?p=112042",
    "content": "<p>Last time, we saw how to <a title=\"How can I prevent the user from changing the widths of ListView columns?\" href=\"https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037\"> prevent the user from changing the widths of ListView columns</a>, but the technique required version 6 of the common controls. What if you&#8217;re stuck in the dark ages and have to use version 5?</p>\n<p>You can deny the ability to change the width of a header item by listening for <code>HDN_<wbr />ITEM­CHANGING</code> and returning 1 to deny the change if there is a change to the width.</p>\n<pre>case WM_NOTIFY:\n    {\n        auto hdr = (NMHDR*)lParam;\n        if (hdr-&gt;code == HDN_ITEMCHANGING) {\n            auto header = (NMHEADER*)lParam;\n            if (header-&gt;pitem-&gt;mask &amp; HDI_WIDTH) {\n                return 1;\n            }\n        }\n    }\n    return 0;\n</pre>\n<p>The above code assumes that it is running in a window procedure. If it&#8217;s running in a dialog procedure, then you need to set the dialog message result.</p>\n<pre>case WM_NOTIFY:\n    {\n        auto hdr = (NMHDR*)lParam;\n        if (hdr-&gt;code == HDN_ITEMCHANGING) {\n            auto header = (NMHEADER*)lParam;\n            if (header-&gt;pitem-&gt;mask &amp; HDI_WIDTH) {\n                <span style=\"border: solid 1px currentcolor; border-bottom: none;\">SetWindowLongPtr(hDlg, DWLP_MSGRESULT, 1);</span>\n                <span style=\"border: solid 1px currentcolor; border-top: none;\">return TRUE;                              </span>\n            }\n        }\n    }\n    return FALSE;\n</pre>\n<p>Note that if somebody tries to change both the width and the text, this will reject the entire change. There is, unfortunately, no way to selectively reject the change: Modifications to <code>header-&gt;pitem-&gt;mask</code> are ignored.¹</p>\n<p>However, all is not lost. Even though changes to the mask are ignored, changes to the <code>pitem-&gt;cxy</code> are still honored, so we can just set the width back to whatever the width is right now.</p>\n<pre>case WM_NOTIFY:\n    {\n        auto hdr = (NMHDR*)lParam;\n        if (hdr-&gt;code == HDN_ITEMCHANGING) {\n            auto header = (NMHEADER*)lParam;\n            if (header-&gt;pitem-&gt;mask &amp; HDI_WIDTH) {\n                <span style=\"border: solid 1px currentcolor; border-bottom: none;\">HDITEM item;                                             </span>\n                <span style=\"border: 1px currentcolor; border-style: none solid;\">item.mask = HDI_WIDTH;                                   </span>\n                <span style=\"border: 1px currentcolor; border-style: none solid;\">if (Header_GetItem(nm-&gt;hdr.hwndFrom, nm-&gt;iItem, &amp;item)) {</span>\n                <span style=\"border: 1px currentcolor; border-style: none solid;\">{                                                        </span>\n                <span style=\"border: 1px currentcolor; border-style: none solid;\">    header-&gt;pitem-&gt;cxy = item.cxy;                       </span>\n                <span style=\"border: solid 1px currentcolor; border-top: none;\">}                                                        </span>\n            }\n        }\n    }\n    return 0;\n</pre>\n<p>One thing we haven&#8217;t fixed, though, is that the mouse cursor changes to a resize cursor when it is on the border between two column headers, even though resizing has been disabled. We&#8217;ll try to fix that next time.</p>\n<p>¹ This is arguably a bug in the version 5 header control, but there&#8217;s no point trying to fix it now. There may be code that relies on the fact that changes to the mask have no effect, and besides, this is the old and busted version 5 control.²</p>\n<p>² The version 6 control has the same bug, but again, there&#8217;s no point trying to fix it now because it will almost certainly break someone. The version 6 common controls are 25 years old, and it&#8217;s probably safe to assume that every possible change will probably break <i>someone</i>.³</p>\n<p>³ Once, I helped fixed a memory leak in the common controls, but we had to back it out because it broke a major application. We couldn&#8217;t figure out why it broke the program, so we couldn&#8217;t put together a shim. We just had to restore the leak. My guess is that the developers of the program had discovered the leak on their own and was somehow working around it, and our fix broke their workaround.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260205-00/?p=112042\">How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls?</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>\n",
    "description": "Deny changes to the width. The post How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls? appeared first on The Old New Thing.",
    "is_fulltext": true,
    "source": "The Old New Thing",
    "pub_date": "Thu, 05 Feb 2026 15:00:00 +0000",
    "fetched_at": "2026-02-06T00:35:32.861260",
    "url_hash": "4b40b193aa2fced0ac6876853f115dad"
  },
  {
    "title": "Fibonacci number certificates",
    "link": "https://www.johndcook.com/blog/2026/02/05/fibonacci-certificate/",
    "content": "<p>Suppose I give you a big number <em>F</em> and claim that <em>F</em> is a Fibonacci number. How could you confirm this?</p>\n<p>Before I go further, let me say what this post is really about. It&#8217;s not about Fibonacci numbers so much as it is about proofs and certificates. There&#8217;s no market for large Fibonacci numbers, and certainly no need to quickly verify that a number is a Fibonacci number.</p>\n<p>You could write a program to generate Fibonacci numbers, and run it until it either produces <em>F</em> , in which case you know <em>F</em> is a Fibonacci number, or the program produces a larger number than <em>F</em> without having produced <em>F</em>, in which case you know it&#8217;s not a Fibonacci number. But there&#8217;s a faster way.</p>\n<p>A certificate is data that allows you to confirm a solution to a problem in less time, usually far less time, than it took to generate the solution. For example, <a href=\"https://www.johndcook.com/blog/2023/01/03/pratt-certificate/\">Pratt certificates</a> give you a way to prove that a number is prime. For a large prime, you could verify its Pratt certificate much faster than directly trying to prove the number is prime.</p>\n<p>There is a theorem that says a number <em>f</em> is a Fibonacci number if and only if one of 5<em>f</em><sup>2</sup> ± 4 is a perfect square. So in addition to <em>F</em> another number <em>r</em> that is a certificate that <em>F</em> is a Fibonacci number. You compute</p>\n<p style=\"padding-left: 40px;\"><em>N</em> = 5<em>F</em>² − <em>r</em>²</p>\n<p>and if <em>N</em> is equal to 4 or −4, you know that <em>F</em> is a Fibonacci number. Otherwise it is not.</p>\n<p>Here&#8217;s a small example. Suppose I give you (12586269025, 28143753123) and claim that the first number is a Fibonacci number and the second number is its certificate. You can compute</p>\n<p style=\"padding-left: 40px;\">5 × 12586269025² − 28143753123²</p>\n<p>and get −4, verifying the claim.</p>\n<p>Certificates are all about the amount of computation the verifier needs to do. The prover, i.e. the person producing the certificate, has to do extra work to provide a certificate in addition to a problem solution. This trade-off is acceptable, for example, in a <a href=\"https://www.johndcook.com/blog/crypto/\">blockchain</a> where a user posts one transaction but many miners will verify many transactions.</p>\n<h2>Related posts</h2>\n<ul>\n<li class='link'><a href='https://www.johndcook.com/blog/2023/01/13/ecpp/'>Elliptic curve primality certificates</a></li>\n<li class='link'><a href='https://www.johndcook.com/blog/2024/11/30/generation-verification-costs/'>Generation versus verification costs</a></li>\n<li class='link'><a href='https://www.johndcook.com/blog/2023/01/13/proof-of-optimization/'>Proof of optimization</a></li>\n<li class='link'><a href='https://www.johndcook.com/blog/2025/11/29/zkp-composite/'>Zero knowledge proof of compositeness</a></li>\n</ul>The post <a href=\"https://www.johndcook.com/blog/2026/02/05/fibonacci-certificate/\">Fibonacci number certificates</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
    "description": "Suppose I give you a big number F and claim that F is a Fibonacci number. How could you confirm this? Before I go further, let me say what this post is really about. It&#8217;s not about Fibonacci numbers so much as it is about proofs and certificates. There&#8217;s no market for large Fibonacci numbers, and certainly [&#8230;] The post Fibonacci number certificates first appeared on John D. Cook.",
    "is_fulltext": true,
    "source": "John D. Cook",
    "pub_date": "Thu, 05 Feb 2026 17:14:20 +0000",
    "fetched_at": "2026-02-06T00:35:48.015112",
    "url_hash": "36a09526e56459e12881ec72e7d5a35b"
  },
  {
    "title": "Writing an LLM from scratch, part 27 -- what's left, and what's next?",
    "link": "https://www.gilesthomas.com/2025/11/llm-from-scratch-27-whats-left-and-whats-next",
    "content": "<p>On 22 December 2024, <a href=\"/2024/12/llm-from-scratch-1\">I wrote</a>:</p>\n\n<blockquote>\n  <p>Over the Christmas break (and probably beyond) I'm planning to work through\n  <a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n  \"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".\n  I'm expecting to get through a chapter or less a day, in order to give things\n  time to percolate properly.  Each day, or perhaps each chapter, I'll post here\n  about anything I find particularly interesting.</p>\n</blockquote>\n\n<p>More than ten months and 26 blog posts later, I've reached the end of the main\nbody of the book -- there's just the appendices to go.\nEven allowing for the hedging, my optimism was adorable.</p>\n\n<p>I don't want to put anyone else off the book by saying that, though!  I expect most people\nwill get through it much faster.  I made a deliberate decision at the start to write up\neverything I learned as I worked through it, and that, I think, has helped me solidify\nthings in my mind much better than I would have done if I'd only been reading it and doing\nthe exercises.  But on the other hand, writing things up does take a <em>lot</em> of time,\nmuch more than the actual learning does.  It's worth it for me, but probably isn't\nfor everyone.</p>\n\n<p>So, what next?  I've finished the main body of the book, and built up a decent backlog\nas I did so.  What do I need to do before I can treat my \"LLM from scratch\" journey as done?\nAnd what other ideas have come up while I worked through it that might be good bases for\nfuture, similar series?</p>\n<p>There are a few sources of ideas for this -- from the book itself and its supplementary\nmaterial, from notes I've made as I went along, and from other things that I've kept on\na mental checklist.</p>\n\n<h3 id=\"the-appendices-and-supplementary-material\">The appendices and supplementary material</h3>\n\n<p>There are five appendices:</p>\n\n<ul>\n<li>A: An introduction to PyTorch</li>\n<li>B: References and further reading</li>\n<li>C: Exercise solutions</li>\n<li>D: Adding bells and whistles to the training loop</li>\n<li>E: Parameter-efficient fine-tuning with LoRA</li>\n</ul>\n\n<p>Raschka also gives a link at the end of chapter 7 to a notebook showing how to do\nfurther fine tuning using <a href=\"https://github.com/rasbt/LLMs-from-scratch/tree/main/ch07/04_preference-tuning-with-dpo\">Direct Preference Optimization</a>,\nwhich also looks fascinating, and he's working on a new project,\n\"<a href=\"https://github.com/rasbt/reasoning-from-scratch\">Build a reasoning model (from scratch)</a>\".</p>\n\n<h3 id=\"things-ive-deferred-myself\">Things I've deferred myself</h3>\n\n<p>While working through the book, I've deliberately deferred various\nthings.  I'd kind of lost track of all of them, so I gave ChatGPT the source\nmarkdown for all of the posts in this series, and asked it to find where I'd done that.\nIt did an amazing job!  There were three categories: long context and attention efficiency,\nmaths, and optimisers.</p>\n\n<h4 id=\"long-context-and-attention-efficiency\">Long context and attention efficiency.</h4>\n\n<p>The model we've built in the book has\na context length of 1,024 tokens, and is <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>O</mi><mo stretchy=\"false\">&#x00028;</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">&#x00029;</mo></mrow></math> in both space and time\nwith respect to the number of tokens you feed it.  There are lots of things that\npeople do to work around that.  Things I need to learn:</p>\n\n<ul>\n<li><strong>The KV cache</strong>.  This is basic stuff and I feel I sorta-kinda understand it, but\nI haven't written about it so I can't be sure.  It's a pretty obvious enhancement\nto avoid repeating work when generating autoregressively -- that is, the normal\nsetup where in order to generate <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>n</mi></mrow></math> tokens, we give the model its input,\nsample our first token from its predictions, then feed the whole thing -- the input\nand that first token -- back in for the second token, and so on.  Obviously,\nbecause attention is causal, we're doing exactly the same work every time for\nall of the tokens in each round apart from the last one, so it makes sense\nto cache things.  The result is that generating the first token is still <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>O</mi><mo stretchy=\"false\">&#x00028;</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">&#x00029;</mo></mrow></math>, but\nsubsequent ones will be something more like <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>O</mi><mo stretchy=\"false\">&#x00028;</mo><mi>n</mi><mo stretchy=\"false\">&#x00029;</mo></mrow></math> each.  That's why real-world modern models tend to take\na while pondering before they generate the first token but then speed up -- they\nneed to fill their cache.</li>\n<li><strong>FlashAttention</strong> and related things: there are lots of ways people have found to reduce the cost of\nattention generally, but this seems to be the most popular one, or at least the best\nto get started with.</li>\n<li><strong>Better positional embeddings</strong>: the context length of our GPT-2-style LLM is fixed\nin part because you need position embeddings for every possible input position.\nThat means that we can never extend it.  More modern LLMs use better ways to\nrepresent positions -- Rotary Position Embeddings (RoPE) look like they're\nvery popular.</li>\n</ul>\n\n<h4 id=\"maths\">Maths</h4>\n\n<p>I really want to understand softmax at a better level than \"it's a magic thing\nthat turns logits into probabilities\".  I'd also like to learn more about higher-order\ntensor operations -- the ones that we use in the book are essentially treating\nthe extra dimensions as the batch, but I believe that there's more to it than that.</p>\n\n<h4 id=\"optimisers\">Optimisers</h4>\n\n<p>I really want to understand in reasonable depth what optimisers do.  I know that\nthey make gradient updates work better than they do with simple gradient descent.\nBut how?</p>\n\n<hr />\n\n<p>That was the set of things I noted at the time I wrote the posts so far, but there are a few more that\ncome to mind as I write this.</p>\n\n<h3 id=\"automatic-differentiation-and-the-backward-pass\">Automatic differentiation and the backward pass</h3>\n\n<p>In some comments that he made on posts in this series, <code>Simon</code> said that it seems like this book\nisn't really \"from scratch\", given that we rely on PyTorch's magic to handle the\nbackward pass.</p>\n\n<p>He's 100% right!  I think I understand why it is that way, though.\nThere would be two different ways that I can see for the book to do it:</p>\n\n<ul>\n<li>Manually code a backward pass to go with the forward pass on each of our modules.\nSimon did this, and was kind enough to share his code with me -- it looks like one of those\nthings (like attention) that is pretty hard to get your head around initially, but\nonce it clicks it's super-clear.  Definitely kudos to him for getting it all to work!\nThe problem with this is that I don't think any\nML practitioners do this nowadays, because automatic differentiation\nis there in every popular framework.  So it might be a good learning experience,\nbut also might nudge people into an unprofitable direction.</li>\n<li>Create our own automatic differentiation system.  Andrej Karpathy pops up again\nwhen looking into this; he created <a href=\"https://github.com/karpathy/micrograd\">micrograd</a>,\nwhich handles back-propagation for scalar functions.  That's really clever -- but\nit would be hard, and a bit of a side quest from the point of the book.  Also,\nthe most interesting stuff (at least from what little I know) for automatic\ndifferentiation is how you do it with non-scalars -- the matrices and higher-order\ntensors that our LLM uses.  From what Simon says, this is where you need to use\nthe mysterious Jacobian matrices I've heard about in the context of back-propagation.</li>\n</ul>\n\n<p>I think I'd definitely like to revisit that at some point.</p>\n\n<h3 id=\"tokenisers\">Tokenisers</h3>\n\n<p>Another one from Simon; while the book does explain how tokenisers work, even down\nto a high-level overview of byte-pair encoding, we don't write our own.  Again, I can\nsee why this is -- we load in the GPT-2 weights, so we need to use that model's tokeniser.\nAnd there's no point in writing our own if we're just going to throw it away.</p>\n\n<p>But perhaps a bit of time playing with one would be useful?</p>\n\n<h4 id=\"trying-to-train-the-llm-as-a-base-model\">Trying to train the LLM as a base model</h4>\n\n<p>The book, quite reasonably, shows you how to train your LLM, does a basic train\non a small dataset, and then we switch to downloading the \"pre-cooked\" weights\nfrom OpenAI.  That makes sense given that not every reader will have access to enough\nhardware to really train from scratch.</p>\n\n<p>But given that I was getting a pretty good training speed on my own hardware, perhaps\nI could train a model really from scratch, perhaps using one of the smaller\n<a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\">FineWeb</a> datasets?  Even if\nI can't do it locally, perhaps it might be doable on a rented cloud machine, like\nthe Lambda Labs ones I used when <a href=\"/fine-tuning\">fine-tuning Llama 3</a>?</p>\n\n<p>After all, Andrej Karpathy is training <a href=\"https://github.com/karpathy/nanochat/discussions/1\">a full model that you can chat with for\n$100</a>.</p>\n\n<h3 id=\"building-an-llm-from-scratch-on-my-own\">Building an LLM from scratch on my own.</h3>\n\n<p>I don't think I ever mentioned this on the blog, but one important plan for me is to\ntry to build an LLM from scratch, only using my own blog posts and what I remember --\nno looking at the book.  If I can do that, then I can be reasonably sure that I really\nhave learned it all.</p>\n\n<p>I'm also thinking that I'll do that using a different library -- that is, not PyTorch.\nThat would stop me from regurgitating code that I've learned.  If you're reading this\nwithin a day or so of the post's publication, I'm\n<a href=\"https://x.com/gpjt/status/1985434030880293004\">running a poll on X/Twitter about which framework to use</a>.\nIf you have an opinion, please do stop by and vote :-)</p>\n\n<h3 id=\"mixture-of-experts\">Mixture-of-experts</h3>\n\n<p>It feels like almost every new model these days is an MoE.  I have read a lot around the\nsubject and would love to build on it.  Essentially, instead of having just one feed-forward network\nafter your attention heads, you have several.  In front of them you have a router -- a trainable\nnetwork of some kind -- that tells you which of these \"expert\" FFNs the token should be\nforwarded to.  You then send it to the top (or top <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>k</mi></mrow></math>) experts, while leaving the others\ninactive.  The result is that you have more space (in terms of parameters) for the LLM\nto know about things, but not all of those parameters are active during inference -- so\nyour model is smarter but still fast.</p>\n\n<p>There's a bunch of interesting stuff there, from how you build it in the first place,\nto how you handle the fact that you're processing lots of tokens at once -- multiple\ntokens in each sequence and multiple sequences in a batch.</p>\n\n<p>It would be a pretty cool follow-on to the \"my own LLM\" series, thinking about it.</p>\n\n<h3 id=\"so-what-next\">So, what next?</h3>\n\n<p>I definitely don't think I need to do all of those things in order to wrap up\nthis series.  Here's the subset I'm planning on doing:</p>\n\n<ul>\n<li>Training the full GPT-2 base model myself.  I'm 100% going to try this.</li>\n<li>From the appendices -- anything that surprises me from the one on PyTorch, and\nperhaps from the \"bells and whistles\" in the training loop.  The others I either\nwon't do, or will pick up later.</li>\n<li>Building my own LLM from scratch in a different framework, without using the book.\nThat is, I think, essential, and perhaps would be the crowning post of this series.\nIt would be a nice way to end it, wouldn't it?</li>\n</ul>\n\n<p>For the other things, I think there are some potential future series to write.</p>\n\n<ul>\n<li>Improving context length -- RoPE and other tricks --\nsounds like an excellent series to start on when I'm done with this.  AIs tell\nme that other interesting things to look into would be ALiBi, NTK/YaRN scaling, and positional interpolation.</li>\n<li>Improving performance: the KV cache, FlashAttention, and other performance enhancements\nlikewise feel like they could make a good series.</li>\n<li>I also want to do a separate series on LoRA.  In that, I'll draw on appendix E from\nthis book, but also on other tutorials.</li>\n<li>Likewise DPO, along with other post-training that can be done to make models more\nuseful as chatbots, like Reinforcement Learning.   I'd really like to spend some\ntime understanding that area.  (And Raschka's upcoming reasoning model book might\nfit into that category too.)</li>\n<li>Optimisers: Adam, AdamW, maybe Muon (though the latter scares me a bit).</li>\n<li>The maths -- softmax and higher-order tensor calculations -- also seems to belong\nin another series, perhaps an extension of the various \"maths for AI\" posts I've\ndone in the past.</li>\n<li>Automatic differentiation and the backward pass; that would make a great series.</li>\n<li>A mixture-of-experts model would be excellent fun, I think.</li>\n<li>Tokenisers would be a great stand-alone post, at least at the level that I can\nsee myself covering it.  Perhaps that would develop into a series if I found\nmyself getting sucked in.</li>\n</ul>\n\n<p>I'm certainly not promising that I'll write up all (or even any) of that second list, but they\nall seem really tempting to me right now.  If you're particularly interested in seeing\nmy take on any of them, please do leave a comment below.</p>\n\n<h3 id=\"coming-up\">Coming up...</h3>\n\n<p>I think the next post in this series -- maybe the next several posts -- will be on\ntrying to train the model code provided in the book from scratch to produce my own\nbase model.  Stay tuned!</p>\n\n<p><a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">Here's a link to the next post in this series</a>.</p>\n",
    "description": "On 22 December 2024, I wrote: Over the Christmas break (and probably beyond) I'm planning to work through Sebastian Raschka's book \"Build a Large Language Model (from Scratch)\". I'm expecting to get through a chapter or less a day, in order to give things time to percolate properly. Each day, or perhaps each chapter, I'll post here about anything I find particularly interesting. More than ten months and 26 blog posts later, I've reached the end of the main body of the book -- there's just the ap",
    "is_fulltext": true,
    "source": "Giles' blog",
    "pub_date": "Tue, 04 Nov 2025 00:40:00 +0000",
    "fetched_at": "2026-02-06T00:35:49.621493",
    "url_hash": "f87a8d57ade8f9b7472d1774b37840f9"
  },
  {
    "title": "How to stop being boring",
    "link": "https://www.joanwestenberg.com/how-to-stop-being-boring/",
    "content": "<img src=\"https://images.unsplash.com/photo-1548159417-f283998827c1?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDgxfHxhYnN0cmFjdHxlbnwwfHx8fDE3NzAzMTg2NjN8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000\" alt=\"How to stop being boring\"><p>The most interesting people I know aren&apos;t trying to be interesting. </p><p>Thank God. </p><p>They&apos;re saying what they actually think and wearing what they actually like, pursuing hobbies that genuinely fascinate them, regardless of whether those hobbies are cool. The most mind-numbingly boring people I know are working overtime to seem interesting: curating their book recommendations, workshopping their opinions to be provocative but not too provocative. </p><p>The effort is palpable. And the effort is exactly what makes them forgettable.</p><p>I&apos;ve come to believe that boring = personality edited down to nothing. Somewhere along the way, too many of us learned to sand off our weird edges, to preemptively remove anything that might make someone uncomfortable or make us seem difficult to be around.</p><p>And the result = boredom.</p><h2 id=\"youve-been-editing-yourself\">You&apos;ve been editing yourself</h2><p>Erving Goffman wrote in 1959 about how we all perform versions of ourselves depending on context. What&apos;s less normal is when the performance becomes the only thing left. When you&apos;ve been editing yourself for so long that you&apos;ve forgotten what the original draft looked like.</p><p>This happens gradually. In middle school, you learn that certain enthusiasms are embarrassing. In high school, you learn which opinions are acceptable in your social group. In college, you refine your persona further. By the time you&apos;re an adult, you&apos;ve become so skilled at reading rooms and ajusting accordingly that you don&apos;t even notice you&apos;re doing it. You&apos;ve automated your own inauthenticity.</p><p>This process feels like maturity, or it feels the way we think maturity ought to feel. It feels like growing up and becoming an adult or a professional. And in some sense, I suppose it is. But there&apos;s a difference between reading a room and erasing yourself to fit into it. Reading a room is social intelligence. Erasing yourself to fit into it is something else.</p><p>I can always tell when I&apos;m talking to someone who&apos;s been over-edited. They have opinions, but the opinions are suspiciously well-calibrated. They have interests, but the interests are respectable. They never say anything that makes me uncomfortable or surprised. They&apos;re like a movie that&apos;s been focus-grouped into mediocrity: technically competent and forgettable.</p><h2 id=\"audit-what-youve-hidden\">Audit what you&apos;ve hidden</h2><p>Make a list of everything you&apos;ve stopped saying or admitting to because you worried it was embarrassing. The band you used to love until someone made fun of it. The hobby you dropped because it wasn&apos;t sophisticated enough. The opinion you stopped voicing because people looked at you weird.</p><p>Most people&apos;s cringe lists are surprisingly long. And most of the items on those lists aren&apos;t actually embarrassing in any objective sense. They&apos;re just things that didn&apos;t fit the persona you decided you needed to maintain.</p><p>I stopped telling people I loved pop punk for half a decade. I hadn&apos;t stopped loving it, but I&apos;d learned that pop punk was supposed to be embarrassing, and I wanted to seem cool, or at least not uncool. Almost everyone I know has some version of this story: the authentic enthusiasm they buried because it didn&apos;t fit.</p><p>The things on your cringe list are probably the most interesting things about you. They&apos;re the parts of your personality that survived despite the editing. The fact that you still feel something about them, even if that something is embarrassment, means they&apos;re still alive in there somewhere.</p><h2 id=\"get-it-back\">Get it back</h2><p>The weird parts are never as weird as you think. Or rather, they&apos;re weird in ways that make you memorable. Being the person who&apos;s really into competitive puzzle-solving or birdwatching gives people somthing to remember. Being the person who&apos;s vaguely interested in the same five acceptable topics as everyone else gives them nothing.</p><p>The recovery protocol is simple. Start saying the thing you would normally edit out. Mention the embarrassing enthusiasm. Voice the opinion that might not land well. Do this in low-stakes situations first: with close friends, with strangers you&apos;ll never see again. Notice that the world doesn&apos;t end. Notice that some people respond positively to the unedited version, even if others don&apos;t.</p><p>The people who respond negatively aren&apos;t your people anyway. That&apos;s the benefit of being unedited: it filters your social world. The more you hide who you actually are, the more you attract people who like the persona, which means the more alone you feel even when surrounded by friends.</p><h2 id=\"be-polarizing\">Be polarizing</h2><p>The most memorable people are polarizing. Some people love them; some people find them insufferable. That&apos;s what having an actual personality looks like from the outside. If everyone has a mild positive reaction to you, you&apos;ve probably sanded youself down into a carefully constructed average of what you think people want.</p><p>Christopher Hitchens was polarizing. So was Julia Child. So is anyone you can actually remember meeting. But provocation for its own sake is another form of performance; what actually matters is that you stop preemptively removing the parts of yourself that might provoke a reaction.</p><p>Some people are going to dislike you.</p><p>They&apos;re allowed to.</p><p>That&apos;s the price of being someone worth remembering.</p>",
    "description": "The most interesting people I know aren&apos;t trying to be interesting. Thank God. They&apos;re saying what they actually think and wearing what they actually like, pursuing hobbies that genuinely fascinate them, regardless of whether those hobbies are cool. The most mind-numbingly boring people I know are",
    "is_fulltext": true,
    "source": "Westenberg.",
    "pub_date": "Thu, 05 Feb 2026 19:11:57 GMT",
    "fetched_at": "2026-02-06T00:35:54.898930",
    "url_hash": "d0e40e1f1d7310138b0f5badcace0a65"
  },
  {
    "title": "How I Reversed Amazon's Kindle Web Obfuscation Because Their App Sucked",
    "link": "https://blog.pixelmelt.dev/kindle-web-drm/",
    "content": "<figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://news.ycombinator.com/item?id=45610226&amp;ref=blog.pixelmelt.dev\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">How I bypassed Amazon&#x2019;s Kindle web DRM | Hacker News</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://blog.pixelmelt.dev/content/images/icon/y18.svg\" alt=\"How I Reversed Amazon&apos;s Kindle Web Obfuscation Because Their App Sucked\"><span class=\"kg-bookmark-author\">Hacker News</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://blog.pixelmelt.dev/content/images/thumbnail/y18.svg\" alt=\"How I Reversed Amazon&apos;s Kindle Web Obfuscation Because Their App Sucked\" onerror=\"this.style.display = &apos;none&apos;\"></div></a><figcaption><img src=\"https://images.unsplash.com/photo-1656115914684-2845b7a13476?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fGNhdCUyMGtpbmRsZXxlbnwwfHx8fDE3NjAzMTQ5NDd8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000\" alt=\"How I Reversed Amazon&apos;s Kindle Web Obfuscation Because Their App Sucked\"><p><span style=\"white-space: pre-wrap;\">This article hit #1 on Hacker News, thanks all!</span></p></figcaption></figure><h2 id=\"tldr\">TL;DR</h2><ul><li>I bought my first ebook from amazon</li><li>Amazon&apos;s Kindle Android app was really buggy and crashed a bunch</li><li>Tried to download my book to use with a functioning reader app</li><li>Realized Amazon no longer lets you do that</li><li>Decided to reverse engineer their obfuscation system out of spite</li><li>Discovered multiple layers of protection including randomized alphabets</li><li>Defeated all of them with font matching wizardry</li></ul><h2 id=\"part-1-amazon-made-this-personal\">Part 1: Amazon Made This Personal</h2><h3 id=\"the-one-time-i-tried-to-do-things-the-right-way\">The One Time I Tried To Do Things The Right Way</h3><p>I&apos;ve been reading ebooks from various sources for years. But this time, I thought: &quot;Let&apos;s support the author.&quot;</p><p>Download Kindle app on Android. Open book.</p><p><strong>Crash.</strong></p><h3 id=\"i-just-wanted-to-read-my-book\">I Just Wanted To Read My Book</h3><p>App crashes. Fine, I&apos;ll use the web reader.</p><p>Oh wait, can&apos;t download it for offline reading. What if I&apos;m on a plane?</p><p>Hold on, I can&apos;t even export it to Calibre? Where I keep ALL my other books?</p><p>So let me get this straight:</p><ul><li>I paid money for this book</li><li>I can only read it in Amazon&apos;s broken app</li><li>I can&apos;t download it</li><li>I can&apos;t back it up</li><li>I don&apos;t actually own it</li><li>Amazon can delete it whenever they want</li></ul><p><strong>This is a rental, not a purchase.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://blog.pixelmelt.dev/content/images/2025/10/image.png\" class=\"kg-image\" alt=\"How I Reversed Amazon&apos;s Kindle Web Obfuscation Because Their App Sucked\" loading=\"lazy\" width=\"495\" height=\"354\"><figcaption><span style=\"white-space: pre-wrap;\">This does not say &quot;Rent&quot;</span></figcaption></figure><h3 id=\"it-becomes-personal\">It Becomes Personal</h3><p>I could&apos;ve refunded and &quot;obtained&quot; it in 30 seconds. Would&apos;ve been easier.</p><p>But that&apos;s not the point.</p><p>The point is I PAID FOR THIS BOOK. It&apos;s mine. And I&apos;m going to read it in Calibre with the rest of my library even if I have to reverse engineer their web client to do it.</p><h3 id=\"reversal-time\">Reversal Time</h3><p>Kindle Cloud Reader (the web version) actually works. While looking through the network requests, I spotted this:</p><pre><code>https://read.amazon.com/renderer/render\n</code></pre><p>To download anything, you need:</p><p>1. Session cookies - standard Amazon login</p><p>2. Rendering token - from the startReading API call</p><p>3. ADP session token - extra auth layer</p><p>Sending the same headers and cookies the browser does returns a TAR file.</p><h3 id=\"whats-inside-the-tar\">What&apos;s Inside The TAR?</h3><pre><code>page_data_0_4.json   # The &quot;text&quot; (spoiler: it&apos;s not text)\nglyphs.json          # SVG definitions for every character\ntoc.json             # Table of contents\nmetadata.json        # Book info\nlocation_map.json    # Position mappings</code></pre><h2 id=\"part-3-amazons-obfuscation-layers-of-ebook-hell\">Part 3: Amazon&apos;s Obfuscation Layers of Ebook Hell</h2><p>Downloaded the first few pages, expected to see text. Got this instead:</p><pre><code class=\"language-JSON\">{\n  &quot;type&quot;: &quot;TextRun&quot;,\n  &quot;glyphs&quot;: [24, 25, 74, 123, 91, 18, 19, 30, 4, ...],\n  &quot;style&quot;: &quot;paragraph&quot;\n}</code></pre><p>These aren&apos;t letters. They&apos;re glyph IDs. Character &apos;T&apos; isn&apos;t Unicode 84, it&apos;s glyph 24.</p><p>And glyph 24 is just a series of numbers that define a stroke path, its just an image of a letter.</p><p>It&apos;s a substitution cipher! Each character maps to a non-sequential glyph ID.</p><h3 id=\"the-alphabet-changes-every-five-pages\">The Alphabet Changes Every. Five. Pages.</h3><p>Downloaded the next batch of pages. Same letter &apos;T&apos; is now glyph 87.</p><p>Next batch? Glyph 142.</p><p><strong>They randomize the entire alphabet on EVERY request.</strong></p><p>This means:</p><ul><li>You can only get 5 pages at a time (API hard limit)</li><li>Each request gets completely new glyph mappings</li><li>Glyph IDs are meaningless across requests</li><li>You can&apos;t build one mapping table for the whole book</li></ul><h3 id=\"let-me-show-you-how-bad-this-is\">Let Me Show You How Bad This Is</h3><p>For my 920-page book:</p><ul><li><strong>184 separate API requests</strong> needed</li><li><strong>184 different random alphabets</strong> to crack</li><li><strong>361 unique glyphs</strong> discovered (a-z, A-Z, punctuation, ligatures)</li><li><strong>1,051,745 total glyphs</strong> to decode</li></ul><h3 id=\"fake-font-hints-theyre-getting-sneaky\">Fake Font Hints (They&apos;re Getting Sneaky)</h3><p>Some SVG paths contained this garbage:</p><pre><code>M695.068,0 L697.51,-27.954 m3,1 m1,6 m-4,-7 L699.951,-55.908 ...</code></pre><p>Looking at it, we see these tiny <code>m3,1 m1,6 m-4,-7</code> commands, they are micro MoveTo operations.</p><p><strong>Why this is evil:</strong></p><ul><li>Browsers handle them fine (native Path2D)</li><li>Python SVG libraries create spurious connecting lines</li><li>Makes glyphs look corrupted when rendered naively</li><li>Breaks path-sampling approaches</li></ul><p>This is deliberate anti-scraping. The glyphs render perfectly in browser but make it so we cant just compare paths in our parser.</p><p>Take a look</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://blog.pixelmelt.dev/content/images/2025/10/image-2.png\" class=\"kg-image\" alt=\"How I Reversed Amazon&apos;s Kindle Web Obfuscation Because Their App Sucked\" loading=\"lazy\" width=\"1768\" height=\"609\" srcset=\"https://blog.pixelmelt.dev/content/images/size/w600/2025/10/image-2.png 600w, https://blog.pixelmelt.dev/content/images/size/w1000/2025/10/image-2.png 1000w, https://blog.pixelmelt.dev/content/images/size/w1600/2025/10/image-2.png 1600w, https://blog.pixelmelt.dev/content/images/2025/10/image-2.png 1768w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Fun!</span></figcaption></figure><p>Eventually I figured out that filling in the complete path mitigated this.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://blog.pixelmelt.dev/content/images/2025/10/id_131_bookerly.png\" class=\"kg-image\" alt=\"How I Reversed Amazon&apos;s Kindle Web Obfuscation Because Their App Sucked\" loading=\"lazy\" width=\"200\" height=\"200\"></figure><h3 id=\"multiple-font-variants\">Multiple Font Variants</h3><p>Not just one font. FOUR variants:</p><ul><li>bookerly_normal (99% of glyphs)</li><li>bookerly_italic (emphasis)</li><li>bookerly_bold (headings)</li><li>bookerly_bolditalic (emphasized headings)</li></ul><p>Plus special ligatures: ff, fi, fl, ffi, ffl</p><p>More variations = more unique glyphs to crack = more pain.</p><h3 id=\"ocr-is-mid-my-failed-attempt\">OCR Is Mid (My Failed Attempt)</h3><p>Tried running OCR on rendered glyphs. Results:</p><ul><li>178/348 glyphs recognized (51%)</li><li>170 glyphs failed completely</li></ul><p>OCR just sucks at single characters without context. Confused &apos;l&apos; with &apos;I&apos; with &apos;1&apos;. Couldn&apos;t handle punctuation. Gave up on ligatures entirely.</p><p>OCR probably need words and sentences to work well.</p><h2 id=\"part-4-the-solution-that-actually-worked\">Part 4: The Solution That Actually Worked</h2><p>Every request includes `glyphs.json` with SVG path definitions:</p><pre><code class=\"language-JSON\">{\n  &quot;24&quot;: {\n    &quot;path&quot;: &quot;M 450 1480 L 820 1480 L 820 0 L 1050 0 L 1050 1480 ...&quot;,\n    &quot;fontFamily&quot;: &quot;bookerly_normal&quot;\n  },\n  &quot;87&quot;: {\n    &quot;path&quot;: &quot;M 450 1480 L 820 1480 L 820 0 L 1050 0 L 1050 1480 ...&quot;,\n    &quot;fontFamily&quot;: &quot;bookerly_normal&quot;\n  }\n}</code></pre><p><strong>Glyph IDs change, but SVG shapes don&apos;t.</strong></p><h3 id=\"why-direct-svg-comparison-failed\">Why Direct SVG Comparison Failed</h3><p>First attempt: normalize and compare SVG path coordinates.</p><p>Failed because:</p><ul><li>Coordinates vary slightly</li><li>Path commands represented differently</li></ul><h3 id=\"pixel-perfect-matching\">Pixel-Perfect Matching</h3><p>Screw coordinate comparison. Let&apos;s just render everything and compare pixels.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://blog.pixelmelt.dev/content/images/2025/10/id_028_bookerly.png\" class=\"kg-image\" alt=\"How I Reversed Amazon&apos;s Kindle Web Obfuscation Because Their App Sucked\" loading=\"lazy\" width=\"128\" height=\"128\"><figcaption><span style=\"white-space: pre-wrap;\">Render that A</span></figcaption></figure><p>1. <strong>Render every SVG as an image</strong></p><ul><li>Use cairosvg (lets us handle those fake font hints correctly)</li><li>Render at 512 x 512px for accuracy</li></ul><p>2. <strong>Generate perceptual hashes</strong></p><ul><li>Hash each rendered image</li><li>The hash becomes the unique identifier</li><li>Same shape = same hash, regardless of glyph ID</li></ul><p>3. <strong>Build normalized glyph space</strong></p><ul><li>Map all 184 random alphabets to hash-based IDs</li><li>Now glyph &quot;a1b2c3d4...&quot; always means letter &apos;T&apos;</li></ul><p>4. <strong>Match to actual characters</strong></p><ul><li>Download Bookerly TTF fonts</li><li>Render every character (A-Z, a-z, 0-9, punctuation)</li><li>Use SSIM (Structural Similarity Index) to match</li></ul><h3 id=\"why-ssim-is-perfect-for-this\">Why SSIM Is Perfect For This</h3><p>SSIM compares image structure, not pixels directly. It handles:</p><ul><li>Slight rendering differences</li><li>Anti-aliasing variations</li><li>Minor scaling issues</li></ul><p>For each unknown glyph, find the TTF character with highest SSIM score. That&apos;s your letter.</p><h3 id=\"handling-the-edge-cases\">Handling The Edge Cases</h3><p><strong>Ligatures:</strong> ff, fi, fl, ffi, ffl</p><ul><li>These are single glyphs for multiple characters</li><li>Had to add them to TTF library manually</li></ul><p><strong>Special characters:</strong> em-dash, quotes, bullets</p><ul><li>Extended character set beyond basic ASCII</li><li>Matched against full Unicode range in Bookerly</li></ul><p><strong>Font variants:</strong> Bold, italic, bold-italic</p><ul><li>Built separate libraries for each variant</li><li>Match against all libraries, pick best score</li></ul><h2 id=\"part-5-the-moment-it-all-worked\">Part 5: The Moment It All Worked</h2><h3 id=\"final-statistics\">Final Statistics</h3><pre><code>=== NORMALIZATION PHASE ===\nTotal batches processed: 184\nUnique glyphs found: 361\nTotal glyphs in book: 1,051,745\n\n=== MATCHING PHASE ===\nSuccessfully matched 361/361 unique glyphs (100.00%)\nFailed to match: 0 glyphs\nAverage SSIM score: 0.9527\n\n=== DECODED OUTPUT ===\nTotal characters: 5,623,847\nPages: 920</code></pre><p>Perfect. Every single character decoded correctly.</p><h3 id=\"epub-reconstruction-with-perfect-formatting\">EPUB Reconstruction With Perfect Formatting</h3><p>The JSON includes positioning for every text run:</p><pre><code class=\"language-JSON\">{\n  &quot;glyphs&quot;: [24, 25, 74],\n  &quot;rect&quot;: {&quot;left&quot;: 100, &quot;top&quot;: 200, &quot;right&quot;: 850, &quot;bottom&quot;: 220},\n  &quot;fontStyle&quot;: &quot;italic&quot;,\n  &quot;fontWeight&quot;: 700,\n  &quot;fontSize&quot;: 12.5,\n  &quot;link&quot;: {&quot;positionId&quot;: 7539}\n}</code></pre><p>I used this to preserve:</p><ul><li>Paragraph breaks (Y-coordinate changes)</li><li>Text alignment (X-coordinate patterns)</li><li>Bold/italic styling</li><li>Font sizes</li><li>Internal links</li></ul><p>The final EPUB is near indistinguishable from the original!</p><h2 id=\"the-real-conclusion\">The Real Conclusion</h2><p>Amazon put real effort into their web obfuscation.</p><h3 id=\"was-it-worth-it\">Was It Worth It?</h3><p>To read one book? No.</p><p>To prove a point? Absolutely.</p><p>To learn about SVG rendering, perceptual hashing, and font metrics? Probably yes.</p><h3 id=\"use-this-knowledge-responsibly\">Use This Knowledge Responsibly</h3><p>This is for backing up books YOU PURCHASED.</p><p>Don&apos;t get me sued into oblivion thanks.</p><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-text\">Due to the nature of this post, if you are in any way affiliated with Amazon, please reach out to pixelmelt + at + protonmail.com.</div></div>",
    "description": "As it turns out they don't actually want you to do this (and have some interesting ways to stop you)",
    "is_fulltext": true,
    "source": "Cats with power tools",
    "pub_date": "Wed, 15 Oct 2025 19:29:16 GMT",
    "fetched_at": "2026-02-06T00:36:28.751737",
    "url_hash": "bddc2e07fa0fb875a13407b8c28381de"
  },
  {
    "title": "Wall Street just lost $285 billion because of 13 markdown files",
    "link": "https://martinalderson.com/posts/wall-street-lost-285-billion-because-of-13-markdown-files/?utm_source=rss",
    "content": "Anthropic's 'legal tool' that triggered a $285bn selloff is 156KB of markdown. The panic reveals a hard truth about the future of software.",
    "description": "Anthropic's 'legal tool' that triggered a $285bn selloff is 156KB of markdown. The panic reveals a hard truth about the future of software.",
    "is_fulltext": false,
    "source": "Martin Alderson",
    "pub_date": "Thu, 05 Feb 2026 00:00:00 GMT",
    "fetched_at": "2026-02-06T00:36:29.327358",
    "url_hash": "26debc212b09ae05914c142cf0534a39"
  }
]