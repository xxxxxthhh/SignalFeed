[
  {
    "title": "Anthropic's public benefit mission",
    "link": "https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/#atom-everything",
    "content": "\n    <p>Someone <a href=\"https://news.ycombinator.com/item?id=47008560#47008978\">asked</a> if there was an Anthropic equivalent to <a href=\"https://simonwillison.net/2026/Feb/13/openai-mission-statement/\">OpenAI's IRS mission statements over time</a>.</p>\n<p>Anthropic are a \"public benefit corporation\" but not a non-profit, so they don't have the same requirements to file public documents with the IRS every year.</p>\n<p>But when I asked Claude it ran a search and dug up this <a href=\"https://drive.google.com/drive/folders/1ImqXYv9_H2FTNAujZfu3EPtYFD4xIlHJ\">Google Drive folder</a> where Zach Stein-Perlman shared Certificate of Incorporation documents he <a href=\"https://ailabwatch.substack.com/p/anthropics-certificate-of-incorporation\">obtained from the State of Delaware</a>!</p>\n<p>Anthropic's are much less interesting that OpenAI's. The earliest document from 2021 states:</p>\n<blockquote>\n<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced Al for the cultural, social and technological improvement of humanity.</p>\n</blockquote>\n<p>Every subsequent document up to 2024 uses an updated version which says:</p>\n<blockquote>\n<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced AI for the long term benefit of humanity.</p>\n</blockquote>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a></p>\n\n\n\n",
    "description": "Someone asked if there was an Anthropic equivalent to OpenAI's IRS mission statements over time. Anthropic are a \"public benefit corporation\" but not a non-profit, so they don't have the same requirements to file public documents with the IRS every year. But when I asked Claude it ran a search and dug up this Google Drive folder where Zach Stein-Perlman shared Certificate of Incorporation documents he obtained from the State of Delaware! Anthropic's are much less interesting that OpenAI's. The e",
    "is_fulltext": true,
    "source": "Simon Willison's Weblog",
    "pub_date": "2026-02-13T23:59:51+00:00",
    "fetched_at": "2026-02-14T00:37:16.832670",
    "url_hash": "6bd8e3ecfc434c26aa8352ee7dfe65e0"
  },
  {
    "title": "Testing Reachy Mini - Hugging Face's Pi powered robot",
    "link": "https://www.jeffgeerling.com/blog/2026/testing-reachy-mini-hugging-face-robot/",
    "content": "<p>When I saw <a href=\"https://www.youtube.com/watch?v=acBv3G8r-1Y\">Jensen Huang introduce the Reachy Mini at CES</a>, I thought it was a gimmick. His keynote showed this little robot responding to human input, turning its head to look at a TODO list on the wall, sending emails, and turning drawings into architectural renderings with motion.</p>\n<figure class=\"insert-image\"><img src=\"https://www.jeffgeerling.com/blog/2026/testing-reachy-mini-hugging-face-robot/reachy-mini-robot-hero.jpeg\"\n alt=\"Reachy Mini robot controlled by Framework laptop\" width=\"700\" height=\"auto\">\n</figure>\n\n<p>HuggingFace and Pollen robotics sent me a <a href=\"https://huggingface.co/spaces/pollen-robotics/Reachy_Mini\">Reachy Mini</a> to test, and, well, at least if you're looking to replicate that setup in the keynote, it's not, as Jensen put it, &quot;utterly trivial now.&quot;</p>",
    "description": "When I saw Jensen Huang introduce the Reachy Mini at CES, I thought it was a gimmick. His keynote showed this little robot responding to human input, turning its head to look at a TODO list on the wall, sending emails, and turning drawings into architectural renderings with motion. HuggingFace and Pollen robotics sent me a Reachy Mini to test, and, well, at least if you're looking to replicate that setup in the keynote, it's not, as Jensen put it, &quot;utterly trivial now.&quot;",
    "is_fulltext": false,
    "source": "Jeff Geerling",
    "pub_date": "Fri, 13 Feb 2026 09:00:00 -0600",
    "fetched_at": "2026-02-14T00:37:17.201828",
    "url_hash": "af3666d13508779eb6fb8cfcc8ea6084"
  },
  {
    "title": "Gadget Review: Topdon TS004 Thermal Monocular ★★★★⯪",
    "link": "https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/",
    "content": "<p>I love thermal imaging cameras. They're great for spotting leaking pipes, inefficient appliances, and showing how <a href=\"https://shkspr.mobi/blog/2024/06/infrared-infrastructure/\">full a septic tank</a> is. The good folks at Topdon have sent me their latest thermal camera to review - it is specifically designed for spotting wildlife.</p>\n\n<p>This is the <a href=\"https://www.topdon.com/products/ts004\">TS004 Thermal Monocular</a>:</p>\n\n<img src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/topdon.webp\" alt=\"Photo of a dark green tube with various buttons on it. It fits snugly in the hand.\" width=\"1024\" height=\"1024\" class=\"aligncenter size-full wp-image-67746\">\n\n<p>Let's put it through its paces!</p>\n\n<h2 id=\"hardware\"><a href=\"https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/#hardware\">Hardware</a></h2>\n\n<p>This is a chunky bit of kit and fits nicely in the hand. It's well weighted and feels sturdy.</p>\n\n<p>The rubber seal fits tightly around your eye and is excellent at keeping light out. The screen is set a little way back, so is easy to focus on. Taking a photo of the screen itself was a little tricky - here's what you can expect to see when using the settings menu:</p>\n\n<img src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/Viewfinder.webp\" alt=\"A menu overlayed on a thermal image.\" width=\"1024\" height=\"768\" class=\"aligncenter size-full wp-image-67812\">\n\n<p>The focus knob near the viewfinder is a little stiff, but it turns silently.</p>\n\n<p>There's a rubber lens cover which is attached and can be easily tucked away next to the standard tripod mount. It comes with a lanyard strap, so you're unlikely to drop it. The buttons are well spaced and respond quickly.</p>\n\n<img src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/ts004-buttons.webp\" alt=\"Photo of buttons. Power, mode, zoom, and photo.\" width=\"966\" height=\"726\" class=\"aligncenter size-full wp-image-67745\">\n\n<p>The USB-C port has a rubber flap to keep out moisture.</p>\n\n<p>OK, let's take some snaps!</p>\n\n<h2 id=\"photos\"><a href=\"https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/#photos\">Photos</a></h2>\n\n<p>Photo quality is pretty good - although limited by the technology behind the thermal sensor. The TS004 has a thermal resolution of 256x192 and images are upscaled to 640x480.</p>\n\n<img src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/Birds-In-Tree.jpg\" alt=\"White hot spots of birds in a tree.\" width=\"640\" height=\"480\" class=\"aligncenter size-full wp-image-67777\">\n\n<p>One thing to note, the user-interface is burned in to the photos. So if you want the battery display on screen, it will also appear on the photo. Similarly, things like the range-finder appear in the image.</p>\n\n<p>There's a <em>reasonable</em> AI built in. It is designed to tell you what sort of wildlife you've spotted. In some cases, it is pretty accurate! A woman walked by me while I was looking for wildlife - here's her photo:</p>\n\n<img src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/human.jpg\" alt=\"A thermal photo of a woman. Her uncovered legs and hands are warmer than her clothed body.\" width=\"640\" height=\"480\" class=\"aligncenter size-full wp-image-67742\">\n\n<p>Nifty!</p>\n\n<p>Here's a photo of a fox:</p>\n\n<img src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/Wild-Boar.jpg\" alt=\"Thermal image. A dog-shaped object glows. It is labelled &quot;Wild Boar&quot;.\" width=\"640\" height=\"480\" class=\"aligncenter size-full wp-image-67741\">\n\n<p>There are remarkably few wild boars in London!</p>\n\n<h2 id=\"video\"><a href=\"https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/#video\">Video</a></h2>\n\n<p>Video is also 640x480. It is a <em>very</em> smooth 42.187 FPS and a rather chunky 2,162 Kbps - leading to a file size of around 20MB per minute. With around 30GB of in-built storage, that shouldn't be a problem though. There's no audio available and, just like the photos, the UI is burned into the picture.</p>\n\n<p>Here are a couple of sample videos I shot. In them, I cycle through the colour modes and zoom levels.</p>\n\n<p>First, an urban fox foraging in London:</p>\n\n<p></p><div style=\"width: 620px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-67739-3\" width=\"620\" height=\"465\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/fox.mp4?_=3\"><a href=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/fox.mp4\">https://shkspr.mobi/blog/wp-content/uploads/2026/02/fox.mp4</a></video></div><p></p>\n\n<p>Second, some parakeets flapping around a tree:</p>\n\n<p></p><div style=\"width: 620px;\" class=\"wp-video\"><video class=\"wp-video-shortcode\" id=\"video-67739-4\" width=\"620\" height=\"465\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/Birds-In-Flight.mp4?_=4\"><a href=\"https://shkspr.mobi/blog/wp-content/uploads/2026/02/Birds-In-Flight.mp4\">https://shkspr.mobi/blog/wp-content/uploads/2026/02/Birds-In-Flight.mp4</a></video></div><p></p>\n\n<p>I'm impressed with the smoothness of the video and how well it picks up heat even from relatively far away.</p>\n\n<h2 id=\"linux\"><a href=\"https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/#linux\">Linux</a></h2>\n\n<p>Bizarrely, on Linux it shows up as <code>1d6b:0101 Linux Foundation Audio Gadget</code>. It presents as a standard USB drive and you can easily copy files to and from it. 100% compatibility!</p>\n\n<p>You can't use it as a WebCam - for anything more complicated than copying files, you need to use the official app.</p>\n\n<h2 id=\"app\"><a href=\"https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/#app\">App</a></h2>\n\n<p>The <a href=\"https://play.google.com/store/apps/details?id=com.topdon.topInfrared\">TopInfrared App for Android</a> is reasonably good. It connects to the camera via WiFi and offers some useful features. Most impressively, it live-streams the camera's view to your phone.</p>\n\n<p>From there you can take photos or videos and have them saved straight onto your device. Handy if you've set the camera up outside and want to view it from somewhere warmer.</p>\n\n<p>Frustratingly, it isn't possible to set all the options on the camera using the app. For that you need to go back to the menu on the camera - which is slightly laborious.</p>\n\n<p>The app isn't mandatory for most operations - thankfully - but it is the only way to set the time and date on the monocular.  You will also need it if there are any firmware updates.</p>\n\n<p>If you don't need the app, you can turn off the WiFi to save some battery life.</p>\n\n<h2 id=\"drawbacks\"><a href=\"https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/#drawbacks\">Drawbacks</a></h2>\n\n<p>The device works - and is great for wildlife spotting - but there are a few little niggles.  I've fed these back to the manufacturer and have included their responses.</p>\n\n<ul>\n<li><p>There's no EXIF in the photos, or any way to get thermal data out of the images.</p>\n\n<ul>\n<li>\"These products focus on image clarity, high sensitivity, and low latency. For example, temperature-measurement thermal cameras typically run at 25 Hz, while the TS004 operates at 50 Hz for smoother viewing. Devices that include EXIF temperature data, raw thermal export, and analytical tools are measurement-focused thermal cameras, which are based on a different design and use case.\"</li>\n</ul></li>\n<li><p>As mentioned, having the UI burned into the photos and videos is slightly annoying.</p>\n\n<ul>\n<li>You can turn off the UI elements on screen which stops them appearing in the photo.</li>\n</ul></li>\n<li><p>The range-finder only works in yards and, while seemingly accurate, isn't overly helpful to those of us who think in metric!</p>\n\n<ul>\n<li>\"Unit switching will be available in the March firmware update\"</li>\n</ul></li>\n<li><p>Once you sync the time with the monocular, all the filenames are timestamped like <code>2026_02_09_12345678</code> but it appears to be hardwired to Hong Kong Time (UTC+8) - so your dates and times might be a little out.</p>\n\n<ul>\n<li>\"We will investigate it and see if it can be implemented in a future update\"</li>\n</ul></li>\n<li><p>The AI detection feature doesn't seem particularly tuned for the UK.</p>\n\n<ul>\n<li>\"Due to hardware limitations, the current recognition is relatively basic, so there is limited room for significant improvement\"</li>\n</ul></li>\n</ul>\n\n<p>In terms of hardware limitations, there's no GPS. I would expect a device in this price-range to have basic GPS functionality to allow you to easily tag photos.</p>\n\n<p>None of these are show-stoppers, but for a device this expensive they are an annoyance.</p>\n\n<h2 id=\"price\"><a href=\"https://shkspr.mobi/blog/2026/02/gadget-review-topdon-ts004-thermal-monocular/#price\">Price</a></h2>\n\n<p>OK, so you want to spot birds in trees and wild boars foraging in the forest - what'll this cost you?</p>\n\n<p>Close to £400 - you can <a href=\"https://amzn.to/4rCrKeq\">use code <code>TERENCE15</code> for a 15% discount until 16 February 2026.</a></p>\n\n<p>The price of thermal imaging equipment is high and this is a fairly niche form-factor. It is easy to use, has a great range, and the rubber eyepiece is much nicer than staring at a bright phone screen.  The battery life is excellent and you certainly can't complain about the generous storage space.</p>\n\n<p>There are some minor irritations as discussed above, but it is an exceptional bit of kit if you like to explore the environment. Are you going to spot any cryptids with it? Who knows! But you'll have lots of fun discovering the natural world around you.</p>\n",
    "description": "I love thermal imaging cameras. They&#039;re great for spotting leaking pipes, inefficient appliances, and showing how full a septic tank is. The good folks at Topdon have sent me their latest thermal camera to review - it is specifically designed for spotting wildlife. This is the TS004 Thermal Monocular: Let&#039;s put it through its paces! Hardware This is a chunky bit of kit and fits nicely in…",
    "is_fulltext": true,
    "source": "Terence Eden’s Blog",
    "pub_date": "Fri, 13 Feb 2026 12:34:17 +0000",
    "fetched_at": "2026-02-14T00:37:27.407208",
    "url_hash": "e457c22fe9311f1564b537f3b0eea508"
  },
  {
    "title": "How can I distinguish between the numeric keypad 0 and the top-row 0 in the WM_CHAR message?",
    "link": "https://devblogs.microsoft.com/oldnewthing/20260213-00/?p=112062",
    "content": "<p>Last time, we looked at <a title=\"How can I distinguish between the numeric keypad 0 and the top-row 0 in the WM_KEY­DOWN message?\" href=\"https://devblogs.microsoft.com/oldnewthing/20260212-00/?p=112059\"> how to distinguish the numeric keypad 0 and the top-row 0 in the <code>WM_<wbr />KEY­DOWN</code> message</a>. We may as well look at the analogous table for <code>WM_<wbr />CHAR</code>.</p>\n<table style=\"border-collapse: collapse;\" border=\"1\" cellspacing=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<th>Event</th>\n<th>wParam</th>\n<th>Extended?</th>\n</tr>\n<tr>\n<td>Numpad0 with NumLock on</td>\n<td><tt>VK_0</tt></td>\n<td>0</td>\n</tr>\n<tr>\n<td>Numpad0 with NumLock off</td>\n<td style=\"text-align: center;\" colspan=\"2\">(no <code>WM_CHAR</code>)</td>\n</tr>\n<tr>\n<td><kbd>Ins</kbd> key</td>\n<td style=\"text-align: center;\" colspan=\"2\">(no <code>WM_CHAR</code>)</td>\n</tr>\n<tr>\n<td>0 on top row</td>\n<td><tt>VK_0</tt></td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>I got the name <code>VK_0</code> from this comment block in <tt>winuser.h</tt>.</p>\n<pre>/*\n * VK_0 - VK_9 are the same as ASCII '0' - '9' (0x30 - 0x39)\n * 0x3A - 0x40 : unassigned\n * VK_A - VK_Z are the same as ASCII 'A' - 'Z' (0x41 - 0x5A)\n */\n</pre>\n<p>Uh-oh. The extended bit doesn&#8217;t distinguish between the two. They both show up as <code>VK_0</code>, non-extended.</p>\n<p>What changes is something not in the above table: The scan code.</p>\n<p>So let&#8217;s convert the scan code back to a virtual key.</p>\n<pre>auto vk_from_scan = MapVirtualKey((lParam &gt;&gt; 16) &amp; 0xFF, MAPVK_VSC_TO_VK);\n</pre>\n<table style=\"border-collapse: collapse;\" border=\"1\" cellspacing=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<th>Event</th>\n<th>wParam</th>\n<th>Extended?</th>\n<th><tt>vk_from_scan</tt></th>\n</tr>\n<tr>\n<td>Numpad0 with NumLock on</td>\n<td><tt>VK_0</tt></td>\n<td>0</td>\n<td><tt>VK_INSERT</tt></td>\n</tr>\n<tr>\n<td>Numpad0 with NumLock off</td>\n<td style=\"text-align: center;\" colspan=\"3\">(no <code>WM_CHAR</code>)</td>\n</tr>\n<tr>\n<td><kbd>Ins</kbd> key</td>\n<td style=\"text-align: center;\" colspan=\"3\">(no <code>WM_CHAR</code>)</td>\n</tr>\n<tr>\n<td>0 on top row</td>\n<td><tt>VK_0</tt></td>\n<td>0</td>\n<td><tt>VK_0</tt></td>\n</tr>\n</tbody>\n</table>\n<p>So we can infer which zero was pressed by taking the scan code, mapping it to a virtual key, and seeing whether it&#8217;s the <kbd>Ins</kbd> key (from the numeric keypad) or the <kbd>0</kbd> key (from the top row).</p>\n<p>But wait, we&#8217;re not done yet.</p>\n<p>There are ways to type the character <tt>0</tt> without using the numeric keypad or the top row. For example, you can hold the <kbd>Alt</kbd> key and then type <kbd>4</kbd>,<kbd>8</kbd> on the numeric keypad, and that will type a <tt>0</tt>. I tried it out, and the <code>vk_from_scan</code> was <code>VK_<wbr />MENU</code>, which is the virtual key code for the <kbd>Alt</kbd> key. Another way of entering a <tt>0</tt> is by using an input method editor (IME). Or there might be a custom keyboard layout that generates a <tt>0</tt> through some wacky chord sequence.</p>\n<p>Therefore, if the <code>vk_<wbr />from_<wbr />scan</code> is neither <code>VK_<wbr />INSERT</code> nor <code>VK_0</code>, you have to conclude that the <tt>0</tt> was entered by some means other than the numeric keypad or the top row.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260213-00/?p=112062\">How can I distinguish between the numeric keypad 0 and the top-row 0 in the &lt;CODE&gt;WM_&lt;WBR&gt;CHAR&lt;/CODE&gt; message?</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>\n",
    "description": "See if it matches the scan code. The post How can I distinguish between the numeric keypad 0 and the top-row 0 in the &lt;CODE&gt;WM_&lt;WBR&gt;CHAR&lt;/CODE&gt; message? appeared first on The Old New Thing.",
    "is_fulltext": true,
    "source": "The Old New Thing",
    "pub_date": "Fri, 13 Feb 2026 15:00:00 +0000",
    "fetched_at": "2026-02-14T00:37:31.274195",
    "url_hash": "16a46ecb15097cee52566139de6e58bf"
  },
  {
    "title": "The Final Bottleneck",
    "link": "https://lucumr.pocoo.org/2026/2/13/the-final-bottleneck/",
    "content": "<p>Historically, writing code was slower than reviewing code.</p>\n<p>It might not have felt that way, because code reviews sat in queues until\nsomeone got around to picking it up.  But if you compare the\nactual acts themselves, creation was usually the more expensive part.  In teams\nwhere people both wrote and reviewed code, it never felt like &#8220;we should\nprobably program slower.&#8221;</p>\n<p>So when more and more people tell me they no longer know what code is in their\nown codebase, I feel like something is very wrong here and it&#8217;s time to\nreflect.</p>\n<h2>You Are Here</h2>\n<p>Software engineers often believe that <a href=\"/2020/1/1/async-pressure/\">if we make the bathtub\nbigger</a>, overflow disappears.  It doesn&#8217;t.\n<a href=\"https://en.wikipedia.org/wiki/OpenClaw\">OpenClaw</a> right now has north of 2,500\npull requests open.  That&#8217;s a big bathtub.</p>\n<p>Anyone who has worked with queues knows this: if input grows faster than\nthroughput, you have an accumulating failure.  At that point, backpressure and\nload shedding are the only things that retain a system that can still operate.</p>\n<p>If you have ever been in a Starbucks overwhelmed by mobile orders, you know the\nfeeling.  The in-store experience breaks down.  You no longer know how many\norders are ahead of you.  There is no clear line, no reliable wait estimate, and\noften no real cancellation path unless you escalate and make noise.</p>\n<p>That is what many AI-adjacent open source projects feel like right now.  And\nincreasingly, that is what a lot of internal company projects feel like in\n&#8220;AI-first&#8221; engineering teams, and that&#8217;s not sustainable.  You can&#8217;t triage, you\ncan&#8217;t review, and many of the PRs cannot be merged after a certain point because\nthey are too far out of date. And the creator might have lost the motivation to\nactually get it merged.</p>\n<p>There is huge excitement about newfound delivery speed, but in private\nconversations, I keep hearing the same second sentence: people are also confused\nabout how to keep up with the pace they themselves created.</p>\n<h2>We Have Been Here Before</h2>\n<p>Humanity has been here before.  Many times over.  We already talk about the\nLuddites a lot in the context of AI, but it&#8217;s interesting to see what led up to\nit.  Mark Cartwright wrote a great <a href=\"https://www.worldhistory.org/article/2183/the-textile-industry-in-the-british-industrial-rev/\">article about the textile\nindustry</a>\nin Britain during the industrial revolution.  At its core was a simple idea:\nwhenever a bottleneck was removed, innovation happened downstream from that.\nWeaving sped up? Yarn became the constraint. Faster spinning? Fibre needed to be\nimproved to support the new speeds until finally the demand for cotton went up\nand that had to be automated too.  We saw the same thing in shipping that led\nto modern automated ports and containerization.</p>\n<p>As software engineers we have been here too.  Assembly did not scale to larger\nengineering teams, and we had to invent higher level languages.  A lot of what\nprogramming languages and software development frameworks did was allow us\nto write code faster and to scale to larger code bases.  What it did not do up\nto this point was take away the core skill of engineering.</p>\n<p>While it&#8217;s definitely easier to write C than assembly, many of the core problems\nare the same.  Memory latency still matters, physics are still our ultimate\nbottleneck, algorithmic complexity still makes or breaks software at scale.</p>\n<h2>Giving Up?</h2>\n<p>When one part of the pipeline becomes dramatically faster, you need to throttle\ninput.  <a href=\"https://pi.dev/\">Pi</a> is a great example of this.  PRs are auto closed\nunless people are trusted.  It takes <a href=\"https://x.com/badlogicgames/status/2021164603506368693\">OSS\nvacations</a>.  That&#8217;s one\noption: you just throttle the inflow.  You push against your newfound powers\nuntil you can handle them.</p>\n<h2>Or Giving In</h2>\n<p>But what if the speed continues to increase?  What downstream of writing code do\nwe have to speed up?  Sure, the pull request review clearly turns into the\nbottleneck.  But it cannot really be automated.  If the machine writes the code,\nthe machine better review the code at the same time.  So what ultimately comes\nup for human review would already have passed the most critical possible review\nof the most capable machine.  What else is in the way?  If we continue with the\nfundamental belief that machines cannot be accountable, then humans need to be\nable to understand the output of the machine.  And the machine will ship\nrelentlessly.  Support tickets of customers will go straight to machines to\nimplement improvements and fixes, for other machines to review, for humans to\nrubber stamp in the morning.</p>\n<p>A lot of this sounds both unappealing and reminiscent of the textile industry.\nThe individual weaver no longer carried responsibility for a bad piece of cloth.\nIf it was bad, it became the responsibility of the factory as a whole and it was\njust replaced outright.  As we&#8217;re entering the phase of single-use plastic\nsoftware, we might be moving the whole layer of responsibility elsewhere.</p>\n<h2>I Am The Bottleneck</h2>\n<p>But to me it still feels different.  Maybe that&#8217;s because my lowly brain can&#8217;t\ncomprehend the change we are going through, and future generations will just\nlaugh about our challenges.  It feels different to me, because what I see taking\nplace in some Open Source projects, in some companies and teams feels deeply\nwrong and unsustainable.  Even Steve Yegge himself now <a href=\"https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163\">casts\ndoubts</a> about the\nsustainability of the ever-increasing pace of code creation.</p>\n<p>So what if we need to give in?  What if we need to pave the way for this new\ntype of engineering to become the standard?  What affordances will we have to\ncreate to make it work?  I for one do not know.  I&#8217;m looking at this with\nfascination and bewilderment and trying to make sense of it.</p>\n<p>Because it is not the final bottleneck.  We will find ways to take\nresponsibility for what we ship, because society will demand it.  Non-sentient\nmachines will never be able to carry responsibility, and it looks like we will\nneed to deal with this problem before machines achieve this status.\nRegardless of how <a href=\"https://en.wikipedia.org/wiki/Moltbook\">bizarre they appear to\nact</a> already.</p>\n<p><a href=\"https://x.com/thorstenball/status/2022310010391302259\">I too am the bottleneck\nnow</a>.  But you know what?\nTwo years ago, I too was the bottleneck.  I was the bottleneck all along.  The\nmachine did not really change that.  And for as long as I carry responsibilities\nand am accountable, this will remain true.  If we manage to push accountability\nupwards, it might change, but so far, how that would happen is not clear.</p>\n",
    "description": "",
    "is_fulltext": true,
    "source": "Armin Ronacher's Thoughts and Writings",
    "pub_date": "2026-02-13T00:00:00+00:00",
    "fetched_at": "2026-02-14T00:37:32.342585",
    "url_hash": "c0f2f354000620c5ef996ce4598a38c2"
  },
  {
    "title": "Premium: The AI Data Center Financial Crisis",
    "link": "https://www.wheresyoured.at/data-center-crisis/",
    "content": "<p>Since the beginning of 2023, big tech has spent over $814 billion in capital expenditures, with a large portion of that going towards meeting the demands of AI companies like OpenAI and Anthropic.&#xA0;</p><p>Big tech has spent big on GPUs, power infrastructure, and data center construction,&#xA0; using a variety of financing methods to do so, including (but not limited to) leasing. And the way they&#x2019;re going about structuring these finance deals is growing increasingly bizarre.&#xA0;</p><p>I&#x2019;m not merely <a href=\"https://www.datacenterdynamics.com/en/news/meta-forms-27-billion-joint-venture-with-blue-owl-to-fund-gigawatt-scale-ai-data-center-campus-in-louisiana/?ref=wheresyoured.at\"><u>talking about Meta&#x2019;s curious arrangement for its facility in Louisiana</u></a>, though that certainly raised some eyebrows. Last year, Morgan Stanley published a report that claimed hyperscalers were increasingly relying on <a href=\"https://www.investing.com/news/stock-market-news/why-hyperscalers-are-increasingly-using-finance-leases-for-data-center-shells-4355028?ref=wheresyoured.at\"><u>finance leases</u></a> to obtain the &#x201C;powered shell&#x201D; of a data center, rather than the more common method of operating leases.&#xA0;</p><p>The key difference here is that finance leases, unlike operating leases, are effectively long-term loans where the borrower is expected to retain ownership of the asset (whether that be a GPU or a building) at the end of the contract. Traditionally, these types of arrangements have been used to finance the bits of a data center that have a comparatively limited useful life &#x2014; like computer hardware, which grows obsolete with time.</p><p>The spending to date is, as I&#x2019;ve written about <a href=\"https://www.wheresyoured.at/why-everybody-is-losing-money-on-ai/\"><u>again</u></a> and <a href=\"https://www.wheresyoured.at/the-case-against-generative-ai/\"><u>again</u></a>, an astronomical amount of spending considering the lack of meaningful revenue from generative AI.&#xA0;</p><p>Even after a year straight of manufacturing consent for Claude Code as the be-all-end-all of software development resulted in putrid results for Anthropic &#x2014; $4.5 billion of revenue and <em>$5.2 billion of losses <strong>before interest, taxes, depreciation and amortization</strong></em> <a href=\"https://www.theinformation.com/articles/anthropic-lowers-profit-margin-projection-revenue-skyrockets?rc=kz8jh3&amp;ref=wheresyoured.at\"><u>according to The Information</u></a> &#x2014; with (<a href=\"https://www.wired.com/story/claude-code-success-anthropic-business-model/?ref=wheresyoured.at\"><u>per WIRED</u></a>) Claude Code only accounting for around $1.1 billion in annualized revenue in December, or around $92 million in monthly revenue.</p><p>This was in a year where Anthropic <a href=\"https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation?ref=wheresyoured.at\"><u>raised</u></a> a <a href=\"https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation?ref=wheresyoured.at\"><u>total</u></a> of $16.5 billion (with $13 billion of that coming in September 2025), and it&#x2019;s <a href=\"https://www.cnbc.com/2026/01/18/sequoia-to-join-gic-coatue-in-anthropic-investment-ft-reports.html?ref=wheresyoured.at\"><u>already working on raising another $25 billion</u></a>. This might be <a href=\"https://the-decoder.com/anthropic-places-21-billion-order-for-google-chips-via-broadcom/?ref=wheresyoured.at\"><u>because it promised to buy $21 billion of Google TPUs from Broadcom</u></a>, or <a href=\"https://www.theinformation.com/articles/anthropic-hikes-2026-revenue-forecast-20-delays-will-go-cash-flow-positive?rc=kz8jh3&amp;ref=wheresyoured.at\"><u>because Anthropic expects AI model training costs to cost over $100 billion in the next 3 years</u></a>. And <a href=\"https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation?ref=wheresyoured.at\"><u>it just raised another $30 billion</u></a> &#x2014; albeit with the caveat that some of said $30 billion came from previously-announced funding agreements with Nvidia and Microsoft, though <em>how much</em> remains a mystery.</p><p>According to Anthropic&#x2019;s new funding announcement, Claude Code&#x2019;s run rate has grown to &#x201C;over $2.5 billion&#x201D; as of February 12 2026 &#x2014; or around $208 million. Based on literally every bit of reporting about Anthropic, costs have likely spiked along with revenue, which hit $14 billion annualized ($1.16 billion in a month) as of that date.&#xA0;</p><p>I have my doubts, but let&#x2019;s put them aside for now.</p><p>Anthropic is also in the midst of one of the most aggressive and dishonest public relations campaigns in history. While its Chief Commercial Officer Paul Smith <a href=\"https://www.cnbc.com/2026/02/11/anthropic-vs-openai-ads-spending-criticism.html?ref=wheresyoured.at\"><u>told CNBC</u></a> that it was &#x201C;focused on growing revenue&#x201D; rather than &#x201C;spending money,&#x201D; it&#x2019;s currently making massive promises &#x2014; <a href=\"https://www.anthropic.com/news/expanding-our-use-of-google-cloud-tpus-and-services?ref=wheresyoured.at\"><u>tens of billions on Google Cloud</u></a>, &#x201C;<a href=\"https://www.anthropic.com/news/anthropic-invests-50-billion-in-american-ai-infrastructure?ref=wheresyoured.at\"><u>$50 billion in American AI infrastructure</u></a>,&#x201D; and <a href=\"https://www.anthropic.com/news/microsoft-nvidia-anthropic-announce-strategic-partnerships?ref=wheresyoured.at\"><u>$30 billion on Azure</u></a>. And despite Smith saying that Anthropic was less interested in &#x201C;flashy headlines,&#x201D; Chief Executive Dario Amodei has said, in the last <em>three weeks</em>, that &#x201C;<a href=\"https://www.theguardian.com/technology/2026/jan/27/wake-up-to-the-risks-of-ai-they-are-almost-here-anthropic-boss-warns?ref=wheresyoured.at\"><u>almost unimaginable power is potentially imminent</u></a>,&#x201D; <a href=\"https://www.entrepreneur.com/business-news/ai-ceo-says-software-engineers-could-be-replaced-in-months/502087?ref=wheresyoured.at\"><u>that AI could replace all software engineers in the next 6-12 months</u></a>, that AI may (it&#x2019;s always fucking <em>may</em>) cause &#x201C;<a href=\"https://www.cnbc.com/2026/01/27/dario-amodei-warns-ai-cause-unusually-painful-disruption-jobs.html?ref=wheresyoured.at\"><u>unusually painful disruption to jobs</u></a>,&#x201D; and <a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology?ref=wheresyoured.at#fnref:10\"><u>wrote a 19,000 word essay</u></a> &#x2014; I guess AI is coming for my job after all! &#x2014; where he repeated his noxious line that &#x201C;we will likely get a century of scientific and economic progress compressed in a decade.&#x201D;</p><h2 id=\"training-costs-should-be-part-of-ai-labs%E2%80%99-gross-margins-and-to-not-include-them-is-deceptive\">Training Costs Should Be Part of AI Labs&#x2019; Gross Margins, And To Not Include Them Is Deceptive</h2><p>Yet arguably the most dishonest part is this word &#x201C;training.&#x201D; When you read &#x201C;training,&#x201D; you&#x2019;re meant to think &#x201C;oh, it&#x2019;s training for something, this is an R&amp;D cost,&#x201D; when &#x201C;training LLMs&#x201D; is as consistent a cost as inference (the creation of the output) or any other kind of maintenance.&#xA0;</p><p>While most people know about pretraining &#x2014; the shoving of large amounts of data into a model (this is a simplification I realize) &#x2014; in reality a lot of the current spate of models use <a href=\"https://towardsdatascience.com/different-ways-of-training-llms-c57885f388ed/?ref=wheresyoured.at\"><strong><u>post-training</u></strong></a>, which covers everything from small tweaks to model behavior to full-blown reinforcement learning where experts reward or punish particular responses to prompts.</p><p>To be clear, all of this is well-known and documented, but the nomenclature of &#x201C;training&#x201D; suggests that it might stop one day, versus the truth: <strong>training costs are increasing dramatically, and &#x201C;training&#x201D; covers anything from training new models to bug fixes on existing ones.</strong> And, more fundamentally, it&#x2019;s <em>an ongoing cost</em> &#x2014; something that&#x2019;s an essential and unavoidable cost of doing business.&#xA0;</p><p>Training is, for an AI lab like OpenAI and Anthropic, as common (and necessary) a cost as those associated with creating outputs (inference), <a href=\"https://www.theinformation.com/articles/anthropic-lowers-profit-margin-projection-revenue-skyrockets?rc=kz8jh3&amp;ref=wheresyoured.at\"><u>yet it&#x2019;s kept entirely out of gross margins</u></a>:</p><blockquote>Anthropic has previously projected gross margins above 70% by 2027, and OpenAI has projected gross margins of at least 70% by 2029, which would put them closer to the gross margins of publicly traded software and cloud firms. But both AI developers also spend a tremendous amount on renting servers to develop new models&#x2014;training costs, which don&#x2019;t factor into gross margins&#x2014;making it more difficult to turn a net profit than it is for traditional software firms.</blockquote><p>This is inherently deceptive. While one would argue that R&amp;D is not considered in gross margins, training<em> isn&#x2019;t</em> gross margins &#x2014; yet gross margins generally include the raw materials necessary to build something, <em>and training is absolutely part of the raw costs of running an AI model.</em> Direct labor and parts are considered part of the calculation of gross margin, and spending on training &#x2014; both the data and the process of training itself &#x2014; are absolutely meaningful, and to leave them out is an act of deception.&#xA0;</p><p>Anthropic&#x2019;s 2025 gross margins were 40% &#x2014; or 38% if you include free users of Claude &#x2014; on inference costs of $2.7 (or $2.79) billion, with <a href=\"https://www.theinformation.com/articles/anthropic-lowers-profit-margin-projection-revenue-skyrockets?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=The%20inference%20costs,models%20last%20year.\"><u>training costs of around $4.1 billion</u></a>. What happens if you add training costs into the equation?&#xA0;</p><p>Let&#x2019;s work it out!</p><ul><li>If Anthropic&#x2019;s gross margin was 38% in 2025, that means its COGS (cost of goods sold) was $2.79 billion.</li><li>If we add training, this brings COGS to $6.89 billion, leaving us with -$2.39 billion after $4.5 billion in revenue.</li><li>This results in a negative 53% gross margin.</li></ul><p>Training is <strong>not an up front cost</strong>, and considering it one only serves to help Anthropic cover for its wretched business model. Anthropic (like OpenAI) <strong>can never stop training, ever, and to pretend otherwise is misleading.</strong> This is not the cost just to &#x201C;train new models&#x201D; but to maintain current ones, build new products around them, and many other things that are <em>direct, impossible-to-avoid components of COGS.</em> They&#x2019;re manufacturing costs, plain and simple.</p><p>Anthropic projects to spend $100 billion on training in the next three years, which suggests it will spend &#x2014; proportional to its current costs &#x2014; around $32 billion on inference in the same period, on top of $21 billion of TPU purchases, on top of $30 billion on Azure (I assume in that period?), on top of &#x201C;tens of billions&#x201D; on Google Cloud. When you actually add these numbers together (assuming &#x201C;tens of billions&#x201D; is $15 billion), that&#x2019;s <em>$200 billion.&#xA0;</em></p><p>Anthropic (<a href=\"https://www.theinformation.com/articles/anthropic-hikes-2026-revenue-forecast-20-delays-will-go-cash-flow-positive?rc=kz8jh3&amp;ref=wheresyoured.at\"><u>per The Information&#x2019;s reporting</u></a>) tells investors it will make $18 billion in revenue in 2026 and $55 billion in 2027 &#x2014; year-over-year increases of 400% and 305% respectively, and is already raising $25 billion after having just closed a $30bn deal. How does Anthropic pay its bills? Why does outlet after outlet print these fantastical numbers without doing the maths of &#x201C;how does Anthropic actually get all this money?&#x201D;</p><p>Because even with their <em>ridiculous</em> revenue projections, this company is still burning cash, and when you start to actually do the maths around <em>anything </em>in the AI industry, things become genuinely worrying.&#xA0;</p><p>You see, every single generative AI company is unprofitable, and appears to be getting <em>less</em> profitable over time. Both <a href=\"http://theinformation.com/articles/anthropic-projects-70-billion-revenue-17-billion-cash-flow-2028?rc=kz8jh3&amp;ref=wheresyoured.at\"><u>The Information</u></a> and <a href=\"https://www.wsj.com/tech/ai/openai-anthropic-profitability-e9f5bcd6?gaa_at=eafs&amp;gaa_n=AWEtsqct1bhG2emXUSPVBjyAjOqtZvlwvJCz16c2pQSaTagstek3UouJVzzaMos1XHU%3D&amp;gaa_ts=698ce0f9&amp;gaa_sig=GK3IQsualnM5_1Yt7Rj4vhcMppOLQQOP-grYebUq7a0-pNQejMUWvfo1g2Zak8SL2f4fL5fEbCySS5QOhCC16Q%3D%3D&amp;ref=wheresyoured.at\"><u>Wall Street Journal</u></a> reported the same bizarre statement in November &#x2014; that Anthropic would &#x201C;turn a profit more quickly than OpenAI,&#x201D; with The Information saying Anthropic would be cash flow positive in 2027 and the Journal putting the date at 2028, only for <a href=\"https://www.theinformation.com/articles/anthropic-hikes-2026-revenue-forecast-20-delays-will-go-cash-flow-positive?rc=kz8jh3&amp;ref=wheresyoured.at\"><u>The Information to report</u></a> in January that 2028 was the more-realistic figure.&#xA0;</p><p>If you&#x2019;re wondering how, the answer is &#x201C;Anthropic will magically become cash flow positive in 2028&#x201D;: </p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.wheresyoured.at/content/images/2026/02/data-src-image-79fb9cbd-a579-4df3-920c-5dff7b5378fc.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1035\" height=\"808\" srcset=\"https://www.wheresyoured.at/content/images/size/w600/2026/02/data-src-image-79fb9cbd-a579-4df3-920c-5dff7b5378fc.png 600w, https://www.wheresyoured.at/content/images/size/w1000/2026/02/data-src-image-79fb9cbd-a579-4df3-920c-5dff7b5378fc.png 1000w, https://www.wheresyoured.at/content/images/2026/02/data-src-image-79fb9cbd-a579-4df3-920c-5dff7b5378fc.png 1035w\" sizes=\"(min-width: 720px) 720px\"></figure><p>This is also the exact same logic as OpenAI, which will, per <a href=\"https://www.theinformation.com/articles/openai-says-business-will-burn-115-billion-2029?rc=kz8jh3&amp;ref=wheresyoured.at\"><u>The Information in September</u></a>, also, somehow, magically turn cashflow positive in 2030:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.wheresyoured.at/content/images/2026/02/data-src-image-8a54e3a4-6259-4807-864e-483ff68cc154.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"805\" height=\"667\" srcset=\"https://www.wheresyoured.at/content/images/size/w600/2026/02/data-src-image-8a54e3a4-6259-4807-864e-483ff68cc154.png 600w, https://www.wheresyoured.at/content/images/2026/02/data-src-image-8a54e3a4-6259-4807-864e-483ff68cc154.png 805w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Oracle, which has a 5-year-long, $300 billion compute deal with OpenAI that it lacks the capacity to serve and that OpenAI lacks the cash to pay for, also <a href=\"http://finance.yahoo.com/news/oracle-raise-50-billion-2026-235033434.html?ref=wheresyoured.at\"><u>appears to have the same magical plan to become cash flow positive in 2029</u></a>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.wheresyoured.at/content/images/2026/02/data-src-image-83b0cb42-d88b-4efa-8bb6-ac63cdaeab39.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"960\" height=\"592\" srcset=\"https://www.wheresyoured.at/content/images/size/w600/2026/02/data-src-image-83b0cb42-d88b-4efa-8bb6-ac63cdaeab39.png 600w, https://www.wheresyoured.at/content/images/2026/02/data-src-image-83b0cb42-d88b-4efa-8bb6-ac63cdaeab39.png 960w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Somehow, Oracle&#x2019;s case is the most legit, in that theoretically at that time it would be done, I assume, paying the $38 billion it&#x2019;s raising for Stargate Shackelford and Wisconsin, but said assumption also hinges on the idea that <strong><em>OpenAI finds $300 billion somehow</em></strong>.</p><p>it also relies upon Oracle raising more debt than it currently has &#x2014; which, even before the AI hype cycle swept over the company, was a lot.&#xA0;</p><p><a href=\"https://www.wheresyoured.at/haters-guide-oracle/#oracle-and-its-partners-have-not-raised-enough-capital-to-pay-for-its-data-centers-%E2%80%94-costs-will-be-at-least-189-billion-in-total-for-45gw-of-data-centers-at-around-42-million-a-megawatt\"><u>As I discussed a few weeks ago in the Hater&#x2019;s Guide To Oracle</u></a>, a megawatt of data center IT load generally costs&#xA0; (per Jerome Darling of TD Cowen) around $12-14m&#xA0; in construction (likely more due to skilled labor shortages, supply constraints and rising equipment prices) and $30m a megawatt in GPUs and associated hardware. In plain terms, Oracle (and its associated partners) need around $189 billion to build the 4.5GW of Stargate capacity to make the revenue from the OpenAI deal, meaning that it needs around another $100 billion once it raises <a href=\"https://archive.is/x5jmZ?ref=wheresyoured.at\"><u>$50 billion in combined debt, bonds, and <em>printing new shares</em></u></a> by the end of 2026.</p><p>I will admit I feel a little crazy writing this all out, because it&#x2019;s somehow a fringe belief to do the very basic maths and say &#x201C;hey, Oracle doesn&#x2019;t have the capacity and OpenAI doesn&#x2019;t have the money.&#x201D; In fact, nobody seems to want to really <em>talk</em> about the cost of AI, because it&#x2019;s much easier to say &#x201C;I&#x2019;m not a numbers person&#x201D; or &#x201C;they&#x2019;ll work it out.&#x201D;</p><p>This is why in today&#x2019;s newsletter I am going to lay out the stark reality of the AI bubble, and debut a model I&#x2019;ve created to measure the actual, real costs of an AI data center.</p><p>While my methodology is complex, my conclusions are simple: running AI data centers is, even when you remove the debt required to stand up these data centers, a mediocre business that is vulnerable to basically any change in circumstances.&#xA0;</p><p>Based on hours of discussions with data center professionals, analysts and economists, I have calculated that in most cases, the average AI data center has gross margins of somewhere between 30% and 40% &#x2014; margins that decay rapidly for every day, week, or month that you take putting a data center into operation.</p><p><a href=\"https://www.theinformation.com/articles/oracle-assures-investors-ai-cloud-margins-struggles-profit-older-nvidia-chips?ref=wheresyoured.at&amp;rc=kz8jh3\"><u>This is why Oracle has negative 100% margins on NVIDIA&#x2019;s GB200 chips</u></a> &#x2014; because the burdensome up-front cost of building AI data centers (as GPUs, servers, and other associated) leaves you billions of dollars in the hole before you even start serving compute, after which you&#x2019;re left to contend with taxes, depreciation, financing, and the cost of actually powering the hardware.&#xA0;</p><p>Yet things sour further when you face the actual financial realities of these deals &#x2014; and the debt associated with them.&#xA0;</p><p>Based on my current model of the 1GW Stargate Abilene data center, Oracle likely plans to make around $11 billion in revenue a year from the 1.2GW (or around 880MW of critical IT). While that sounds good, when you add things like depreciation, electricity, colocation costs of $1 billion a year from Crusoe, opex, and the myriad of other costs, its margins sit at a stinkerific 27.2% &#x2014; and that&#x2019;s <strong>assuming OpenAI actually pays, on time, in a reliable way.</strong></p><p>Things only get worse when you factor in the cost of debt. While Oracle has funded Abilene using a mixture of bonds and existing cashflow, it very clearly has yet to receive the majority of the $25 billion+ in GPUs and associated hardware (with only 96,000 GPUs &#x201C;<a href=\"https://finance.yahoo.com/news/oracle-q2-earnings-beat-estimates-165200782.html?ref=wheresyoured.at\"><u>delivered</u></a>&#x201D;), meaning that it likely bought them out of <a href=\"https://www.datacenterdynamics.com/en/news/oracle-takes-on-18bn-in-debt-ahead-of-ai-data-center-build-out/?ref=wheresyoured.at\"><u>its $18 billion bond sale from last September</u></a>.&#xA0;</p><p>If we assume that maths, this means that Oracle is paying a little less than $963 million a year (<a href=\"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001341439/75771445-e1ef-4297-b255-2b5c92a4f3d2.pdf?ref=wheresyoured.at\"><u>per the terms of the bond sale</u></a>) whether or not a single GPU is even turned on, leaving us with a net margin of 22.19%... <em>and this is assuming OpenAI pays every single bill, every single time, and there are absolutely no delays.</em></p><p>These delays are also very, very expensive. Based on my model, if we assume that 100MW of critical IT load is operational (roughly two buildings and 100,000 GB200s) but has yet to start generating revenue, Oracle is burning, with depreciation (which starts once the chips are installed), around $4.69 million a day in<em> cash</em>. I have also confirmed with sources in Abilene that there is no chance that Stargate Abilene is fully operational in 2026.</p><p>In simpler terms:</p><ul><li>AI startups are all unprofitable, and do not appear to have a path to sustainability.&#xA0;</li><li>AI data centers are being built in anticipation of demand that doesn&#x2019;t exist, and will only exist if AI startups &#x2014; which are all unprofitable &#x2014; can afford to pay them.</li><li>Oracle, which has committed to building 4.5GW of data centers, is burning cash every day that OpenAI takes to set up its GPUs, and when it starts making money, it does so from a starting position of <em>billions and billions of dollars in debt.</em></li><li>Margins are low throughout the entire stack of AI data center operators &#x2014; from landlords like Applied Digital to compute providers like CoreWeave &#x2014; thanks to the billions in debt necessary to fund both construction and IT hardware to make them run, putting both parties in a hole that can only be filled with revenues that come from either hyperscalers or AI startups.&#xA0;</li><li>In a very real sense, the AI compute industry is dependent on AI &#x201C;working out,&#x201D; because if it doesn&#x2019;t, every single one of these data centers will become a burning hole in the ground.</li></ul><p>I will admit I&#x2019;m quite disappointed that the media at large has mostly ignored this story. Limp, cautious &#x201C;are we in an AI bubble?&#x201D; conversations are insufficient to deal with the potential for collapse we&#x2019;re facing.&#xA0;</p><p>Today, I&#x2019;m going to dig into the reality of the costs of AI, and explain in gruesome detail exactly how easily these data centers can rapidly approach insolvency in the event that their tenants fail to pay.&#xA0;</p><p>The <a href=\"https://www.wheresyoured.at/the-enshittifinancial-crisis/#the-%E2%80%9Cchain-of-pain%E2%80%9D-that-bursts-the-ai-bubble\"><u>chain of pain</u></a> is real:</p><blockquote>These GPUs are purchased, for the most part, using debt provided by banks or financial institutions. While hyperscalers can and do fund GPUs using cashflow, even they have started to turn to debt.<br><br>At that point, the company that bought the GPUs sinks hundreds of millions of dollars to build a data center, and once it turns on, provides compute to a model provider, which then begins losing money selling access to those GPUs. For example, both OpenAI and Anthropic lose billions of dollars, and both rely on venture capital to fund their ability to continue paying for accessing those GPUs.<br><br>At that point, OpenAI and Anthropic offer either subscriptions &#x2014; which cost far more to offer than the revenue they provide &#x2014; or API access to their models on a per-million-token basis. AI startups pay to access these models to run their services, which end up costing more than the revenue they make, which means they have to raise venture capital to continue paying to access those models.<br><br>Outside of hyperscalers paying NVIDIA for GPUs out of cashflow, none of the AI industry is fueled by revenue. Every single part of the industry is fueled by some kind of subsidy.<br><br>As a result, the AI bubble is really a stress test of the global venture capital, private equity, private credit, institutional and banking system, and its willingness to fund all of this forever, because there isn&apos;t a single generative AI company that&apos;s got a path to profitability.</blockquote><p>Today I&#x2019;m going to explain how easily it breaks.</p>",
    "description": "Since the beginning of 2023, big tech has spent over $814 billion in capital expenditures, with a large portion of that going towards meeting the demands of AI companies like OpenAI and Anthropic.&#xA0;Big tech has spent big on GPUs, power infrastructure, and data center construction,&#xA0; using a",
    "is_fulltext": true,
    "source": "Ed Zitron's Where's Your Ed At",
    "pub_date": "Fri, 13 Feb 2026 19:08:34 GMT",
    "fetched_at": "2026-02-14T00:37:58.974468",
    "url_hash": "a525bbdab7b1901c0417757ff0839107"
  },
  {
    "title": "This Week on The Analog Antiquarian",
    "link": "https://www.filfre.net/2026/02/this-week-on-the-analog-antiquarian/",
    "content": "<blockquote class=\"wp-embedded-content\" data-secret=\"mQRZ7D6JiE\"><p><a href=\"https://analog-antiquarian.net/2026/02/13/chapter-13-the-shades-of-the-earth/\">Chapter 13: The Shades of the Earth</a></p></blockquote>\n<p><iframe class=\"wp-embedded-content\" sandbox=\"allow-scripts\" security=\"restricted\"  title=\"&#8220;Chapter 13: The Shades of the Earth&#8221; &#8212; The Analog Antiquarian\" src=\"https://analog-antiquarian.net/2026/02/13/chapter-13-the-shades-of-the-earth/embed/#?secret=MAtRGY2WTH#?secret=mQRZ7D6JiE\" data-secret=\"mQRZ7D6JiE\" width=\"600\" height=\"338\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\"></iframe></p>\n",
    "description": "Chapter 13: The Shades of the Earth",
    "is_fulltext": true,
    "source": "The Digital Antiquarian",
    "pub_date": "Fri, 13 Feb 2026 17:17:50 +0000",
    "fetched_at": "2026-02-14T00:38:03.748668",
    "url_hash": "b8ae52adcc6645aea638ee0f19c801a5"
  },
  {
    "title": "The Small Web is Tricky to Find",
    "link": "https://matduggan.com/the-small-web-is-tricky-to-find/",
    "content": "<p>One of the most common requests I&apos;ve gotten from users of my little Firefox extension(<a href=\"https://timewasterpro.xyz/admin.html\">https://timewasterpro.xyz</a>) has been more options around the categories of websites that you get returned. This required me to go through and parse the website information to attempt to put them into different categories. I tried a bunch of different approaches but ended up basically looking at the websites themselves seeing if there was anything that looked like a tag or a hint on each site. </p><p>This is the end conclusion of my effort at putting stuff into categories. </p><figure class=\"kg-card kg-image-card\"><img src=\"https://matduggan.com/content/images/2026/01/SCR-20260127-jala.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"562\" srcset=\"https://matduggan.com/content/images/size/w600/2026/01/SCR-20260127-jala.png 600w, https://matduggan.com/content/images/size/w1000/2026/01/SCR-20260127-jala.png 1000w, https://matduggan.com/content/images/size/w1600/2026/01/SCR-20260127-jala.png 1600w, https://matduggan.com/content/images/2026/01/SCR-20260127-jala.png 2350w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Unknown just means I wasn&apos;t able to get any sort of data about it. This is the result of me combining Ghost, Wordpress and Kagi Small Web data sources. </p><p>Interestingly one of my most common requests is &quot;I would like less technical content&quot; which <em>as it turns out</em> is tricky to provide because it&apos;s pretty hard to find. They sorta exist but for less technical users they don&apos;t seem to have bought into the value of the small web own your own web domain (or if they have, I haven&apos;t been able to figure out a reliable way to find them). </p><p>This is an interesting problem, especially because a lot of the tools I would have previously used to solve this problem are....basically broken. It&apos;s difficult for me to really use Google web search to find anything at this point even remotely like &quot;give me all the small websites&quot; because everything is weighted to steer me away from that towards Reddit. So anything that might be a little niche is tricky to figure out. </p><h3 id=\"interesting-findings\">Interesting findings</h3><p>So there&apos;s no point in building a web extension with a weighting algorithm to return less technical content if I cannot find a big enough pool of non-technical content to surface. It isn&apos;t that these sites <em>don&apos;t exist</em> its just that we never really figured out a way to reliably surface &quot;what is a small website&quot;. </p><p>So from a technical perspective I have a bunch of problems. </p><ul><li>First I need to reliably sort websites into a genre, which can be a challenge when we&apos;re talking about small websites because people typically write about whatever moves them that day. Most of the content on a site might be technical, but some of it might not be. Big sites tend to be more precise with their SEO settings but small sites that don&apos;t care don&apos;t do that, so I have fewer reliable signals to work with. </li><li>Then I need to come up with a <em>lot</em> of different feeding systems for independent websites. The Kagi Small Web was a good starting point, but Wordpress and Ghost websites have a much higher ratio of non-technical content. I need those sites, but it&apos;s hard to find a big batch of them reliably. </li><li>Once I have the type of website as a general genre and I have a series of locations, then I can start to reliably distribute the <em>types</em> of content you get. </li></ul><p>I think I can solve....some of these, but the more I work on the problem the more I&apos;m realizing that the entire concept of &quot;the small web&quot; had a series of pretty serious problems. </p><ul><li>Google was the only place on Earth sending any traffic there </li><li>Because Google was the only one who knew about it, there never needed to be another distribution system</li><li>Now that Google is broken, it&apos;s almost impossible to recreate that magic of becoming the top of list for a specific subgenre without a ton more information than I can get from public records. </li></ul><p></p><p></p>",
    "description": "One of the most common requests I&apos;ve gotten from users of my little Firefox extension(https://timewasterpro.xyz) has been more options around the categories of websites that you get returned. This required me to go through and parse the website information to attempt to put them into",
    "is_fulltext": true,
    "source": "matduggan.com",
    "pub_date": "Fri, 13 Feb 2026 12:50:36 GMT",
    "fetched_at": "2026-02-14T00:38:16.313019",
    "url_hash": "0511e88b12a22d8a3a37eaa3ff4f4807"
  },
  {
    "title": "Attack of the SaaS clones",
    "link": "https://martinalderson.com/posts/attack-of-the-clones/?utm_source=rss",
    "content": "I cloned Linear's UI and core functionality using Claude Code in about 20 prompts. Here's what that means for SaaS companies.",
    "description": "I cloned Linear's UI and core functionality using Claude Code in about 20 prompts. Here's what that means for SaaS companies.",
    "is_fulltext": false,
    "source": "Martin Alderson",
    "pub_date": "Fri, 13 Feb 2026 00:00:00 GMT",
    "fetched_at": "2026-02-14T00:38:21.233192",
    "url_hash": "513fe5883c58b92f909b1e7908ab688d"
  }
]