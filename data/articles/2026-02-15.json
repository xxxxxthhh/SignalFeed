[
  {
    "title": "How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt",
    "link": "https://simonwillison.net/2026/Feb/15/cognitive-debt/#atom-everything",
    "content": "\n    \n<p><strong><a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</a></strong></p>\nThis piece by Margaret-Anne Storey is the best explanation of the term <strong>cognitive debt</strong> I've seen so far.</p>\n<blockquote>\n<p><em>Cognitive debt</em>, a term gaining <a href=\"https://www.media.mit.edu/publications/your-brain-on-chatgpt/\">traction</a> recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to “go fast” or to make changes. Even if AI agents produce code that could be easy to understand, the humans involved may have simply lost the plot and may not understand what the program is supposed to do, how their intentions were implemented, or how to possibly change it.</p>\n</blockquote>\n<p>Margaret-Anne expands on this further with an anecdote about a student team she coached:</p>\n<blockquote>\n<p>But by weeks 7 or 8, one team hit a wall. They could no longer make even simple changes without breaking something unexpected. When I met with them, the team initially blamed technical debt: messy code, poor architecture, hurried implementations. But as we dug deeper, the real problem emerged: no one on the team could explain why certain design decisions had been made or how different parts of the system were supposed to work together. The code might have been messy, but the bigger issue was that the theory of the system, their shared understanding, had fragmented or disappeared entirely. They had accumulated cognitive debt faster than technical debt, and it paralyzed them.</p>\n</blockquote>\n<p>I've experienced this myself on some of my more ambitious vibe-code-adjacent projects. I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.</p>\n<p>I no longer have a firm mental model of what they can do and how they work, which means each additional feature becomes harder to reason about, eventually leading me to lose the ability to make confident decisions about where to go next.\n\n    <p><small></small>Via <a href=\"https://martinfowler.com/fragments/2026-02-13.html\">Martin Fowler</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a></p>\n\n\n\n",
    "description": "How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt This piece by Margaret-Anne Storey is the best explanation of the term cognitive debt I've seen so far. Cognitive debt, a term gaining traction recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to “go fast” or to make changes. Even if AI agents produce code that could be easy to understand, the h",
    "is_fulltext": true,
    "source": "Simon Willison's Weblog",
    "pub_date": "2026-02-15T05:20:11+00:00",
    "fetched_at": "2026-02-15T12:15:36.342053",
    "url_hash": "854e923af68218d2882973f605856f5c"
  },
  {
    "title": "The empire always falls",
    "link": "https://www.joanwestenberg.com/the-empire-always-falls/",
    "content": "<img src=\"https://www.joanwestenberg.com/content/images/2026/02/ChatGPT-Image-Feb-15--2026--02_59_17-PM.png\" alt=\"The empire always falls\"><p>A citizen of Rome in 117 AD, under Emperor Trajan, would&apos;ve found it difficult to imagine the empire <em>not</em> existing. The roads, the aqueducts, the legal system, the trade networks stretching from Britain to Mesopotamia: all of it seemed to be a near-fact of nature, like gravity // the Mediterranean itself.</p><p>Edward Gibbon gave us six volumes explaining how that feeling turned out to be wrong, and even he couldn&apos;t fully untangle all the causes.</p><p>But the overarching theme might be this: the permanence was a mirage, and belief in the permanence a catastrophic delusion.</p><p>Popular AI commentary treats the current crop of foundation model companies the way those Roman citizens treated the legions: as inevitable, as the only possible structure the world could take. The posting classes assume that because OpenAI and Google and Anthropic and Meta have built impressive things, those impressive things will continue to compound in a linear fashion until every job is automated and every economy is restructured, leaving a permanent underclass of unemployable humans in a world that no longer needs them. This is treated as so obvious that questioning it marks you as either naive or sentimental.</p><p>But companies destroy <em>themselves</em> and empires rot <em>from within</em>, and the people living inside these systems almost never see the collapse coming, because the system itself is the lens through which they view the world.</p><h2 id=\"permanence-is-the-most-dangerous-feeling-in-history\">Permanence is the most dangerous feeling in history</h2><p>Thomas Kuhn argued in <em>The Structure of Scientific Revolutions</em> that the scientists working within a dominant framework don&apos;t use it as a tool so much as inhabit it. Normal science is puzzle-solving within a framework that nobody questions, until the anomalies pile up so high that someone proposes a new framework entirely, and the old guard spends twenty years insisting nothing&apos;s changed. The Ptolemaic model of the solar system survived for over a thousand years, largely because everyone concerned was brilliant enough to keep adding epicycles to make the data fit, making every new complication feel like...well, progress.</p><p>In the &quot;AI inevitability thesis&quot; every limitation gets explained away as a temporary obstacle on the path to AGI. Reasoning will improve, costs will fall etc <em>and to be fair, they might.</em> But the confidence with which these predictions are delivered should remind you of the confidence with which the British Empire&apos;s administrators, circa 1900, reviewed the permanent nature of their civilizational project. They had the world&apos;s largest navy and the world&apos;s most extensive telegraph network, plus control of roughly a quarter of the earth&apos;s land surface. Within fifty years, nearly all of it was gone. And that dissolution happened because the underlying conditions that made the empire possible changed in ways that no amount of naval power could address.</p><h2 id=\"sure-things-fill-graveyards\">Sure things fill graveyards</h2><p>In 2007, Research In Motion controlled roughly half the US smartphone market and had a market capitalization north of $60 billion. RIM&apos;s co-CEO Mike Lazaridis reportedly studied the iPhone at launch and concluded it was impossible for the device to work as advertised on a cellular network. He was, in a narrow technical sense, almost right. The first iPhone had appalling battery life and a network that could barely support it. But he was catastrophically wrong about everthing else.</p><p>Nokia held about 40% of the global mobile phone market at its peak. Internal documents later revealed that middle management had become so terrified of senior leadership&apos;s reactions to bad news that critical information about competitive threats stopped flowing upward. The company suffocated on its own hierarchy. Xerox PARC invented the graphical user interface, the mouse, the laser printer and Ethernet, and then Xerox managed to commercialize approximately one of those things while Steve Jobs walked out of a demo and built Apple&apos;s future on what he&apos;d seen.</p><p>The Soviet Union fell because the gap between its internal model of reality and actual reality became unsustainable. The Ottoman Empire spent its last century implementing increasingly frantic reforms, each one an attempt to bolt modernity onto a structure that couldn&apos;t support it.</p><p>I&apos;m reminded of Shelley&apos;s <em>Ozymandias</em>: the lone and level sands stretch far away, and they stretch away from every kingdom that ever declared itself eternal.</p><p>And yes, that could still include Google, Anthropic and anyone // everyone else.</p><h2 id=\"straight-lines-never-stay-straight\">Straight lines never stay straight</h2><p>The current AI narrative draws a line from model 1 to model 2 to model 3 to whatever comes next, projects it forward, and concludes that human labor // existence is finished. But straight-line projections are the most reliably wrong predictions in the history of forecasting, tech or otherwise.</p><p>What actually happens, in empires // companies alike, is that progress hits unexpected walls and leaders make strategic blunders while some force that nobody took seriously finds an approach that makes the incumbent architecture look like Ptolemy&apos;s epicycles: elaborate and technically sophisticated but pointed in entirely the wrong direction. The blunder creates the opening creates the backlash creates the opportunity for the insurgent. Why should the AI industry be exempt from this? What is it about foundation models that repeals the laws of entropy that have governed every dominant system in recorded history?</p><p>Clayton Christensen documented the corporate version of this in <em>The Innovator&apos;s Dilemma</em>. Across industries and decades, incumbents fail to respond to disruptive threats because responding would require cannibalizing their existing business (or identity, or vision, or mission) and admitting that the strategy everyone got promoted for executing was wrong. AKA: Dominant systems produce the very conditions that destroy them, because the success of the system makes it impossible for the people inside it to perceive its weaknesses.</p><h2 id=\"what-collapse-looks-like-before-it-arrives\">What collapse looks like before it arrives</h2><p>We haven&apos;t seen the first great AI collapse. We haven&apos;t seen a foundation model company make the BlackBerry mistake or the Nokia mistake, or the Roman mistake, or the Ottoman mistake or reach their Bunker-in-Berlin mistake. But we will, and we&apos;ll see it multiple times, because these mistakes = features of power concentration. The hubris that makes a company or an empire dominant in one era is frequently the quality that blinds it to the next one. If you could ask Lazaridis in 2006, or a British colonial administrator in 1900, whether their model of the world was permanent, each would&apos;ve given you a very convincing explanation for why it in fact was.</p><p>When someone tells you that AGI is inevitable and the permanent economic displacement of most humans is a foregone conclusion, what they&apos;re really telling you is that they believe the current leaders of the AI industry will execute flawlessly, indefinitely, against challenges they can&apos;t yet foresee, in an environment that&apos;s changing faster than perhaps any technological enviroment in history. They believe that this particular set of institutions, at this particular moment, has broken the pattern that has held for every empire and every corporation in human history.</p><p>But roads crumble and legions go home, the epicycles collapse into a simpler truth, and something else, something nobody predicted, grows in the spaces left behind. It always has and it always will.</p>",
    "description": "A citizen of Rome in 117 AD, under Emperor Trajan, would&apos;ve found it difficult to imagine the empire not existing. The roads, the aqueducts, the legal system, the trade networks stretching from Britain to Mesopotamia: all of it seemed to be a near-fact of nature, like gravity // the",
    "is_fulltext": true,
    "source": "Westenberg.",
    "pub_date": "Sun, 15 Feb 2026 04:06:02 GMT",
    "fetched_at": "2026-02-15T12:16:06.145322",
    "url_hash": "d46611a9742b33d70f6b78138b315881"
  },
  {
    "title": "Separating Download from Install in Docker Builds",
    "link": "https://nesbitt.io/2026/02/15/separating-download-from-install-in-docker-builds.html",
    "content": "<p>Docker layer caching works best when each layer’s inputs are narrow, and a layer that only depends on a lockfile can survive most builds untouched because you’re usually changing application code, not dependencies. Most package managers combine downloading and installing into a single command though, so the layer that fetches from the registry also depends on source files, and any source change invalidates the layer and forces every dependency to re-download even when the lockfile is identical to last time.</p>\n\n<p>That costs more than build time. crates.io, rubygems.org, and pypi.org all run on bandwidth donated by Fastly, and every redundant download in a Docker build is a cost someone else is volunteering to cover. npm is backed by Microsoft and Go’s module proxy by Google, so they can absorb it, but for the community-funded registries it adds up. It feels instant from the developer’s side, a few seconds of progress bars, so nobody thinks about the hundreds of HTTP requests firing against those services on every build where the lockfile has changed by even one line, or when you’re debugging a failed install and rebuilding the same image over and over.</p>\n\n<p>If package managers exposed a <code class=\"language-plaintext highlighter-rouge\">download</code> that populates the local cache from the lockfile and an <code class=\"language-plaintext highlighter-rouge\">install</code> that works offline from that cache, Docker layer caching would handle the rest:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">COPY</span><span class=\"s\"> lockfile .</span>\n<span class=\"k\">RUN </span>pkg download\n<span class=\"k\">COPY</span><span class=\"s\"> . .</span>\n<span class=\"k\">RUN </span>pkg <span class=\"nb\">install</span> <span class=\"nt\">--offline</span>\n</code></pre></div></div>\n\n<h3 id=\"go-mod-download\">go mod download</h3>\n\n<p>Go modules shipped with Go 1.11 in August 2018, and the community figured out the Docker pattern <a href=\"https://blog.container-solutions.com/faster-builds-in-docker-with-go-1-11\">within weeks</a>. It’s now the canonical Go Dockerfile pattern, recommended by <a href=\"https://docs.docker.com/guides/golang/build-images/\">Docker’s own documentation</a>:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">COPY</span><span class=\"s\"> go.mod go.sum ./</span>\n<span class=\"k\">RUN </span>go mod download\n<span class=\"k\">COPY</span><span class=\"s\"> . .</span>\n<span class=\"k\">RUN </span><span class=\"nv\">CGO_ENABLED</span><span class=\"o\">=</span>0 go build <span class=\"nt\">-o</span> /app .\n</code></pre></div></div>\n\n<p><code class=\"language-plaintext highlighter-rouge\">go mod download</code> reads <code class=\"language-plaintext highlighter-rouge\">go.mod</code> and <code class=\"language-plaintext highlighter-rouge\">go.sum</code> and fetches everything without doing any resolution or building, and the layer caches when those two files haven’t changed.</p>\n\n<p>Before Go 1.11, <code class=\"language-plaintext highlighter-rouge\">GOPATH</code>-based dependency management didn’t have a clean two-file manifest that could be separated from source code for layer caching, and the design of <code class=\"language-plaintext highlighter-rouge\">go.mod</code> and <code class=\"language-plaintext highlighter-rouge\">go.sum</code> as small standalone files made this Docker pattern fall out naturally once modules landed.</p>\n\n<p><code class=\"language-plaintext highlighter-rouge\">go build</code> can still contact the checksum database (<code class=\"language-plaintext highlighter-rouge\">sum.golang.org</code>) after <code class=\"language-plaintext highlighter-rouge\">go mod download</code> to verify modules not yet in <code class=\"language-plaintext highlighter-rouge\">go.sum</code>. Setting <code class=\"language-plaintext highlighter-rouge\">GOFLAGS=-mod=readonly</code> after the download step prevents any network access during the build.</p>\n\n<h3 id=\"pnpm-fetch\">pnpm fetch</h3>\n\n<p>pnpm is the only JavaScript package manager with a download-only command, and <a href=\"https://pnpm.io/cli/fetch\"><code class=\"language-plaintext highlighter-rouge\">pnpm fetch</code></a> was designed specifically for Docker. It reads <code class=\"language-plaintext highlighter-rouge\">pnpm-lock.yaml</code> and downloads all packages into pnpm’s content-addressable store without reading <code class=\"language-plaintext highlighter-rouge\">package.json</code> at all:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">COPY</span><span class=\"s\"> pnpm-lock.yaml pnpm-workspace.yaml ./</span>\n<span class=\"k\">RUN </span>pnpm fetch <span class=\"nt\">--prod</span>\n<span class=\"k\">COPY</span><span class=\"s\"> . .</span>\n<span class=\"k\">RUN </span>pnpm <span class=\"nb\">install</span> <span class=\"nt\">-r</span> <span class=\"nt\">--offline</span> <span class=\"nt\">--prod</span>\n</code></pre></div></div>\n\n<p>The download layer only depends on the lockfile, and the install step uses <code class=\"language-plaintext highlighter-rouge\">--offline</code> so it never touches the network. In monorepos this is particularly useful because you don’t need to copy every workspace’s <code class=\"language-plaintext highlighter-rouge\">package.json</code> before the download step, and pnpm’s authors thinking about container builds when they designed the CLI is the same kind of design awareness that made <code class=\"language-plaintext highlighter-rouge\">go mod download</code> standard in Go.</p>\n\n<h3 id=\"cargo-fetch\">cargo fetch</h3>\n\n<p><a href=\"https://doc.rust-lang.org/cargo/commands/cargo-fetch.html\"><code class=\"language-plaintext highlighter-rouge\">cargo fetch</code></a> reads <code class=\"language-plaintext highlighter-rouge\">Cargo.lock</code> and downloads all crate source into the registry cache. After fetching, <code class=\"language-plaintext highlighter-rouge\">--frozen</code> (which combines <code class=\"language-plaintext highlighter-rouge\">--locked</code> and <code class=\"language-plaintext highlighter-rouge\">--offline</code>) prevents any network access during the build:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">COPY</span><span class=\"s\"> Cargo.toml Cargo.lock ./</span>\n<span class=\"k\">RUN </span><span class=\"nb\">mkdir </span>src <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">touch </span>src/main.rs\n<span class=\"k\">RUN </span>cargo fetch <span class=\"nt\">--locked</span>\n<span class=\"k\">COPY</span><span class=\"s\"> . .</span>\n<span class=\"k\">RUN </span>cargo build <span class=\"nt\">--release</span> <span class=\"nt\">--frozen</span>\n</code></pre></div></div>\n\n<p>The dummy <code class=\"language-plaintext highlighter-rouge\">src/main.rs</code> is needed because <code class=\"language-plaintext highlighter-rouge\">cargo fetch</code> requires a valid project structure even though it’s only reading the lockfile, and there’s been an <a href=\"https://github.com/rust-lang/cargo/issues/2644\">open issue</a> about removing that requirement since 2016.</p>\n\n<p>Almost nobody uses <code class=\"language-plaintext highlighter-rouge\">cargo fetch</code> in Dockerfiles. The Rust community skipped straight to caching compilation with <a href=\"https://github.com/LukeMathWalker/cargo-chef\">cargo-chef</a>, because compiling hundreds of crates is where builds spend most of their wall-clock time and downloads feel cheap by comparison. But every <code class=\"language-plaintext highlighter-rouge\">cargo build</code> without a prior <code class=\"language-plaintext highlighter-rouge\">cargo fetch</code> is still hitting crates.io for every crate whenever the layer rebuilds, and Fastly is absorbing that traffic whether it takes three seconds or thirty.</p>\n\n<h3 id=\"pip-download\">pip download</h3>\n\n<p><a href=\"https://pip.pypa.io/en/stable/cli/pip_download/\"><code class=\"language-plaintext highlighter-rouge\">pip download</code></a> fetches distributions into a directory, and <code class=\"language-plaintext highlighter-rouge\">pip install --no-index --find-links</code> installs from that directory offline:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">COPY</span><span class=\"s\"> requirements.txt .</span>\n<span class=\"k\">RUN </span>pip download <span class=\"nt\">-r</span> requirements.txt <span class=\"nt\">-d</span> /tmp/pkgs\n<span class=\"k\">COPY</span><span class=\"s\"> . .</span>\n<span class=\"k\">RUN </span>pip <span class=\"nb\">install</span> <span class=\"nt\">--no-index</span> <span class=\"nt\">--find-links</span> /tmp/pkgs <span class=\"nt\">-r</span> requirements.txt\n</code></pre></div></div>\n\n<p>There’s a <a href=\"https://github.com/pypa/pip/issues/7863\">known bug</a> where build dependencies like setuptools aren’t included in the download, so packages that ship only as source distributions can fail during the offline install, though most Python projects in 2026 ship as prebuilt wheels unless you’re doing something unusual with C extensions.</p>\n\n<p>Neither Poetry nor uv have download-only commands. Poetry has had an <a href=\"https://github.com/python-poetry/poetry/issues/2184\">open issue</a> since 2020, and uv has <a href=\"https://github.com/astral-sh/uv/issues/3163\">one</a> with over a hundred upvotes. Both suggest exporting to <code class=\"language-plaintext highlighter-rouge\">requirements.txt</code> and falling back to pip.</p>\n\n<h3 id=\"bundle-cache\">bundle cache</h3>\n\n<p>Bundler has <code class=\"language-plaintext highlighter-rouge\">bundle cache --no-install</code>, which fetches <code class=\"language-plaintext highlighter-rouge\">.gem</code> files into <code class=\"language-plaintext highlighter-rouge\">vendor/cache</code> without installing them, and <code class=\"language-plaintext highlighter-rouge\">bundle install --local</code> installs from that cache without hitting the network:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">COPY</span><span class=\"s\"> Gemfile Gemfile.lock ./</span>\n<span class=\"k\">RUN </span>bundle cache <span class=\"nt\">--no-install</span>\n<span class=\"k\">COPY</span><span class=\"s\"> . .</span>\n<span class=\"k\">RUN </span>bundle <span class=\"nb\">install</span> <span class=\"nt\">--local</span>\n</code></pre></div></div>\n\n<p>In practice this has enough rough edges that it rarely gets used in Dockerfiles. Git-sourced gems <a href=\"https://github.com/ruby/rubygems/issues/6499\">still try to reach the remote</a> even with <code class=\"language-plaintext highlighter-rouge\">--local</code>, and platform-specific gems need <code class=\"language-plaintext highlighter-rouge\">--all-platforms</code> plus <code class=\"language-plaintext highlighter-rouge\">bundle lock --add-platform</code> to work across macOS development and Linux containers. The command was designed for vendoring gems into your repository rather than for Docker layer caching.</p>\n\n<h3 id=\"npm-and-yarn\">npm and yarn</h3>\n\n<p>npm has no download-only command. <code class=\"language-plaintext highlighter-rouge\">npm ci</code> reads the lockfile and skips resolution, but downloads and installs as one atomic operation with no way to separate them, and there’s no <code class=\"language-plaintext highlighter-rouge\">--download-only</code> flag or RFC proposing one.</p>\n\n<p>Yarn Classic has an offline mirror that saves tarballs as a side effect of install, but no standalone download command. Yarn Berry has no fetch command either, despite <a href=\"https://github.com/yarnpkg/berry/issues/4529\">multiple</a> <a href=\"https://github.com/yarnpkg/berry/issues/5998\">open</a> issues requesting one.</p>\n\n<p>The standard JavaScript Docker pattern is still:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">COPY</span><span class=\"s\"> package.json package-lock.json ./</span>\n<span class=\"k\">RUN </span>npm ci\n<span class=\"k\">COPY</span><span class=\"s\"> . .</span>\n</code></pre></div></div>\n\n<p>When the lockfile hasn’t changed the layer caches and nothing gets downloaded, but when it has changed every package re-downloads from the registry, and pnpm is the only JavaScript package manager where you can avoid that.</p>\n\n<h3 id=\"buildkit-cache-mounts\">BuildKit cache mounts</h3>\n\n<p>Docker BuildKit has <code class=\"language-plaintext highlighter-rouge\">--mount=type=cache</code>, which persists a cache directory across builds so package managers can reuse previously downloaded packages even when the layer invalidates:</p>\n\n<div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">RUN </span><span class=\"nt\">--mount</span><span class=\"o\">=</span><span class=\"nb\">type</span><span class=\"o\">=</span>cache,target<span class=\"o\">=</span>/root/.npm npm ci\n</code></pre></div></div>\n\n<p>Cache mounts solve the problem from the wrong end. The package manager has the lockfile and knows the cache format, but Docker doesn’t know any of that, which is why the Dockerfile author has to specify internal cache paths that vary between tools and sometimes between versions of the same tool. Not every build system supports BuildKit cache mounts either, and not every CI environment preserves them between builds, so a download command in the package manager itself would be more broadly useful.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Registry</th>\n      <th>Funding</th>\n      <th>Download command</th>\n      <th>Offline install</th>\n      <th>Used in practice?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Go module proxy</td>\n      <td>Google</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">go mod download</code></td>\n      <td>implicit</td>\n      <td>Yes, canonical</td>\n    </tr>\n    <tr>\n      <td>npm registry</td>\n      <td>Microsoft</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">pnpm fetch</code> (pnpm only; npm and yarn have nothing)</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">--offline</code></td>\n      <td>pnpm yes, others no</td>\n    </tr>\n    <tr>\n      <td>crates.io</td>\n      <td>Fastly (donated)</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">cargo fetch</code></td>\n      <td><code class=\"language-plaintext highlighter-rouge\">--frozen</code></td>\n      <td>Rarely</td>\n    </tr>\n    <tr>\n      <td>PyPI</td>\n      <td>Fastly (donated)</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">pip download</code> (pip only; Poetry and uv have nothing)</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">--no-index --find-links</code></td>\n      <td>Rarely</td>\n    </tr>\n    <tr>\n      <td>rubygems.org</td>\n      <td>Fastly (donated)</td>\n      <td><code class=\"language-plaintext highlighter-rouge\">bundle cache --no-install</code></td>\n      <td><code class=\"language-plaintext highlighter-rouge\">--local</code></td>\n      <td>Rarely</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Most package managers were designed around a persistent local cache on a developer’s laptop, <code class=\"language-plaintext highlighter-rouge\">~/.cache</code> or <code class=\"language-plaintext highlighter-rouge\">~/.gem</code> or <code class=\"language-plaintext highlighter-rouge\">~/.npm</code>, that warms up over time and stays warm. Ephemeral build environments start clean every time, and Docker layers are the only caching mechanism available, which means the network-dependent part of a build needs to be isolated from the rest for caching to work.</p>\n\n<p>Opportunities:</p>\n\n<ul>\n  <li>npm could add an <code class=\"language-plaintext highlighter-rouge\">npm fetch</code> that reads <code class=\"language-plaintext highlighter-rouge\">package-lock.json</code> and populates the cache without installing</li>\n  <li>Poetry has had an <a href=\"https://github.com/python-poetry/poetry/issues/2184\">open issue</a> requesting a download command since 2020, and uv has <a href=\"https://github.com/astral-sh/uv/issues/3163\">one</a> with strong community interest</li>\n  <li>Bundler’s <code class=\"language-plaintext highlighter-rouge\">bundle cache --no-install</code> would work if it handled git gems and cross-platform builds more reliably</li>\n  <li>Cargo’s <code class=\"language-plaintext highlighter-rouge\">cargo fetch</code> shouldn’t need a dummy source file to run a command that only reads the lockfile</li>\n</ul>",
    "description": "Most package managers could separate download from install for better Docker layer caching.",
    "is_fulltext": true,
    "source": "Andrew Nesbitt",
    "pub_date": "2026-02-15T00:00:00+00:00",
    "fetched_at": "2026-02-15T12:16:07.690787",
    "url_hash": "a19b9996a33a7d10ec901abeccbad2e0"
  },
  {
    "title": "tiny corp’s product – a training box",
    "link": "https://geohot.github.io//blog/jekyll/update/2026/02/15/tiny-corp-product.html",
    "content": "<p><img src=\"/blog/assets/images/hk_office.jpg\" /></p>\n<blockquote>\n  <p>Our new Hong Kong office.</p>\n</blockquote>\n\n<p>It’s starting to shape up what tiny corp’s product will be. It’s not much of a change from what we sell and do now, but the vision is clearer.</p>\n\n<p>Every month, we see these LLMs become more and more human. However, there’s a major difference. They do not learn. Everyone has the same Claude/Codex/Kimi, with the same weights, the same desires, and the same biases. If current trends continue, the collapse in diversity will be staggering. To paraphrase:</p>\n\n<blockquote>\n  <p>I think there is a world market for maybe five people.</p>\n</blockquote>\n\n<p>This is not the future I want to live in.</p>\n\n<hr />\n<p><br /></p>\n\n<p>If trends continue where there’s a single model with frozen weights and all learning is in-context, the cloud will win. Except in some highly latency sensitive (fighting robots) or connectivity critical (self driving cars) environments, it will be cheaper to run in batch on the cloud.</p>\n\n<p>The enshittification that came to the web won’t be the driving force to local models. We either live in a world where open models are so bad even user-hostile closed models are better, or open models are good enough, and competition to run them through sites like <a href=\"https://openrouter.ai/\">openrouter</a> will prevent enshittification.</p>\n\n<p>The only way local models win is if there’s some value in full on learning per user or organization. At that point, with entirely different compute needing to run per user, local will beat out cloud.</p>\n\n<p>The open question is if everything that’s unique about you can fit in a 10 kB CLAUDE.md. If that’s true, we have a pretty sad future ahead. It’s the Attack of the Clones, swarms of identical minds you have no say over all varying in a small boxed-in way. This isn’t learning, it’s <em>costuming</em>. Everyone who has used these things knows how little of an impact prompting makes compared to the model. It’s the Internet funneled into a little box you can edit on your profile. Write 3 paragraphs about what makes you unique.</p>\n\n<p>We have to build for a future where that isn’t true. 90% of people will choose the cloud, and what they will find is that they are no longer meaningfully in the loop. The dream is an AI product that will do your job for you while you continue to get paid. But this cannot exist, that’s way too much of a fee to pay to the middleman. If you choose the homogenous mind, you are superfluous and will be cut out. Is there anything uniquely valuable about you? And I mean honestly, not the self-esteem pumping speeches you may have heard in school. If there’s not, I have some bad news for you…</p>\n\n<hr />\n<p><br /></p>\n\n<p>We already <a href=\"https://tinygrad.org/#tinybox\">sell the hardware</a>. Consumer GPUs still are the cheapest way to run models. There’s tons of work required on <a href=\"https://github.com/tinygrad/tinygrad\">the infrastructure</a>. The frontend will be the future iterations of <a href=\"https://openclaw.ai/\">OpenClaw</a> and <a href=\"https://opencode.ai/\">opencode</a>. But the key distinction from what you have today is that your tinybox will learn. It will update the weights based on its interactions with you. Like living things.</p>\n\n<p>This is many years away. Currently, we are focused on large LLM training (even running these things is hard, have you tried to use vLLM not on NVIDIA?) and generic infrastructure for driving GPUs. But this is the long term idea.</p>\n\n<p>Not API keyed SaaS clones. Something that lives in your house and learns your values. Your child.</p>",
    "description": "Our new Hong Kong office.",
    "is_fulltext": true,
    "source": "the singularity is nearer",
    "pub_date": "2026-02-15T00:00:00+08:00",
    "fetched_at": "2026-02-15T12:16:14.445427",
    "url_hash": "d2c96b448c51b048ffbaa54d788e5c51"
  },
  {
    "title": "How Michael Abrash doubled Quake framerate",
    "link": "https://fabiensanglard.net/quake_asm_optimizations/index.html",
    "content": "",
    "description": "",
    "is_fulltext": false,
    "source": "Fabien Sanglard",
    "pub_date": "14 Feb 2026 00:00:00 +0000",
    "fetched_at": "2026-02-15T12:16:22.010588",
    "url_hash": "f11357828acfc12e51bc68064af5962c"
  }
]